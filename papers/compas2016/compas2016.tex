%===========================================================
%                              Choix de track
%===========================================================
% Une des trois options 'parallelisme', 'architecture', 'systeme' 
% doit être utilisée avec le style compas2016
\documentclass[parallelisme]{compas2016}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{boxedminipage}
\usepackage{xspace}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{verbatim}
\usepackage{parcolumns}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%Prevents floating item to "jump" between sections
\usepackage[section]{placeins}
\usepackage{booktabs}
\usepackage{tkz-graph}
\usepackage{setspace}
%\setstretch{0.95}




\renewcommand{\ttdefault}{pcr}
\lstset{
	tabsize=4,
%	frame=single,
	breaklines=true,
	basicstyle=\ttfamily,
	frame=tb,
	framerule=0.2pt,
%	frameround={tttt},
	showstringspaces=false,
	language=c,
%	linewidth=0.95\textwidth,
	keywordstyle=\color{black}\bfseries,
%	keywordstyle=\color{blue},
	commentstyle=\color{OliveGreen},
	stringstyle=\color{red}\itshape,
	inputencoding=utf8/latin1,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
% OMP define
emph={\#,pragma, taskwait, omp, task, depend}, emphstyle=\color{RoyalBlue}\bfseries,
emph={[2]in,inout,out,cw}, emphstyle={[2]\color{BrickRed}\bfseries},
emph={[3]tied,untied,shared}, emphstyle={[3]\color{Gray}\bfseries},
emph={[4]lu0,fwd,bdiv,bmod}, emphstyle={[4]\color{DarkGreen}\bfseries},
emph={[5]cw}, emphstyle={[5]\color{DarkViolet}\bfseries},
    %moredelim=**[is][\only<3>{\color{red}}]{@}{@},
}
\lstdefinestyle{smaller}{basicstyle=\scriptsize\ttfamily}
\lstMakeShortInline|
%===========================================================
%                               Title
%===========================================================

\toappear{1} % Conserver cette ligne pour la version finale

\begin{document}

\title{Amélioration des stratégies d'ordonnancement sur architectures NUMA à l'aide
des dépendances de données}

\author{Philippe Virouleau}
\address{Inria,\\
   Univ. Grenoble Alpes,  CNRS, Grenoble Institute of Technology, LIG, Grenoble, France\\
   philippe.virouleau@inria.fr\\
}
\date{}


\maketitle

\newcommand{\benchs}{KASTORS }
\newcommand{\kaapi}{\textsc{\mbox{Xkaapi}}\xspace}
%===========================================================         %
%R\'esum\'e
%===========================================================  
\begin{abstract}
Le récent ajout des dépendances de données à la norme OpenMP 4.0 offre
au programmeur une manière flexible de synchroniser les tâches.
Grâce à cela, le compilateur et le support exécutif peuvent tous les deux savoir
exactement quelles données sont lues ou écrites par quelles tâches.
Les performances sur architectures NUMA peuvent être fortement impactées par
le placement des données et l'ordonnancement des tâches. Les informations présentes
dans les dépendances de données peuvent être utilisées pour contrôler le placement physique des données,
ainsi que pour contrôler les stratégies de placement des tâches en
fonction de la topologie.
Cet article présente plusieurs heuristiques pour ces stratégies, et leurs implémentations
dans notre support exécutif OpenMP : \kaapi.
Nous présentons également nos évaluations sur des applications d'algèbre linéaire,
exécutées sur une machine NUMA à 192 cœurs, et comparées aux stratégies proposées
par l'état de l'art.
  \MotsCles{OpenMP, tâche avec dépendances, support exécutif, NUMA, ordonnancement}
\end{abstract}

	
\vspace*{-4ex}
\section{Introduction}

Les architectures à temps d'accès mémoire non uniforme (NUMA) sont
aujourd'hui le choix le plus populaire pour créer des grosses machines à mémoire
partagées.
Compte tenu de la différence de temps d'accès en fonction de la distribution physique
de la mémoire (une donnée proche sera accédée beaucoup plus rapidement qu'une donnée distante),
contrôler le placement des données tout au long de l'exécution d'une application
est donc l'un des points clés pour améliorer sa scalabilité et ses performances.

Les environnements de programmation parallèle tels que OpenMP, OpenCL, ou TBB sont devenus très
populaires pour exploiter les machines à mémoire partagée avec plusieurs centaines de cœurs.
Ils offrent un moyen d'exprimer beaucoup de parallélisme à grain fin, tout en
ayant un coût relativement faible. La plupart d'entre eux fournissent également
un moyen d'équilibrer dynamiquement la charge de travail sur tous les processeurs,
mais aucun d'entre eux ne fournis de moyen de gérer la localité
des données sur des systèmes NUMA.

L'ajout récent des dépendances de données au modèle de programmation par tâches
d'OpenMP fourni au support exécutif des informations précises, notamment sur les
accès mémoires des tâches de l'application.
L'ordonnanceur de tâches peut implémenter des stratégies dédiées aux architectures
NUMA, comme montré dans cet article.

Cet article décrit plusieurs stratégies que nous avons implémenté dans notre support
exécutif \kaapi exploité à travers libKOMP~\cite{libkomp}, une interface pour la norme OpenMP.
Nous avons identifié trois points majeurs dans l'ordonnanceur qui
peuvent impacter les applications parallèles pour systèmes NUMA : la distribution
de données, l'assignation des tâches prête, et la manière de parcourir l'architecture
pour effectuer l'équilibrage de charge.

Cet article les décrit et les évalue, en montrant leur impact sur les performances
d'applications s'exécutant sur une machine NUMA à 192 cœurs.
Nous les comparons également aux stratégies de l'état de l'art implémentées dans \kaapi.

Le plan de cet article est le suivant : dans la partie~\ref{sec:background},
nous donnons quelques informations essentielles sur les architectures NUMA et
le modèle de tâches avec dépendances. Ensuite dans la partie~\ref{sec:contributions}
nous décrivons les idées, stratégies, et détails d'implémentation que nous avons
utilisé pour améliorer les performances du système exécutif. La partie~\ref{sec:performances-evaluation}
présente les évaluations de performances. Enfin nous présentons des travaux de l'état
de l'art dans la partie~\ref{sec:related-work}, avant de conclure.


\vspace*{-1ex}
\section{Description des systèmes NUMA et de leur exploitation}

\label{sec:background}

Nous avons effectué nos expériences sur une machine SGI UV2000, constituée de
24 nœuds NUMA, possédant chacun un processeur Intel Xeon E5-4640 à 8 cœurs,
le tout formant un total de 192 cœurs. Nous utilisons le nom Intel192 pour faire
référence à cette machine dans l'article.

Afin d'exploiter les grosses architectures à mémoire partagée, le programmeur a besoin :
d'une part d'exprimer beaucoup de parallélisme à grain fin, pour profiter au
maximum du nombre important de processeurs disponibles; d'autre part de contrôler
l'exécution de l'application, en particulier la manière dont sont distribuées
les données.

Les environnements de programmation parallèle à base de tâches fournissent un moyen
d'exprimer le parallélisme à grain fin. OpenMP~\cite{openmp40}, le standard utilisé
en pratique pour la programmation des architectures à mémoire partagée, supporte
le parallélisme à base de tâches avec dépendances de données depuis la version 4.0.

\subsection{Un aperçu du modèle de tâches d'OpenMP}

Une \emph{tâche} OpenMP peut être vue comme une \emph{unité indépendante de travail} qu'un thread
OpenMP peut exécuter. Les tâches peuvent être exécutées par un thread quelconque
de la région parallèle.

Dans la norme OpenMP 3.0, la synchronisation des tâches est effectuée à l'aide du
mot clé |taskwait|, imposant l'attente de la complétion de toutes les tâches
de la région parallèle courante.

La norme 4.0 enrichie ce concept en introduisant le mot clé |depend|, qui permet
de spécifier le mode d'accès des variables utilisées par une tâches pendant son exécution.
Le mode d'accès peut être soit |in|, |out|, ou |inout|, en fonction de si la
variable correspondante est respectivement lue par la tâche, écrite, ou bien les deux.
Cette information peut être analysée par le support exécutif, qui décidera si
la tâche est prête à être exécutée ou si elle doit attendre la complétion d'une
autre tâche.


%\subsubsection{Un exemple de la suite de benchmarks KASTORS}
%La figure~\ref{lulst} montre l'implémentation d'une factorisation de Cholesky
%à l'aide des tâches avec dépendances d'OpenMP.
%Cet algorithme est très proche de celui implémenté dans la suite de benchmark
%KASTORS~\cite{virouleau:hal-01081974}, utilisée dans nos expérimentations.

%Dans certaines situations, la programmation par tâches avec dépendances permet
%plusieurs scénarios d'exécution valide de l'application, ce qui permet au
%support exécutif plus de liberté dans l'ordonnancement des tâches.
%Dans l'exemple, il y a plusieurs instances des BLAS |dtrsm|, |dsyrk|, et |dgemm|
%qui peuvent s'exécuter de manière concurrente.

%\begin{figure}[tbp]
%\hrule
%\begin{minipage}[t]{.43\textwidth}
%\begin{lstlisting}[frame=none,style=smaller,showlines=true,label=lst:LU-deps]{lst:LU-deps1}
%for (size_t k=0; k < NB; ++k) {
%#pragma omp task shared(A) \
  %depend(inout: A[k][k])
  %dpotrf(NB,&A[k][k]);

  %for (int m=k; m < NB; ++m) 
%#pragma omp task shared(A)\
  %depend(in: A[k][k]) \
  %depend(inout: A[m][k])
    %dtrsm(NB,&A[k][k],&A[m][k]);

%\end{lstlisting}
%\end{minipage}\hfill
%\begin{minipage}[t]{.485\textwidth}
%\begin{lstlisting}[frame=none,style=smaller,label=lst:LU-deps,firstnumber=12]{lst:LU-deps2}
  %for (int m=k; m < NB; ++m) {
%#pragma omp task shared(A)\
  %depend(in: A[m][k]) \
  %depend(inout: A[m][m])
   %dsyrk(NB,&A[m][k], &A[m][m]);

    %for (int n=k; n < m; ++n)
%#pragma omp task shared(A)\
  %depend(in: A[m][k],A[n][k])\
  %depend(inout: A[m][n])
     %dgemm(NB,
       %&A[m][k],&A[n][k],&A[m][n]);
  %}
 %}
%\end{lstlisting}
%\end{minipage}
%\hrule
%\caption{Cholesky factorization with OpenMP-4.0 task dependencies} \label{lulst}
%\end{figure}


\subsection{Notre manière d'exécuter les programmes à base de tâches}

Le support exécutif que nous développons, \kaapi, implémente un modèle d'exécution par vol
de travail pour les programmes à base de tâches OpenMP. Ce modèle a été présenté
à la base dans Cilk~\cite{cilk5}, et est couramment utilisé dans les environnements
de programmation à base de tâches.

\kaapi créer un thread, appelé un \emph{kproc}, pour chaque unité d'exécution
(dans le cas d'une machine NUMA, il s'agit d'un cœur).
Chaque kproc possède sa propre queue de travail dans laquelle il poussera les
tâches qu'il créer, et qui est implémentée comme une pile,

\vspace*{-1ex}
\subsection{Contrôler la distribution des données sur les architectures NUMA}

%TODO déplacer en 2
Contrôler le placement des données nécessite de bien connaître l'architecture NUMA.
Les programmeurs peuvent contrôler l'allocation des données à l'aide d'outils
dédiés, tels que \verb/numactl/~\cite{DBLP:journals/corr/abs-1101-0093}.

Par exemple l'option \verb/--interleave=all/ permet de répartir toutes les pages
sur l'architecture suivant une politique de round-robin sur tous les nœuds NUMA.
Cette politique est largement utilisé avec du parallélisme dynamique,
car elle répartie le trafic mémoire sur tous les contrôleurs mémoire, rendant
tous les processeurs "\emph{équitablement mauvais}" par rapport aux accès mémoire.

Nous proposons deux approches différentes pour distribuer la mémoire sur les systèmes
NUMA, basé sur le principe de l'allocation \emph{first-touch} de la mémoire : la première
en donnant la possibilité d'allouer explicitement la donnée sur un nœud NUMA
spécifique de la machine, via une API que nous fournissons~\cite{Durand2013,BroFurGogWacNam10IJPP};
la seconde en donnant la possibilité de marquer certaines portions de code comme
étant du code d'initialisation, et le support exécutif se charge de la distribution
des tâches créées dans cette portion.
Olivier et al.~\cite{Olivier:2012:CMW:2388996.2389085} ont utilisés cette
approche avec des restrictions sur l'exécution des tâches créées par l'application.

Ces deux approches permettent au support exécutif de savoir où les données
de l'application sont stockées, et cela peut être utilisé pour guider l'équilibrage de charge.



\vspace*{-1ex}
\section{Amélioration du placement des tâches et des données grâce aux dépendances}
\label{sec:contributions}

Dans cette partie, nous décrivons comment le support exécutif peut avoir un impact
positif sur l'exécution de l'application, en utilisant les informations présentes
dans les dépendances.


\vspace*{-1ex}
\subsection{La mécanique interne de \kaapi}

\kaapi implémente un modèle d'exécution par vol de travail pour les programmes
à base de tâches OpenMP, d'après le modèle présenté dans Cilk~\cite{cilk5}.
Les paragraphes suivant décrivent quelques mécanismes clés.


\kaapi modélise la topologie de l'architecture comme une hiérarchie de \verb/places/.
Une \verb/place/ est en fait une liste de tâches associées à un ensemble de cœurs.
La plupart du temps \kaapi considère deux niveaux de places : les places de niveau
NUMA (une place par nœud NUMA, regroupant l'ensemble des cœurs sur ce nœud),
et les places au niveau du processeur (une place par cœur).


Le graphe de dépendances est construit, et lors d'un vol le voleur va choisir
l'une des tâches dans la liste des tâches prêtes de la victime.

L'environnement de développement dans \kaapi~\cite{DBLP:journals/corr/BleuseGLMT14} repose sur des fonctions
virtuelles pour \textit{sélectionner une victime}, \textit{sélectionner une place} dans
laquelle pousser une tâche prête, et \textit{pousser} un ensemble de tâches initialement
prêtes.
Quand un processeur devient inactif, le support exécutif utilise une fonction appelée
\verb/WSselect/ pour parcourir la topologie à la recherche d'une place à voler.

La complétion d'une tâche peut rendre possible l'exécution d'un ensemble de tâches.
Le choix de la place où pousser chacune des tâches est implémentée par une fonction
appelée \verb/WSpush/.
Avant l'exécution parallèle de l'application, le support exécutif peut répartir
les tâches prêtes à l'aide de la fonction appelée \verb/WSpush_init/.

Ces trois fonctions sont les trois points d'entrée à préciser pour définir
un algorithme d'ordonnancement dans \kaapi.



\vspace*{-1ex}
\subsection{Distribution initiale des tâches : stratégies WSpush\_init}

Nous avons implémenté deux stratégies de distribution : \verb/cyclicnuma/, qui
distribue les tâches en round-robin sur les nœuds NUMA, et \verb/randnuma/, aléatoirement.
Il faut noter que contrairement à \verb/numactl/, nos stratégies fonctionnent
par blocs de données dans les clauses |depend|, et pas par pages.


\vspace*{-1ex}
\subsection{Distribution des tâches prêtes : stratégies WSpush}

Nous présentons quatre stratégies pour pousser les tâches prêtes dans une place
du système NUMA. Deux d'entre elles ne prennent pas en compte les données, les deux
autre si, via la clause OpenMP |depend| sur les tâches.

La stratégie \verb/pLoc/ permet au processeur de pousser localement les tâches prêtes.
La stratégie \verb/pLocNuma/ permet au processeur de les pousser dans la place de son nœud NUMA local.

La stratégie \verb/pNumaW/ pousse les tâches sur la place NUMA où les données
écrites par la tâche se trouvent (W est l'initiale de Write).
La dernière stratégie, \verb/pNumaWLoc/, est similaire à \verb/pNumaW/, mais
si la donnée est allouée sur le nœud NUMA local, alors la tâche est poussé dans
la place du processeur courant.


\vspace*{-1ex}
\subsection{Équilibrage dynamique de charge par vol de travail : stratégies WSselect}

Les deux premières, \verb/sRand/ et \verb/sRandNuma/, sont similaires à celles étudiées
dans~\cite{DBLP:journals/ijhpca/OlivierPWSP12}. \verb/sRand/ sélectionne une place
processeur aléatoire, et \verb/sRandNuma/ sélectionne une place NUMA aléatoire.
Nous avons également implémenté plusieurs stratégies prenant en compte les deux niveaux
de hiérarchie :

\begin{itemize}
  \item \verb/sProcNuma/ : dans un premier temps on visite la place du processeur.
    en cas d'échec, on visite la topologie dans l'ordre suivant : d'abord les
    processeurs voisins; puis, une fois que tous les voisins ont été visités, on
    visite la place NUMA locale; puis on visite toutes les places processeur d'un
    nœud NUMA aléatoire; et éventuellement sa place NUMA.
    
  \item \verb/sNumaProc/ : cette stratégie est similaire, mais on visite les places
    NUMA avant de visiter les places processeur.
  \item \verb/sProc/ : le voleur ne visite que les places processeur et sa place
    NUMA locale.
  \item \verb/sNuma/ : le voleur ne visite que les places NUMA et sa place processeur.
\end{itemize}



\vspace*{-1ex}
\section{Évaluation de performances}
\label{sec:performances-evaluation}
Toutes nos expériences ont été effectuées sur la machine Intel192 décrite en partie~\ref{sec:background}.
Nous avons évalué nos stratégies sur deux applications tirées des benchmarks
KASTORS~\cite{virouleau:hal-01081974}\footnote{git disponible ici : https://scm.gforge.inria.fr/anonscm/git/kastors/kastors.git, tag \emph{paper-hierarchy}} :
une factorisation QR par bloc (\verb/dgeqrf_taskdep/), et une factorisation de
Cholesky par block (\verb/dpotrf_taskdep/).
Ces applications utilisent des routines BLAS et LAPACK fournies par OpenBLAS 2.15.
Nous avons utilisé la librairie OpenMP compatible avec GCC libKOMP~\cite{libkomp} basée
sur le support exécutif \kaapi\footnote{https://scm.gforge.inria.fr/anonscm/git/kaapi/kaapi.git,
branche \emph{viroulea/public/paper}}.
Nos traces et résultats sont également disponibles en ligne\footnote{https://github.com/viroulep/results-paper-hierarchy} afin que notre analyse
soit reproductible par une tierce personne.


\vspace*{-1ex}
\subsection{Impact de la distribution de données}

Dans un premier temps nous avons évalué l'impact de la distribution de données sur
les performance de la factorisation de Cholesky. Nous avons fait cette évaluation sur plusieurs
tailles de matrices et tailles de bloc, ainsi que sur plusieurs combinaisons
de stratégies WSpush et WSselect. Nous avons comparé ces résultats aux performances de GCC (via libGOMP),
en utilisant, ou pas, une initialisation parallèle, ainsi que \verb/numactl/.

La conclusion de ces expériences est que l'initialisation parallèle est nécessaire
pour obtenir les meilleures performances possibles avec libGOMP, et atteignant 1750 Gflops en moyenne.
Sans initialisation parallèle, libGOMP n'atteint pas 300 Gflops, et l'utilisation de
\verb/numactl/ est impérative pour voir un réel gain de performances, avec une moyenne à 1140 Gflops.

Pour \kaapi, l'utilisation d'une distribution contrôlée de données (\verb/cyclicnuma/,
\verb/randnuma/) permet aux stratégies indépendantes de la hiérarchie de dépasser
les meilleures performances de libGOMP (respectivement 1814, 1779 Gflops).
L'utilisation de stratégies hiérarchiques permet d'atteindre les meilleures performances
de l'application, avec une moyenne de 1980 Gflops.

La distribution \verb/cyclicnuma/ sera utilisée comme stratégie par défaut pour
les expériences suivantes, étant donné qu'elle offre les meilleures performances.

\vspace*{-1ex}
\subsection{Aperçu des performances des différentes stratégies}

Nous avons choisi une distribution de données, \verb/cyclicnuma/, et comparé les
différentes stratégies.
Une nouvelle fois, même si les performances obtenues ne sont évidemment pas les
même, la comparaison des stratégies les unes par rapport aux autres est similaire
pour les différentes applications que nous avons lancé.

La Figure \ref{fig:eval-all-strat} montre les résultats des expériences pour
la factorisation de Cholesky sur des matrices de largeur 32K, divisées par
bloc de 512$\times$512 éléments (meilleure taille de bloc pour cette taille de matrice).
La ligne en pointillé représente la performance du runtime de GCC,
libGOMP~\footnote{GCC 5.2.0}, en utilisant une initialisation parallèle.

La première chose à noter est que même une combinaison de stratégies \verb/WSselect/
\verb/WSpush/ basique telle que \verb/sRand/+\verb/pLoc/ (Stratégie 1) obtient des performances
acceptables grâce à la distribution de données.
De plus, étant donnée une stratégie de sélection (e.g. \verb/sRandNuma/), pousser
les tâches sur le nœud NUMA où elle écrit des données (stratégies 7) offre
de meilleures performances que simplement pousser les tâches sur son nœud NUMA (stratégie 2).

En considérant que les tâches sont poussées en utilisant la stratégie \verb/pNumaWLoc/,
ne prendre en compte qu'un niveau de hiérarchie (stratégies 3 ou 5) ne permet
pas d'atteindre les performances obtenues avec des stratégies naïves. En revanche,
prendre en compte les deux niveaux de hiérarchie (stratégies 4 et 6)
permet d'obtenir de meilleures performances que les stratégies étudiées précédemment.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.5]{figures/graph_all_strat.pdf}
\vspace*{-3ex}
  \caption{Évaluation des toutes les stratégies (WSselect + WSpush)}
\label{fig:eval-all-strat}
\end{figure}

\subsection{Scalabilité des performances des stratégies}

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.5]{figures/graph_details_strat.pdf}
\vspace*{-2ex}
\caption{Évaluation d'une sélection de stratégies sur plusieurs tailles}
\label{fig:eval-strat-sizes}
\end{figure}

Nous avons finalement sélectionné les trois combinaisons de stratégies meilleures
que les performances de GCC pour évaluer leur scalabilité en fonction de la taille
de la matrice en entrée.

La Figure \ref{fig:eval-strat-sizes} montre leurs performances en utilisant une distribution
\verb/cyclicnuma/.
Les performances ont été obtenues en utilisant la meilleure taille de bloc pour
chacune des tailles de matrice (qui est dans notre cas 256 pour une taille de 16384,
et 512 pour les autres).

Comme attendu, utiliser une combinaison de stratégies qui prend en compte à la fois
la localité des données et la hiérarchie de l'architecture (\verb/sNumaProc/ + \verb/pNumaWLoc/)
permet d'obtenir les meilleures performances.
La seule exception apparait pour les petites tailles de matrices (16384), où il
n'y a peut être tout simplement pas assez de travail pour tirer partie de ces stratégies. 

On peut également noter que la simple utilisation d'une distribution des données
utilisées par les tâches permet d'obtenir des performances convenables avec une
combinaison de stratégies basique (\verb/sRand/ + \verb/pLoc/).

\section{État de l'art}
\label{sec:related-work}

De nombreux travaux traitent de la localité et/ou de stratégie d'ordonnancement
spécifiques aux architectures NUMA.

Clet-Ortega et al.~\cite{DBLP:conf/europar/Clet-OrtegaCP14} ont étudié et évalué plusieurs
façon de décorer la topologie de l'architecture, en privilégiant des listes de tâches
privées par thread, parcourue de manière hiérarchique. Nos étendons ces travaux
en considérant également des listes au niveau des nœuds, et nous avons montré
que cela peut aider à améliorer les performances sur des architectures NUMA.

Olivier et al.~\cite{DBLP:journals/ijhpca/OlivierPWSP12} ont évalué des stratégies
hiérarchiques d'ordonnancement de tâches, en utilisant des structures centralisées
ou distribuées. Notamment en créant un ensemble de threads, appelé \emph{shepherd}, par
nœud NUMA, permettant à l'ordonnanceur hiérarchique d'avoir de meilleures performances
qu'avec les autres approches.

Tahan et al.~\cite{DBLP:journals/corr/Tahan14} ont également étudié le comportement
des programmes à base de tâches OpenMP sur les systèmes NUMA, en étendant le
support exécutif NANOS avec deux ordonnanceurs de tâches appellés DFWSPT et DFWSRPT,
prenant en compte la notion de priorité de tâches, et essayant également de diminuer
la distance de la mémoire lors de l'équilibrage de charge.
D'autres travaux ont été fait dans le même context~\cite{DBLP:conf/europar/TerbovenSCM12,DBLP:journals/corr/abs-1101-0093},
mais aucun ne tire partie de la clause OpenMP |depend|, qui indique précisément les
données lues et écrites par une tâches donnée. Comme montrée par notre combinaison
de stratégies \verb/sNumaProc+pNumaWLoc/, il est rentable de prendre en compte
cette information lors de l'ordonnancement.
Drebes et al.~\cite{LIP610333} ont caractérisé les problèmes de performances des
algorithmes de vol de travail ignorant les spécificités des architectures NUMA,
et proposé des améliorations mises en œuvre dans OpenStream, leur support exécutif à base de tâches.


\section{Conclusion et travaux à venir}

Les environnements de programmation à base de tâches tels qu'OpenMP sont devenus
un moyen standard de programmer des systèmes NUMA à large échelle.
Ils offrent au programmeur un moyen d'exprimer du parallélisme à grain fin,
qui peut être associé dynamiquement à la topologie de l'architecture.
OpenMP a récemment évoluer pour permettre d'exprimer les dépendances de données
entre tâches.

Cet article présente plusieurs stratégie d'exécution pour affecter efficacement
les tâches prêtes aux listes de tâches représentant l'architecture NUMA.
Ces stratégies contrôlent à la fois la manière dont les tâches prêtes sont
initialement réparties, ainsi que la manière dont les tâches sont volées.
Nous avons évalué plusieurs distributions de données, et évalué différentes
combinaisons de stratégies "push" et "select" sur un système NUMA de 192 cœurs,
en utilisant des applications d'algèbre linéaire.
Les meilleures performances sont obtenues avec les stratégies prenant en compte à
la fois le placement initial des données et la hiérarchie de la machine.

À court terme nous prévoyons d'étendre le compilateur pour indiquer de manière
compatible avec OpenMP quelles sont les tâches d'initialisation.

Sur le long terme nous prévoyons de nous concentrer sur certaines techniques
de compilation qui pourrait nous permettre d'évaluer des informations
importantes sur les tâches, comme leur intensité opérationnelle, qui pourrait
aider à la prise de décision au sein du support exécutif. Nous pensons
que la coopération entre le compilateur et le support exécutif peut améliorer
significativement les performances et la scalabité des applications à base de tâches.

\vspace*{-2ex}
\section*{Remerciements}

PIA ELCI - Projet financé par le gouvernement français pour le développement
des applications hautes performances.

\bibliography{Bib/paper.bib}

\end{document}






