\documentclass{Styles/llncs}
%\documentclass[12pt,letterpaper]{article}
\usepackage{times}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{boxedminipage}
\usepackage{xspace}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{verbatim}
\usepackage{parcolumns}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%Prevents floating item to "jump" between sections
\usepackage[section]{placeins}
\usepackage{booktabs}
\usepackage{tkz-graph}
\newcommand{\arevoir}[1]{#1}

\newcommand{\kaapi}{\textsc{\mbox{kaapi}}\xspace}

\newcommand{\libXKOMP}{\textsc{libKOMP}\xspace}

\usepackage{xcolor}
\usepackage{todonotes}
\usepackage[color,leftbars]{changebar}

\newcommand{\cfsect}[1]{(\textit{cf.} section~\ref{#1})}
\newcommand{\cfsectpage}[1]{(\textit{cf.} section~\ref{#1}, page~\pageref{#1})}
\providecommand{\figureref}[1]{\figname~\ref{#1}}
\providecommand{\cftab}[1]{(\textit{cf.} tableau~\ref{#1})}
\newcommand{\cmd}[1]{{\upshape\texttt{\symbol{"5C}#1}}}

\newenvironment{remarque}
{\description \item[Remarque:] \ \slshape}
{\enddescription}

\makeatletter
\newbox\sf@box
\newenvironment{SubFloat}[2][]%
  {\def\sf@one{#1}%
   \def\sf@two{#2}%
   \setbox\sf@box\hbox
     \bgroup}%
  { \egroup
   \ifx\@empty\sf@two\@empty\relax
     \def\sf@two{\@empty}
   \fi
   \ifx\@empty\sf@one\@empty\relax
     \subfloat[\sf@two]{\box\sf@box}%
   \else
     \subfloat[\sf@one][\sf@two]{\box\sf@box}%
   \fi}
\makeatother
\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}
\renewcommand{\ttdefault}{pcr}
\lstset{
	tabsize=4,
%	frame=single,
	breaklines=true,
	basicstyle=\ttfamily,
	frame=tb,
	framerule=0.2pt,
%	frameround={tttt},
	showstringspaces=false,
	language=c,
%	linewidth=0.95\textwidth,
	keywordstyle=\color{black}\bfseries,
%	keywordstyle=\color{blue},
	commentstyle=\color{OliveGreen},
	stringstyle=\color{red}\itshape,
	inputencoding=utf8/latin1,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
% OMP define
emph={\#,pragma, taskwait, omp, task, depend}, emphstyle=\color{RoyalBlue}\bfseries,
emph={[2]in,inout,out,cw}, emphstyle={[2]\color{BrickRed}\bfseries},
emph={[3]tied,untied,shared}, emphstyle={[3]\color{Gray}\bfseries},
emph={[4]lu0,fwd,bdiv,bmod}, emphstyle={[4]\color{DarkGreen}\bfseries},
emph={[5]cw}, emphstyle={[5]\color{DarkViolet}\bfseries},
    %moredelim=**[is][\only<3>{\color{red}}]{@}{@},
}
\lstdefinestyle{smaller}{basicstyle=\scriptsize\ttfamily}
\lstMakeShortInline|

\newcommand{\benchs}{KASTORS }

\sloppy

\begin{document}

\title{Using data dependencies to improve task based scheduling strategies on NUMA architectures}
\author{
  Philippe Virouleau$^1$ \and François Broquedis$^2$ \and Thierry Gautier$^{1,3}$
 \vspace*{-1ex}}
\institute{
   $^1$INRIA, $^2$Grenoble Institure of Technology, $^3$LIP\\
   CORSE and AVALON Teams, Computer Science Laboratories of Grenoble
   and Lyon, France\\
   \email{philippe.virouleau@inria.fr}\\
   \email{francois.broquedis@imag.fr}\\
   \email{thierry.gautier@inrialpes.fr}\\
}
\date{}
\maketitle

\begin{abstract}
  \vspace*{-5ex} The OpenMP 4.0 specifications extended the task parallelism
  constructs by providing the user a more flexible way to synchronize tasks :
  data dependencies between tasks.
  Using such an approach allows both the compiler and the runtime system
  to know exactly which data are read or written by a given task, and how these
  data will be used through the program lifetime.  Data placement and scheduling
  strategies have a significant impact on performances when considering
  NUMA architectures.  Numerous papers focus on these topics, however none
  has made extensive use of the information available through dependencies.
  One can use these information to modify the behaviour of the application at
  several levels : during initialization to control data placement,
  and during the application runtime to dynamically control both the preferred
  task placement and stealing strategy, depending on the topology.
  This paper introduces several heuristics for these strategies and their
  implementations in our OpenMP runtime XKaapi.
  We also evaluate their performances by using several linear algebra
  applications, executed on a 192 cores NUMA machine. We finally
  compare them to strategies presented previously by related works.

\smallskip
  \noindent\textbf{Keywords:}
  \emph{
    OpenMP, task dependencies, benchmark, runtime systems, NUMA, Kaapi, scheduling, workstealing
  }
\end{abstract}


\section{Introduction}

TODO

task dependencies enable full knowledge about data

Different approaches for dealing with NUMA :

Initial data distribution/placement (set affinity)

Initial task placement (pushinit)

Dynamic ready-task placement (push)

Work-stealing strategies (select)

Contribution : new heuristic for these approaches, evaluation and comparison against previous one on a large NUMA machine.



%HPC architectures evolved so rapidly that it is now common to build
%shared-memory configurations with several dozens of cores.  The recent
%appearance of technologies such as the Intel Xeon Phi co-processor makes
%affordable configurations with thousands of cores a not-so-far
%reality.  Efficiently programming such large-scale platforms requires
%to express more and more fine-grain parallelism.

%Standard parallel programming environments such as OpenMP have evolved
%to address this requirement, introducing new ways of designing highly
%parallel programs. Extending OpenMP to support task parallelism stands
%as a first step to improve the scalability of OpenMP applications on
%large-scale platforms. Indeed, task parallelism usually comes with lower
%runtime-related overhead than thread-based approaches, allowing OpenMP
%programmers to create a large amount of tasks at low cost. Task
%parallelism also promotes the runtime system to a central role, as
%having more units of work to execute requires smarter scheduling
%decisions and load balancing capabilities.

%OpenMP was recently extended to support task dependencies. Instead of
%explicitly synchronizing all the tasks of a parallel region at once, the
%application programmer can now specify a list of variables a task will
%read as input or write as output instead. This information is
%transmitted to the task scheduling runtime system. The runtime then marks
%a task as ready for execution only once all its dependencies have been
%resolved. Dependencies therefore provide a way to define finer
%synchronizations between tasks, able to scale better than global
%synchronizations on large-scale platforms. Dependencies also give the
%runtime system more options to efficiently schedule tasks, as these
%become ready for execution as soon as the data they access has been
%updated.

%As promising as it looks however, any new feature needs proper
%evaluation to encourage application programmers to embrace it. While
%several compilers and runtime systems are now beginning to support
%OpenMP~4.0 task dependencies, no benchmark suite currently exists to
%evaluate their respective benefits and compare them to traditional task
%parallelism.

%This paper highlights two major contributions. We first introduce a
%new benchmark suite to experiment with OpenMP~4.0 task dependencies. We
%present performance results for both the GCC/libGOMP and the
%CLANG\footnote{Intel branch with support for OpenMP:
%\url{http://clang-omp.github.io/}}/libIOMP compilers and their runtime
%systems, comparing kernels
%involving either dependent or independent tasks. Secondly, we
%comment on the issues we met while implementing these benchmarks along
%the lines of current 4.0 revision of the OpenMP specification. Building
%on this experience, we contribute some extension proposals to the
%existing OpenMP specification, to improve the expressiveness of the
%task dependency support.

%The remainder of this paper is organized as follows. Section
%\ref{sec:omp-deps} describes the task dependency programming model in
%OpenMP~4.0. It then analyzes the strategies adopted by GCC/libGOMP and
%CLANG/libIOMP to implement this model. Section \ref{sec:benchs}
%introduces the \benchs benchmark suite we have designed to evaluate
%OpenMP~4.0's task model implementations. Section \ref{sec:perfs}
%presents the performance results of \benchs using two different hardware
%configurations. We identify and discuss practical issues with the
%current OpenMP specification, and we propose extensions in section
%\ref{sec:extensions} to address these issues. We finally present some
%related works in section \ref{sec:related-work} before concluding.

\section{Background}
\subsection{Hardware background}
Most of nowadays parallel shared memory architectures are built according to a NUMA~\footnote{Non-Uniform Memory Access} design where the memory is physically split in several banks attached to processors.
Many vendors assemble these banks in a hierarchical way, thus building shared memory machines embedding several hundreds of cores.
Exploiting such architectures at their full potential requires a fine control of the execution of a parallel application, as accessing local memory is most of the time faster than accessing memory stored in a memory bank attached to a remote processor.

The machine we experimented on is a SGI UV2000 platform made of 24 NUMA nodes.
Each NUMA node holds a 8-core Intel Xeon E5-4640 CPU for a total of 192 cores.
We refer to this machine as Xeon192 in the paper. 

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=0.6]
\tikzset{VertexStyle/.append style = {minimum size = 30pt, inner sep = 0pt}}

\Vertex[x=0, y=0, L=$n_{14,15}$]{n0}
\Vertex[x=0, y=2, L=$n_{22,23}$]{n4}
\Vertex[x=0, y=4, L=$n_{8,9}$]{n8}
\Vertex[x=0, y=6, L=$n_{12,13}$]{n12}
\Vertex[x=2, y=0, L=$n_{10,11}$]{n1}
\Vertex[x=2, y=2, L=$n_{6,7}$]{n5}
\Vertex[x=2, y=4, L=$n_{0,1}$]{n9}
\Vertex[x=2, y=6, L=$n_{16,17}$]{n13}

\Vertex[x=4, y=0, L=$n_{20,21}$]{n2}
\Vertex[x=4, y=2, L=$n_{4,5}$]{n6}
\Vertex[x=4, y=4, L=$n_{2,3}$]{n10}
\Vertex[x=4, y=6, L=$n_{10,11}$]{n14}
\Vertex[x=6, y=0, L=$n_{12,13}$]{n3}
\Vertex[x=6, y=2, L=$n_{8,9}$]{n7}
\Vertex[x=6, y=4, L=$n_{18,19}$]{n11}
\Vertex[x=6, y=6, L=$n_{14,15}$]{n15}

\Edge(n0)(n5)
\Edge(n4)(n5)
\Edge(n1)(n5)

\Edge(n13)(n9)
\Edge(n12)(n9)
\Edge(n8)(n9)

\Edge(n15)(n10)
\Edge(n14)(n10)
\Edge(n11)(n10)

\Edge(n3)(n6)
\Edge(n2)(n6)
\Edge(n7)(n6)

\Edge(n9)(n5)
\Edge(n9)(n10)
\Edge(n5)(n10)
\Edge(n9)(n6)
\Edge(n5)(n6)
\Edge(n10)(n6)

\end{tikzpicture}
\end{center}
\caption{Memory topology of the Xeon192 SGI UV2000 machine.}
\label{fig:idchire}
\end{figure}

Figure \ref{fig:idchire} shows the Xeon192 machine memory topology.
NUMA nodes are organized by pairs of nodes connected together through Intel QuickPath Interconnect.
These pairs can communicate together through a proprietary fabric called NUMALink6, represented by the edges on the graph.
In other words, taking node $n_0$ as example :
\begin{itemize}
\item node $n_0$ communicates with node $n_1$ through Intel QPI ;
\item node $n_0$ is one hop away from node $n_4$ \emph{(e.g. communications between node $n_0$ et node $n_4$ cross one NUMALink6 memory controller)};
\item node $n_0$ is two hops away from node $n_{20}$ \emph{(e.g. communications between node $n_0$ et node $n_{20}$ cross two NUMALink6 memory controllers)};
\end{itemize}

Table \ref{tab:idchire} shows the distances advertised by the hwloc library~\cite{DBLP:conf/pdp/BroquedisCMFGMTN10} that represent factors on the communication time, communicating from node $n_0$ to any other node of the Xeon192 machine, whether the node is a peer (node $n_1$ in this case), is located one hop away (nodes $n_2$ to $n_9$, $n_{12}$, $n_{13}$, $n_{16}$ and $n_{17}$) or is located two hops away (nodes $n_{10}$, $n_{11}$, $n_{14}$, $n_{15}$ and $n_{18}$ to $n_{23}$). These factors, which have been correlated with experimental values on both bandwidth and latency by Pilla et al.~\cite{pilla:tel-00981136}, show how much impact the architecture topology may have on a parallel application performance.

\vspace{-0.5cm}

\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\begin{table}
\caption{NUMA distances from node 0 advertised by the hwloc library on Xeon192.}
\begin{center}
\begin{tabular}{C{3cm} C{2cm} C{2cm} C{2cm} C{2cm}}
    \toprule[0.15em]
		\emph{NUMA nodes location} & \emph{local} & \emph{peer} & \emph{one hop away} & \emph{two hops away} \\
    \midrule[0.1em]
	hwloc distances & 1.0 & 5.0 & 6.5 & 7.9 \\
    \bottomrule[0.15em]
\end{tabular}
\end{center}
\label{tab:idchire}
\end{table}

\vspace{-1cm}

\subsection{Software background}

To exploit such large-scale shared memory architectures, the application programmer needs :
\begin{enumerate}
\item to express massive fine grain parallelism to get the most out of the numerous processing units of the platform ;
\item to control the execution of the application, especially the way computations and data are distributed over the platform, to prevent the NUMA design to have a negative impact on the overall application performance.
\end{enumerate}

Task-based parallel programming environments provide ways of expressing fine grain parallelism that can be dynamically assigned to processors at runtime.
OpenMP~\cite{openmp40}, the de-facto standard for shared-memory parallel programming, supports task parallelism with dependencies since revision 4.0.

\subsubsection{A glimpse at OpenMP tasking}

An OpenMP \emph{task} can be seen as an independant \emph{unit of work} an OpenMP thread can execute.
Tasks can be created by an OpenMP thread and executed by any thread of the same parallel region. 
As managing tasks at runtime is way cheaper than creating and synchronizing threads, the application programmer can take the parallelization of its application further, as he can now consider portions of code that were too fine grain to be parallelized using only threads.
The synchronization of OpenMP 3.0 tasks is performed thanks to the |taskwait| keyword that waits for the completion of all the tasks generated from the current OpenMP parallel region.
On one hand, the application programmer is responsible for creating and synchronizing OpenMP tasks explicitly. 
On the other hand, the runtime system is in charge of assigning tasks to threads during the application execution, considering the underlying system state.

OpenMP 4.0 pushes the concept of task further introducing the |depend| keyword to specify the access mode of each shared variable a task will access during its execution. 
Access modes can be set to either |in|, |out| or |inout| whether the corresponding variable is respectively read as input, written as output or both read and written by the considered task. 
This information is then processed by the underlying runtime system to decide whether a task is ready for execution or should first wait for the completion of other ones.

Listing 1.1 shows the implementation of a blocked matrix multiplication. It is
taken from the official OpenMP 4.0 examples~\footnote{available at http://www.openmp.org}.
Running this |depend| version of the program leads to the creation of a dependency graph.
A task created on line 7 can thus be executed \emph{as soon as} the data they access has been updated by its predecessors in this graph.
When an OpenMP thread turns idle, the runtime system browses this graph to decide which task should be executed next, according to tasks' current state and resolved dependencies.

Task dependency support comes with several benefits. 
First, task dependencies involve decentralized, selective synchronization operations that should scale better than the broad-range taskwait-based approaches.
In some situations, this way of programming unlocks more valid execution scenarios than explicitly synchronized tasks, which provides the runtime system with many more valid task schedules to choose from. 
As an added benefit, information about task dependencies also enables the runtime system to optimize further, such as improving task and data placement on NUMA systems, like detailed in this paper.

\vspace{-0.5cm}

\begin{figure}[htbp]
\begin{lstlisting}[caption=Blocked matrix multiplication with OpenMP task dependencies,frame=tlrb,style=smaller,label=lst:MM-deps]{lst:MM-deps}
// Assume BS divides N perfectly
void matmul_depend(int N, int BS, float A[N][N], float B[N][N], float C[N][N]) {
     int i, j, k, ii, jj, kk;
     for (i = 0; i < N; i+=BS) {
          for (j = 0; j < N; j+=BS) {
               for (k = 0; k < N; k+=BS) {
                    #pragma omp task depend (in: A[i:BS][k:BS], B[k:BS][j:BS]) \
                         depend (inout: C[i:BS][j:BS])
                    for (ii = i; ii < i+BS; ii++)
                         for (jj = j; jj < j+BS; jj++)
                              for (kk = k; kk < k+BS; kk++)
                                   C[ii][jj] = C[ii][jj] + A[ii][kk] * B[kk][jj];
               }
          }
     }
}\end{lstlisting}
\end{figure}

\vspace{-1cm}

\subsubsection{The way we execute task-based applications}

Most task-based programming environments rely on a work-stealing execution model, originally introduced in Cilk~\cite{cilk5}. 
Work-stealing is indeed often considered when it comes to dynamically balance the work load among processing units. 
The work-stealing principle can be summarized as follows. 
An idle thread, called a thief, initiates a steal request to a randomly selected victim. 
On reply, the thief receives a copy of one ready task, leaving the original task marked as stolen.
Coherency between a thief and its victim is ensured by a variant of Cilk's T.H.E protocol, also described in \cite{cilk5}.

The runtime system we develop, called \kaapi, also implements the work-stealing execution model to execute OpenMP task-based applications.
The runtime creates a system thread, called a \emph{kproc}, for each processing unit to be used.
On a NUMA multicore machine, a processing unit is a core.
A kproc creates tasks and pushes them on its own work queue, which is implemented as a stack.
The enqueue operation is very fast : it takes around ten cycles on modern x86/64 processors~\cite{libkomp}.
As in Cilk, a  running \kaapi task can create children tasks. 
During the execution of a task-based application, if a kproc encounters a task that is marked as stolen, it suspends its execution and switches to
the work stealing scheduler that waits for dependencies to be met before
resuming the kproc execution. 

To find a ready task, a thief kproc iterates through the victim's queue from
the least recently pushed task to the most recently one and it computes true
data-flow dependencies for each task. 
%False dependencies are resolved through variables renaming. 
The iteration stops as soon as the kproc finds a ready task on the victim's queue.

In addition, \kaapi promotes a cooperative way of stealing tasks,
enabling multiple steal requests to the same processor to be served at once~\cite{DBLP:conf/europar/TchiboukdjianDGMR10}.
In practice, if the victim kproc is in a stealing state, the thief aborts the steal request and selects another victim randomly.
The \kaapi cooperative stealing algorithm divides the work into $k+1$ pieces when $k$ steal requests target the same processor.

\subsection{Summary}
Task-based programming environments stand as a promising candidate to exploit large-scale NUMA machines.
They give the application programmer the ability to generate the massive fine-grain parallelism that is desperately needed to occupy every processing unit of such architectures.
The work-stealing execution model they rely on also fill the need for balancing the application workload in a dynamic way, both tackling applications with irregular workloads and improving the overlapping of non-uniform memory access with actual computation.
However, exploiting NUMA architectures at their full potential requires some control on the way tasks and data are distributed over the platform, in order to avoid NUMA-related performance penalties. 
Preserving data locality while guaranteeing dynamic load balancing at runtime imposes an embarrassing dilemma to the runtime system that has a direct impact on :
\begin{itemize}   
\item the way data is allocated ;
\item the way ready tasks are originally assigned to processors ;
\item the way the scheduler browses the architecture topology to find a processor to steal tasks from.
\end{itemize}

The following of the paper study all these aspects and how they impact on each other, and more generally how they affect the overall application performance.

\section{Using OpenMP tasks dependencies to improve tasks and data
  placement on NUMA machines}

In this section, we describe how the runtime system can have a positive impact on the application execution using the information provided by data dependencies.
The following sections describe the way we adapted the behavior of the runtime system to control data placement during the initialization phase, when data will be allocated and accessed for the first time, and how we modified the way tasks that perform the actual computations are dynamically assigned to processors while maximizing data locality.


\subsection{Inside the \kaapi task-based runtime system}

This section describes some of the key internal structures and mecanisms of the \kaapi runtime system.

\subsubsection{The way \kaapi models the architecture}

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.65]{./figures/topology.pdf}
\caption{Processors and Nodes representation}
\label{fig:detail-topology}
\end{figure}

\kaapi sees the architecture topology as a hierarchy of \verb/places/.
A \verb/place/ is a list of tasks associated to a subset of the machine processing units.
\kaapi's places are very similar to the notion of \emph{shepherd} introduced in \cite{DBLP:journals/ijhpca/OlivierPWSP12}, or ForestGOMP's \emph{runqueues}~\cite{BroFurGogWacNam10IJPP}.
\kaapi most of the time only considers two levels of places : node-level places, which are bound to the set of processors contained in a NUMA node, and processor-level places, which are bound to a single processor of the platform.
This way, at the processor level one \verb/place/ is associated to each of the physical cores, and
at the NUMA node level one \verb/place/ is associated to each of the NUMA nodes, like represented in
Figure \ref{fig:detail-topology}.


\subsubsection{The way \kaapi enables ready tasks and steal them}
FIXME : doit on paler de la différence pop/steal ? Genre ça doit être plus cher de
faire un steal mais il faudrait l'expliquer.

When a processor becomes idle, the runtime system browses the topology to find a place from which stealing a task.
There are many ways to do so, implemented in different \emph{selection strategies} called \verb/WSselect/, for \emph{work-stealing select} in the remaining of the paper.
Once a place is selected, the processor will take a ready task from its queue.

The completion of a task most of the time unlocks the execution of some of its children in the dependency graph.
This means marking them as ready for execution and pushing at least one of them to a place.
Once again, there are many ways of selecting the place where to push ready tasks, implemented in strategies we refer to as \verb/WSpush/, for \emph{work-stealing push}.


\subsection{Initial data distribution/placement}

Taking into account data locality is a very important point when one want to get performance
on large NUMA architecture.
Therefore particular care should be given to the initial data distribution over the machine.

Multiple works have been written on this, using \emph{numactl} is an easy solution which has
been covered~\cite{DBLP:journals/corr/abs-1101-0093}, \cite{Olivier:2012:CMW:2388996.2389085},
but it's often not enough to guarantee performances, as data locality is not likely
to be tight enough to the part of the application which will use it. Olivier et al.~\cite{Olivier:2012:CMW:2388996.2389085}
have shown that specifying affinity for initialization tasks can lead to
huge improvement over locality oblivious techniques.

We want to be able to distinguish the initialization parallel region from the computational
parallel region, therefore we added runtime calls to indicate to the runtime that
we enter and leave initialization.
Inside the initialization region the runtime will use a user-defined task placement
policy : it will ensure the data distribution because of the first-touch policy
on memory pages.
It is important to note that unlike \emph{numactl}, the memory placement is
not based on pages but on data touched by the initialization tasks.

We have implemented several strategies :

\begin{itemize}
  \item Cyclic numa : data are distributed in a round-robin fashion among the nodes.
  \item Rand numa : data are distributed at random among the nodes.
  \item FIXME introduire l'affinité ? Default numa : uses application's affinity
\end{itemize}



\subsection{Initial task placement when ready : WSpush strategies}

When a task finishes its execution, the processor responsible for its execution
will look for the tasks depending on the eventual data that were just produced.
If a task has all of its dependencies met, it should be pushed to a place
so that it can be executed.

This task placement should not be neglected, and we have multiple possible
strategies for this.

\subsubsection{Data independent}

We implemented the following strategies, that are data oblivious :
\begin{itemize}
  \item \verb/local/ : in this strategy the processor will push ready tasks to
    its own place.
  \item \verb/numa/ : the processor pushes to its NUMA node's place.
\end{itemize}

\subsubsection{Using data dependencies}

We can also use the information present in the task's data dependencies,
therefore we additionally implemented the following strategies :

FIXME : citer HPF ?

\begin{itemize}
  \item \verb/Wnuma/ : we first identify an \verb/out/ dependency, then we
    look for the NUMA node where it is allocated, and we push the task to this
    node's place.
  \item \verb/Whws/ : this is the same as the above strategy, except that if
    the data is allocated on our own node, we push the task to our processor's place.
\end{itemize}


\subsection{Dynamic load balancing using workstealing : WSselect strategies}

Another critical step when using a workstealing approach is the selection of
the victim we want to steal from.
For this we should take into account the machine hierarchy.

The following strategies are similar to those studied in~\cite{DBLP:journals/ijhpca/OlivierPWSP12}
and distinguish two levels of hierarchy.

\begin{itemize}
  \item \verb/rand/ : Select a random processor's place
  \item \verb/numa/ : Select a random NUMA node's place
\end{itemize}

We additionally implemented several strategies mixing both level of hierarchy,
described below.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.7]{figures/strategies.pdf}
\caption{Illustration of the "hws\_P\_N" strategy}
\label{fig:detail-strategy}
\end{figure}
\begin{itemize}
  \item \verb/hws_P_N/ : \verb/P_N/ stands for "Processor-Node". This strategy is
    illustrated in figure \ref{fig:detail-strategy}.
    First of all we select the processor's place (1), upon failure we select
    another place in the following order : we select one of the neighbour
    processors (2a); when all the neighbours have been visited, we select the
    local NUMA place (2b); we continue by looking at all the processors' place
    from a random remote node (3a) and its NUMA place (3b).
  \item \verb/hws_N_P/ : This strategy is similar, except we always look at the
    NUMA place before looking at the processors' place (in figure \ref{fig:detail-strategy}
    it means we switch step 2a with 2b, and step 3a with 3b).
  \item \verb/hws_P/ : In this strategy the stealer will visit only the
    processors' places and its own NUMA place. In figure \ref{fig:detail-strategy} it means we do
    every steps but step 3b.
  \item \verb/hws_N/ : In this strategy the stealer will visit only NUMA places
    and its neighbour. In figure \ref{fig:detail-strategy} it means we do
    every steps but step 3a.
\end{itemize}


\subsubsection{Restricting steal to local nodes}

This idea has already been described in~\cite{Olivier:2012:CMW:2388996.2389085} :
we can prevent processors to steal from other NUMA nodes to improve data locality.
We implemented this to be used conjointly with any strategy, on figure \ref{fig:detail-strategy}
it would simply means we don't go through steps 3a and 3b.
We use \verb/strict/ to refer to strategies which restrict steals, and \verb/loose/
to refer to those which don't.


\section{Evaluation}

TODO version de tout, tag dépot etc...

Evaluating distribution :

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.6]{figures/graph_distrib.pdf}
\caption{Evaluating several data distributions}
\label{fig:eval-distrib}
\end{figure}

Conclusion générale : gérer la distribution c'est indispensable, après cyclicnuma a l'air suffisant.


Evaluating "strictness" :
FIXME mercredi utiliser hws\_N plutôt que hws\_P
FIXME courbes avec numactl

Raison : beaucoup de tâche écrive sur la même partie du dernier bloc, il y a plus de tâche que de coeurs qui sont restreintes sur ce coeur,
c'est du coup moins rentable d'empêcher le vol...
Dans l'avenir des travaux : regarder sur d'autres applis.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.6]{figures/graph_strict.pdf}
\caption{Evaluating impact of strictness}
\label{fig:eval-strict}
\end{figure}

Conclusion : le strict c'est nul.

Evaluating all the custom strategies :

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.6]{figures/graph_all_strat.pdf}
\caption{Evaluating all strategies}
\label{fig:eval-all-strat}
\end{figure}

Conclusion : OCR c'est bien, peu de différences entre N/P et P/N, P et N sont un peu en dessous.

Evaluating selected strategies on multiple sizes :

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.6]{figures/graph_details_strat.pdf}
\caption{Evaluating specific strategies on multiple sizes}
\label{fig:eval-strat-sizes}
\end{figure}

Conclusion : pas besoin de s'embeter pour les petites tailles.
FIXME : tester sur des plus grosses

\section{Related work}
\begin{itemize}
\item Ce qu'on fait, c'est mieux que Bronis parce qu'on a la vision
  (et le contrôle indirect) du placement des données. Encore faut-il
  le démontrer! Une coucourbe hwsnumafirst.numa.* (TODO : à vérifier)
  devrait suffire à montrer que ce qu'on fait va plus loin (en termes
  de perfs) que ce qu'ils proposaient.
\item La question qui tue (Thierry ?) : si on est capable de contrôler le
  placement des tâches dans les kprocs, et de contrôler le placement
  mémoire, pourquoi on ne fait pas de placement statique à la SCOTCH
  ou autre?
\end{itemize}

CEA~\cite{DBLP:conf/europar/Clet-OrtegaCP14},
Bronis~\cite{DBLP:journals/sp/OlivierSSP13} TODO: c'est peut-être pas
la bonne ref pour Bronis, OpenMP task-scheduling et
NUMA~\cite{DBLP:journals/corr/Tahan14}, solution au niveau appli
(terboven)~\cite{DBLP:conf/europar/TerbovenSCM12}, Olivier et al.~\cite{DBLP:journals/ijhpca/OlivierPWSP12}.


\section*{Acknowledgments}

This work has been partially supported by the IRSES2011-295217
HPC-GA Project.

  \small \bibliographystyle{Styles/iplain}
%\nocite{*}
  \bibliography{Bib/paper}

\end{document}
