\documentclass{Styles/llncs}
%\documentclass[12pt,letterpaper]{article}
\usepackage{times}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{boxedminipage}
\usepackage{xspace}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{verbatim}
\usepackage{parcolumns}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%Prevents floating item to "jump" between sections
\usepackage[section]{placeins}
\usepackage{booktabs}
\usepackage{tkz-graph}
\usepackage{setspace}
\setstretch{0.95}
\newcommand{\arevoir}[1]{#1}

\newcommand{\kaapi}{\textsc{\mbox{Xkaapi}}\xspace}

\newcommand{\libXKOMP}{\textsc{libKOMP}\xspace}

\usepackage{xcolor}
\usepackage{todonotes}
\usepackage[color,leftbars]{changebar}

\newcommand{\TG}[1]{{\color{red}\bfseries TG: #1}}
\newcommand{\PV}[1]{{\color{red}\bfseries PV: #1}}


\newcommand{\cfsect}[1]{(\textit{cf.} section~\ref{#1})}
\newcommand{\cfsectpage}[1]{(\textit{cf.} section~\ref{#1}, page~\pageref{#1})}
\providecommand{\figureref}[1]{\figname~\ref{#1}}
\providecommand{\cftab}[1]{(\textit{cf.} tableau~\ref{#1})}
\newcommand{\cmd}[1]{{\upshape\texttt{\symbol{"5C}#1}}}

\newenvironment{remarque}
{\description \item[Remarque:] \ \slshape}
{\enddescription}

\makeatletter
\newbox\sf@box
\newenvironment{SubFloat}[2][]%
  {\def\sf@one{#1}%
   \def\sf@two{#2}%
   \setbox\sf@box\hbox
     \bgroup}%
  { \egroup
   \ifx\@empty\sf@two\@empty\relax
     \def\sf@two{\@empty}
   \fi
   \ifx\@empty\sf@one\@empty\relax
     \subfloat[\sf@two]{\box\sf@box}%
   \else
     \subfloat[\sf@one][\sf@two]{\box\sf@box}%
   \fi}
\makeatother
\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}
\renewcommand{\ttdefault}{pcr}
\lstset{
	tabsize=4,
%	frame=single,
	breaklines=true,
	basicstyle=\ttfamily,
	frame=tb,
	framerule=0.2pt,
%	frameround={tttt},
	showstringspaces=false,
	language=c,
%	linewidth=0.95\textwidth,
	keywordstyle=\color{black}\bfseries,
%	keywordstyle=\color{blue},
	commentstyle=\color{OliveGreen},
	stringstyle=\color{red}\itshape,
	inputencoding=utf8/latin1,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
% OMP define
emph={\#,pragma, taskwait, omp, task, depend}, emphstyle=\color{RoyalBlue}\bfseries,
emph={[2]in,inout,out,cw}, emphstyle={[2]\color{BrickRed}\bfseries},
emph={[3]tied,untied,shared}, emphstyle={[3]\color{Gray}\bfseries},
emph={[4]lu0,fwd,bdiv,bmod}, emphstyle={[4]\color{DarkGreen}\bfseries},
emph={[5]cw}, emphstyle={[5]\color{DarkViolet}\bfseries},
    %moredelim=**[is][\only<3>{\color{red}}]{@}{@},
}
\lstdefinestyle{smaller}{basicstyle=\scriptsize\ttfamily}
\lstMakeShortInline|

\newcommand{\benchs}{KASTORS }

\sloppy

\begin{document}

\title{Using data dependencies to improve task-based scheduling strategies on NUMA architectures}
\author{
  Philippe Virouleau \and François Broquedis \and Thierry Gautier \and Fabrice Rastello
 \vspace*{-1ex}}
\institute{
   Inria,
   Univ. Grenoble Alpes, LIG, CNRS, Grenoble, France,
   Grenoble Institute of Technology,
   LIP, ENS de Lyon\\
   \email{firstname.lastname@inria.fr}\\
   \email{thierry.gautier@inrialpes.fr}\\
}
\date{}
\maketitle

\begin{abstract}
  \vspace*{-5ex} The OpenMP 4.0 specifications extended the task parallelism
  constructs by providing the user a more flexible way to synchronize tasks :
  data dependencies between tasks.
  Using such an approach allows both the compiler and the runtime system
  to know exactly which data are read or written by a given task, and how these
  data will be used through the program lifetime.  Data placement and scheduling
  strategies have a significant impact on performances when considering
  NUMA architectures.  While numerous papers focus on these topics, none of them
  has made extensive use of the information available through dependencies.
  One can use this information to modify the behavior of the application at
  several levels : during initialization to control data placement,
  and during the application runtime to dynamically control both the preferred
  task placement and stealing strategy, depending on the topology.
  This paper introduces several heuristics for these strategies and their
  implementations in our OpenMP runtime XKaapi.
  We also evaluate their performances by using several linear algebra
  applications, executed on a 192 cores NUMA machine. We finally
  compare them to strategies presented previously by related works.

\smallskip
  \noindent\textbf{Keywords:}
  \emph{
    OpenMP, task dependencies, benchmark, runtime systems, NUMA, Kaapi, scheduling, work-stealing
  }
\end{abstract}


\section{Introduction}

While non-uniform memory access (NUMA) architectures have been a popular way of designing large-scale shared memory machines for many years, exploiting them at their full potential remains challenging.
On such architectures, the memory is split into several NUMA nodes and
both bandwidth and latency depend on which processor accesses specific data :
accessing memory allocated locally is most of the time faster than accessing data allocated to remotely-located NUMA nodes.
Controlling data locality over the application lifetime is one of the key steps to
achieve both good performance and scalability on these architectures.

Task-based parallel programming environments like OpenMP have become very popular when it comes to program shared memory machines with hundreds of cores. Indeed, they offer ways of expressing massive fine-grain parallelism with a relatively low overhead. Most of them also come with facilities to dynamically perform load balancing of tasks over the processors. Even if such characteristics fill the need of generating more and more parallelism out of parallel applications, standard parallel programming environments still not explicitly address the problem of data locality on NUMA systems.

The runtime system plays a central role in the execution of a task-based parallel application.
For example, it is responsible for assigning ready tasks to the target platforms's processors. 
It is also in charge of performing load balancing when a processor idles.
Both these decisions should take the architecture topology into account in order to avoir NUMA-related performance penalties on the overall application performance.

The recent addition of data dependencies to the OpenMP tasking model provides the
runtime system with very precise information about which part of an application accesses which variables. 
Thanks to these dependencies, the runtime system knows which memory areas are read or written by which task. As shown in this paper, the task scheduler can rely on this information when assigning tasks to processors to implement NUMA-aware strategies.

This paper describes several of these strategies we implemented inside the \kaapi runtime system.
We identified three major steps in the task scheduler workflow that may have an impact on parallel applications on NUMA systems : the data distribution, the assignment of ready tasks to the processors and the way the task scheduler browses the architecture topology to perform load balancing.
This paper describes and evaluates them, showing how they impact the application performance on a 192-core NUMA machine.
We also compare them to state-of-the-art task scheduling strategies taken from related works and implemented within \kaapi.

The layout of this paper falls into six sections as follows.
In Section~\ref{sec:background}, we first give some background about NUMA architectures
and the task programming model with data dependencies.
We  then describe in Section~\ref{sec:contributions} the ideas, strategies and implementation details that we used to improve the runtime performances
for these applications. Section~\ref{sec:performances-evaluation} is devoted to the presentation performances evaluation.
We eventually present some related works in Section~\ref{sec:related-work}
before concluding.

%\subsection{Summary}
%Task-based programming environments stand as a promising candidate to exploit large-scale NUMA machines.
%They give the application programmer the ability to generate the massive fine-grain parallelism that is desperately needed to occupy every processing unit of such architectures.
%The work-stealing execution model they rely on also fill the need for balancing the application workload in a dynamic way, both tackling applications with irregular workloads and improving the overlapping of non-uniform memory access with actual computation.
%However, exploiting NUMA architectures at their full potential requires some control on the way tasks and data are distributed over the platform, in order to avoid NUMA-related performance penalties. 
%Preserving data locality while guaranteeing dynamic load balancing at runtime imposes an embarrassing dilemma to the runtime system that has a direct impact on :
%\begin{itemize}   
%\item the way data are allocated ;
%\item the way ready tasks are originally assigned to processors ;
%\item the way the scheduler browses the architecture topology to find a processor to steal tasks from.
%\end{itemize}
%
%The following of the paper study all these aspects and how they impact on each other, and more generally how they affect the overall application performance.
%\TG{Section un peu longue, à merger avec l'intro: typiquement cette dernière phrase mérite d'être lue avant ?}

%HPC architectures evolved so rapidly that it is now common to build
%shared-memory configurations with several dozens of cores.  The recent
%appearance of technologies such as the Intel Xeon Phi co-processor makes
%affordable configurations with thousands of cores a not-so-far
%reality.  Efficiently programming such large-scale platforms requires
%to express more and more fine-grain parallelism.

%Standard parallel programming environments such as OpenMP have evolved
%to address this requirement, introducing new ways of designing highly
%parallel programs. Extending OpenMP to support task parallelism stands
%as a first step to improve the scalability of OpenMP applications on
%large-scale platforms. Indeed, task parallelism usually comes with lower
%runtime-related overhead than thread-based approaches, allowing OpenMP
%programmers to create a large amount of tasks at low cost. Task
%parallelism also promotes the runtime system to a central role, as
%having more units of work to execute requires smarter scheduling
%decisions and load balancing capabilities.

%OpenMP was recently extended to support task dependencies. Instead of
%explicitly synchronizing all the tasks of a parallel region at once, the
%application programmer can now specify a list of variables a task will
%read as input or write as output instead. This information is
%transmitted to the task scheduling runtime system. The runtime then marks
%a task as ready for execution only once all its dependencies have been
%resolved. Dependencies therefore provide a way to define finer
%synchronizations between tasks, able to scale better than global
%synchronizations on large-scale platforms. Dependencies also give the
%runtime system more options to efficiently schedule tasks, as these
%become ready for execution as soon as the data they access has been
%updated.

%As promising as it looks however, any new feature needs proper
%evaluation to encourage application programmers to embrace it. While
%several compilers and runtime systems are now beginning to support
%OpenMP~4.0 task dependencies, no benchmark suite currently exists to
%evaluate their respective benefits and compare them to traditional task
%parallelism.

%This paper highlights two major contributions. We first introduce a
%new benchmark suite to experiment with OpenMP~4.0 task dependencies. We
%present performance results for both the GCC/libGOMP and the
%CLANG\footnote{Intel branch with support for OpenMP:
%\url{http://clang-omp.github.io/}}/libIOMP compilers and their runtime
%systems, comparing kernels
%involving either dependent or independent tasks. Secondly, we
%comment on the issues we met while implementing these benchmarks along
%the lines of current 4.0 revision of the OpenMP specification. Building
%on this experience, we contribute some extension proposals to the
%existing OpenMP specification, to improve the expressiveness of the
%task dependency support.

%The remainder of this paper is organized as follows. Section
%\ref{sec:omp-deps} describes the task dependency programming model in
%OpenMP~4.0. It then analyzes the strategies adopted by GCC/libGOMP and
%CLANG/libIOMP to implement this model. Section \ref{sec:benchs}
%introduces the \benchs benchmark suite we have designed to evaluate
%OpenMP~4.0's task model implementations. Section \ref{sec:perfs}
%presents the performance results of \benchs using two different hardware
%configurations. We identify and discuss practical issues with the
%current OpenMP specification, and we propose extensions in section
%\ref{sec:extensions} to address these issues. We finally present some
%related works in section \ref{sec:related-work} before concluding.

\section{Background}
\label{sec:background}
\subsection{Hardware background}
\label{sec:hardware}
Most of nowadays parallel shared memory architectures are built according to a NUMA~\footnote{Non-Uniform Memory Access} design where the memory is physically split into several banks attached to processors.
Many vendors assemble these banks in a hierarchical way, thus building shared memory machines embedding several hundreds of cores.
Exploiting such architectures at their full potential requires a fine control of the execution of a parallel application, as accessing local memory is most of the time faster than accessing memory stored in a memory bank attached to a remote processor.

The machine we experimented on is an SGI UV2000 platform made of 24 NUMA nodes.
Each NUMA node holds an 8-core Intel Xeon E5-4640 CPU for a total of 192 cores.
We refer to this machine as Intel192 in the paper. 
%\begin{figure}
%\begin{center}
%\begin{tikzpicture}[scale=0.6]
%\tikzset{VertexStyle/.append style = {minimum size = 30pt, inner sep = 0pt}}
%
%\Vertex[x=0, y=0, L=$n_{14,15}$]{n0}
%\Vertex[x=0, y=2, L=$n_{22,23}$]{n4}
%\Vertex[x=0, y=4, L=$n_{8,9}$]{n8}
%\Vertex[x=0, y=6, L=$n_{12,13}$]{n12}
%\Vertex[x=2, y=0, L=$n_{10,11}$]{n1}
%\Vertex[x=2, y=2, L=$n_{6,7}$]{n5}
%\Vertex[x=2, y=4, L=$n_{0,1}$]{n9}
%\Vertex[x=2, y=6, L=$n_{16,17}$]{n13}
%
%\Vertex[x=4, y=0, L=$n_{20,21}$]{n2}
%\Vertex[x=4, y=2, L=$n_{4,5}$]{n6}
%\Vertex[x=4, y=4, L=$n_{2,3}$]{n10}
%\Vertex[x=4, y=6, L=$n_{10,11}$]{n14}
%\Vertex[x=6, y=0, L=$n_{12,13}$]{n3}
%\Vertex[x=6, y=2, L=$n_{8,9}$]{n7}
%\Vertex[x=6, y=4, L=$n_{18,19}$]{n11}
%\Vertex[x=6, y=6, L=$n_{14,15}$]{n15}
%
%\Edge(n0)(n5)
%\Edge(n4)(n5)
%\Edge(n1)(n5)
%
%\Edge(n13)(n9)
%\Edge(n12)(n9)
%\Edge(n8)(n9)
%
%\Edge(n15)(n10)
%\Edge(n14)(n10)
%\Edge(n11)(n10)
%
%\Edge(n3)(n6)
%\Edge(n2)(n6)
%\Edge(n7)(n6)
%
%\Edge(n9)(n5)
%\Edge(n9)(n10)
%\Edge(n5)(n10)
%\Edge(n9)(n6)
%\Edge(n5)(n6)
%\Edge(n10)(n6)
%
%\end{tikzpicture}
%\end{center}
%\caption{Memory topology of the Intel192 SGI UV2000 machine. \TG{Je supprimerai la figure.}}
%\label{fig:idchire}
%\end{figure}
The memory topology is organized by pairs of NUMA nodes connected together through Intel QuickPath Interconnect.
These pairs can communicate together through a proprietary fabric called NUMALink6 with up to two hops.
%, represented by the edges of the graph.In other words, taking node $n_0$ as example :
%\begin{itemize}
%\item node $n_0$ communicates with node $n_1$ through Intel QPI ;
%\item node $n_0$ is one hop away from node $n_4$ \emph{(e.g. communications between node $n_0$ et node $n_4$ cross one NUMALink6 memory controller)};
%\item node $n_0$ is two hops away from node $n_{20}$ \emph{(e.g. communications between node $n_0$ et node $n_{20}$ cross two NUMALink6 memory controllers)};
%\end{itemize}

Table \ref{tab:idchire} shows the distances advertised by the hwloc library~\cite{DBLP:conf/pdp/BroquedisCMFGMTN10} that represent factors on the communication time, communicating from one core to different NUMA nodes. Distances named \textit{local} and \textit{peer} are through Intel QuickPath only.
%node $n_0$ to any other node of the Intel192 machine, whether the node is a peer (node $n_1$ in this case), is located one hop away (nodes $n_2$ to $n_9$, $n_{12}$, $n_{13}$, $n_{16}$ and $n_{17}$) or is located two hops away (nodes $n_{10}$, $n_{11}$, $n_{14}$, $n_{15}$ and $n_{18}$ to $n_{23}$). These factors, which have been correlated with experimental values on both bandwidth and latency by Pilla et al.~\cite{pilla:tel-00981136}, show how much impact the architecture topology may have on a parallel application performance.
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\begin{table}
\caption{NUMA distances from node 0 advertised by the hwloc library on Intel192.}
\begin{center}
\begin{tabular}{C{3cm} C{2cm} C{2cm} C{2cm} C{2cm}}
    \toprule[0.15em]
		\emph{NUMA nodes location} & \emph{local} & \emph{peer} & \emph{one hop away} & \emph{two hops away} \\
    \midrule[0.1em]
	hwloc distances & 1.0 & 5.0 & 6.5 & 7.9 \\
    \bottomrule[0.15em]
\end{tabular}
\end{center}
\label{tab:idchire}
\end{table}

\subsection{Software background}

To exploit  large-scale shared memory architectures, the application programmer needs:
\begin{enumerate}
\item to express massive fine grain parallelism to get the most out of the numerous processing units of the platform ;
\item to control the execution of the application, especially the way computations and data are distributed over the platform, to prevent the NUMA design to have a negative impact on the overall application performance.
\end{enumerate}

Task-based parallel programming environments provide ways of expressing fine grain parallelism that can be dynamically assigned to processors at runtime.
OpenMP~\cite{openmp40}, the de-facto standard for shared-memory parallel programming, supports task parallelism with dependencies since revision 4.0.

\subsubsection{A glimpse at OpenMP tasking}

An OpenMP \emph{task} can be seen as an independent \emph{unit of work} an OpenMP thread can execute.
Tasks can be created by an OpenMP thread and executed by any thread of the same parallel region. 
As managing tasks at runtime is way cheaper than creating and synchronizing threads, the application programmer can take the parallelization of its application further, as he can now consider portions of code that were too fine grain to be parallelized using only threads.
The synchronization of OpenMP 3.0 tasks is performed thanks to the |taskwait| keyword that waits for the completion of all the tasks generated from the current OpenMP parallel region.
On one hand, the application programmer is responsible for creating and synchronizing OpenMP tasks explicitly. 
On the other hand, the runtime system is in charge of assigning tasks to threads during the application execution, considering the underlying system state.

OpenMP 4.0 pushes the concept of task further introducing the |depend| keyword to specify the access mode of each shared variable a task will access during its execution. 
Access modes can be set to either |in|, |out| or |inout| whether the corresponding variable is respectively read as input, written as output or both read and written by the considered task. 
This information is then processed by the underlying runtime system to decide whether a task is ready for execution or should first wait for the completion of other ones.

\subsubsection{KASTORS Benchmark suite}
Listing ~\ref{lulst} shows the implementation of a Cholesky factorization 
implemented with OpenMP task dependencies. 
This factorization algorithms come from PLASMA library and its very closed to real implementation in the KASTORS benchmark suite~\cite{virouleau:hal-01081974}.
Task dependencies support comes with several benefits. First, task
dependencies involve decentralized, selective synchronization operations
that should scale better than the broad-range taskwait-based approaches.
In some situations, this way of programming unlocks more valid execution
scenarios than explicitly synchronized tasks, which provides the runtime
system with many more valid task schedules to choose from. For example in the Cholesky facorization,
many instances of  the |dtrsm|, |dsyrk| and |dgem| BLAS computations can legally
run concurrently when using the version with task dependencies.

\begin{figure}[tbp]
\hrule
\begin{minipage}[t]{.43\textwidth}
\begin{lstlisting}[frame=none,style=smaller,showlines=true,label=lst:LU-deps]{lst:LU-deps1}
for (size_t k=0; k < NB; ++k) {
#pragma omp task shared(A) \
  depend(inout: A[k][k]) 
  dpotrf(NB,&A[k][k]);

  for (int m=k; m < NB; ++m) 
#pragma omp task shared(A)\
  depend(in: A[k][k]) \
  depend(inout: A[m][k]) 
    dtrsm(NB,&A[k][k],&A[m][k]);

\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}[t]{.485\textwidth}
\begin{lstlisting}[frame=none,style=smaller,label=lst:LU-deps,firstnumber=12]{lst:LU-deps2}
  for (int m=k; m < NB; ++m) {
#pragma omp task shared(A)\
  depend(in: A[m][k]) \
  depend(inout: A[m][m]) 
   dsyrk(NB,&A[m][k], &A[m][m]);

    for (int n=k; n < m; ++n)
#pragma omp task shared(A)\
  depend(in: A[m][k],A[n][k])\
  depend(inout: A[m][n]) 
     dgemm(NB,
       &A[m][k],&A[n][k],&A[m][n]);
  }
 }
\end{lstlisting}
\end{minipage}
\hrule
\caption{Cholesky factorization with OpenMP-4.0 task dependencies} \label{lulst}
\end{figure}

%\begin{figure}[tbp]
%\hrule
%\begin{minipage}[t]{.43\textwidth}
%\begin{lstlisting}[frame=none,style=smaller,showlines=true,label=lst:LU-deps]{lst:LU-deps1}
%for (k=0; k<NB; k++) {
%#pragma omp task untied shared(M)\
%    depend(inout: M[k*NB+k:BS*BS])
%  lu0(M[k*NB+k]);
%
%  for (j=k+1; j<NB; j++)
%#pragma omp task untied shared(M)\
%    depend(in: M[k*NB+k:BS*BS])\
%    depend(inout: M[k*NB+j:BS*BS])
%    fwd(M[k*NB+k], M[k*NB+j]);
%\end{lstlisting}
%\end{minipage}\hfill
%\begin{minipage}[t]{.485\textwidth}
%\begin{lstlisting}[frame=none,style=smaller,label=lst:LU-deps,firstnumber=11]{lst:LU-deps2}
%  for (i=k+1; i<NB; i++)
%#pragma omp task untied shared(M)\
%    depend(in: M[k*NB+k:BS*BS])\
%    depend(inout: M[i*NB+k:BS*BS])
%    bdiv(M[k*NB+k], M[i*NB+k]);
%
%  for (i=k+1; i<NB; i++)
%    for (j=k+1; j<NB; j++)
%#pragma omp task untied shared(M)\
%   depend(in: M[i*NB+k:BS*BS])\
%   depend(in: M[k*NB+j:BS*BS])\
%   depend(inout: M[i*NB+j:BS*BS])
%    bmod(M[i*NB+k],M[k*NB+j],M[i*NB+j]);
%}
%\end{lstlisting}
%\end{minipage}
%\hrule
%\caption{LU factorization with OpenMP-4.0 task dependencies} \label{lulst}
%\end{figure}

%Listing 1.1 shows the implementation of a blocked matrix multiplication. It is
%taken from the official OpenMP 4.0 examples~\footnote{available at http://www.openmp.org}.
%Running this |depend| version of the program leads to the creation of a dependency graph.
%A task created on line 7 can thus be executed \emph{as soon as} the data they access has been updated by its predecessors in this graph.
%When an OpenMP thread turns idle, the runtime system browses this graph to decide which task should be executed next, according to tasks' current state and resolved dependencies.
%
%Task dependency support comes with several benefits. 
%First, task dependencies involve decentralized, selective synchronization operations that should scale better than the broad-range taskwait-based approaches.
%In some situations, this way of programming unlocks more valid execution scenarios than explicitly synchronized tasks, which provides the runtime system with many more valid task schedules to choose from. 
%As an added benefit, information about task dependencies also enables the runtime system to optimize further, such as improving task and data placement on NUMA systems, like detailed in this paper.
%
%\vspace{-0.5cm}
%
%\begin{figure}[htbp]
%\begin{lstlisting}[caption=Blocked matrix multiplication with OpenMP task dependencies,frame=tlrb,style=smaller,label=lst:MM-deps]{lst:MM-deps}
%// Assume BS divides N perfectly
%void matmul_depend(int N, int BS, float A[N][N], float B[N][N], float C[N][N]) {
%     int i, j, k, ii, jj, kk;
%     for (i = 0; i < N; i+=BS) {
%          for (j = 0; j < N; j+=BS) {
%               for (k = 0; k < N; k+=BS) {
%                    #pragma omp task depend (in: A[i:BS][k:BS], B[k:BS][j:BS]) \
%                         depend (inout: C[i:BS][j:BS])
%                    for (ii = i; ii < i+BS; ii++)
%                         for (jj = j; jj < j+BS; jj++)
%                              for (kk = k; kk < k+BS; kk++)
%                                   C[ii][jj] = C[ii][jj] + A[ii][kk] * B[kk][jj];
%               }
%          }
%     }
%}
%\end{lstlisting}
%\end{figure}
%

\subsubsection{The way we execute task-based applications}

Most task-based programming environments rely on a work-stealing execution model, originally introduced in Cilk~\cite{cilk5}. 
Work-stealing is indeed often considered when it comes to dynamically balance the workload among processing units. 
The work-stealing principle can be summarized as follows. 
An idle thread, called a thief, initiates a steal request to a randomly selected victim. 
On reply, the thief receives a copy of one ready task, leaving the original task marked as stolen.
Coherency between a thief and its victim is ensured by a variant of Cilk's T.H.E protocol, also described in \cite{cilk5}.

The runtime system we develop, called \kaapi, also implements the work-stealing execution model to execute OpenMP task-based applications.
The runtime creates a system thread, called a \emph{kproc}, for each processing unit to be used.
On a NUMA multicore machine, a processing unit is a core.
A kproc creates tasks and pushes them on its own work queue, which is implemented as a stack.
The enqueue operation is very fast : it takes around ten cycles on modern x86/64 processors~\cite{libkomp}.
As in Cilk, a  running \kaapi task can create children tasks. 
%During the execution of a task-based application, if a kproc encounters a task that is marked as stolen, it suspends its execution and switches to
%the work stealing scheduler that waits for dependencies to be met before
%resuming the kproc execution. 
Depending on the number of tasks per thread, \kaapi implements two strategies to find a ready task.
If the number of tasks is lower than a threshold, \kaapi follows the Cilk's~\textit{work first principle} to report overhead
from the work to the critical pass. In that case, the thief iterates through the victim's stack queue from 
the least recently pushed task to the most recently one and it computes true
data-flow dependencies for each task until ready task is found. If the number of tasks is greater than the threshold, 
the data flow graph is built and the thief picks a tasks into the victim's list of ready task~\cite{Bleuse2014}.
By the nature of our benchmarks, this latter strategy is de facto selected and we developed new NUMA aware strategies.


%To find a ready task, a thief kproc iterates through the victim's queue from
%the least recently pushed task to the most recently one and it computes true
%data-flow dependencies for each task. 
%%False dependencies are resolved through variables renaming. 
%The iteration stops as soon as the kproc finds a ready task on the victim's queue.
%
%In addition, \kaapi promotes a cooperative way of stealing tasks,
%enabling multiple steal requests to the same processor to be served at once~\cite{DBLP:conf/europar/TchiboukdjianDGMR10}.
%In practice, if the victim kproc is in a stealing state, the thief aborts the steal request and selects another victim randomly.
%The \kaapi cooperative stealing algorithm divides the work into $k+1$ pieces when $k$ steal requests target the same processor.

%\subsection{Summary}
%Task-based programming environments stand as a promising candidate to exploit large-scale NUMA machines.
%They give the application programmer the ability to generate the massive fine-grain parallelism that is desperately needed to occupy every processing unit of such architectures.
%The work-stealing execution model they rely on also fill the need for balancing the application workload in a dynamic way, both tackling applications with irregular workloads and improving the overlapping of non-uniform memory access with actual computation.
%However, exploiting NUMA architectures at their full potential requires some control on the way tasks and data are distributed over the platform, in order to avoid NUMA-related performance penalties. 
%Preserving data locality while guaranteeing dynamic load balancing at runtime imposes an embarrassing dilemma to the runtime system that has a direct impact on :
%\begin{itemize}   
%\item the way data are allocated ;
%\item the way ready tasks are originally assigned to processors ;
%\item the way the scheduler browses the architecture topology to find a processor to steal tasks from.
%\end{itemize}
%
%The following of the paper study all these aspects and how they impact on each other, and more generally how they affect the overall application performance.
%\TG{Section un peu longue, à merger avec l'intro: typiquement cette dernière phrase mérite d'être lue avant ?}

\section{Using OpenMP tasks dependencies to improve tasks and data
  placement on NUMA machines}
\label{sec:contributions}

In this section, we describe how the runtime system can have a positive impact on the application execution using the information provided by data dependencies.
The following sections describe the way we adapted the behavior of the runtime system to control data placement during the initialization phase, when data will be allocated and accessed for the first time, and how we modified the way tasks that perform the actual computations are dynamically assigned to processors while maximizing data locality.


\subsection{Inside the \kaapi task-based runtime system}

This section describes some of the key internal structures and mechanisms of the \kaapi runtime system.

\subsubsection{The way \kaapi models the architecture.}

%\begin{figure}[t]
%  \centering
%  \includegraphics[scale=0.5]{./figures/topology.pdf}
%\caption{Processors and Nodes representation}
%\label{fig:detail-topology}
%\end{figure}

\kaapi sees the architecture topology as a hierarchy of \verb/places/.
A \verb/place/ is a list of tasks associated with a subset of the machine processing units.
\kaapi's places are very similar to the notion of \emph{shepherd} introduced in \cite{DBLP:journals/ijhpca/OlivierPWSP12}, or ForestGOMP's \emph{runqueues}~\cite{BroFurGogWacNam10IJPP}.
\kaapi most of the time only considers two levels of places : node-level places, which are bound to the set of processors contained in a NUMA node, and processor-level places, which are bound to a single processor of the platform.
This way, at the processor level one \verb/place/ is associated to each of the physical cores, and
at the NUMA node level one \verb/place/ is associated to each of the NUMA nodes.


\subsubsection{The way \kaapi enables ready tasks and steals them.}
%FIXME : doit on paler de la différence pop/steal ? Genre ça doit être plus cher de
%faire un steal mais il faudrait l'expliquer.
The scheduling framework in \kaapi~\cite{Bleuse2014} relies on virtual functions for \textit{selecting a victim}, \textit{selecting a place} to push a ready task and \textit{pushing} a set of initial ready tasks.
When a processor becomes idle, the runtime system calls a function, called  \verb/WSselect/ for \emph{work-stealing select}, to browse the topology to find a place from which stealing a task from the place task queue.
%There are many ways to do so, implemented in different \emph{selection strategies} called \verb/WSselect/, for \emph{work-stealing select} in the remaining of the paper.
%Once a place is selected, the processor will take a ready task from its queue.

The completion of a task may unlock the execution of some of its children in the dependency graph.
This means marking them as ready for execution and pushing at least one of them to a place.
Once again, there are many ways of selecting the place where to push ready tasks, implemented in strategies we refer to as \verb/WSpush/, for \emph{work-stealing push}.
Before parallel computation begins, the runtime system distributed the set of ready tasks accordingly to the strategy defined by \verb/WSpush_init/.

These functions are the three main entry points to specify a scheduling algorithm in \kaapi. 

\subsection{Controlling data distribution on a NUMA system}

Controlling the way data are allocated on a NUMA system requires a good understanding of the underlying memory architecture.
Application programmers can achieve this using dedicated tools or libraries, like libNUMA's \verb!numactl!~\cite{DBLP:journals/corr/abs-1101-0093}, which can be used to set a default memory allocation policy for the whole application.
For example, the \verb!--interleave=all! memory policy spreads out all the memory pages of dynamically allocated variables, over all the NUMA nodes of the machine.
This policy is widely used on NUMA systems in conjunction with dynamic parallelism, like task-based programs, as it distributes the memory traffic over all the memory controllers, making processors "\emph{all equally bad}" when it comes to memory access.
To better control data placement, parallel application programmers are used to relying on the \emph{first-touch} allocation policy, which is the default behavior for memory allocation on most Linux systems.
This allows allocating memory pages when they are accessed for the first time.
%Using this policy, a memory page is allocated on the same NUMA node than the first thread that accesses it.
%Many parallel applications perform the initialization of data structures in parallel, to indirectly distribute data thanks to the first-touch policy.
%However, in order to avoid remote memory accesses, the threads must access the data during the computation phase the exact same way it was accessed during the initialization phase, which is very difficult to guarantee with dynamic task-based parallelism.

To better control data distribution on NUMA systems, we propose two different approaches :
\begin{itemize}
\item either the application programmer explicitly allocates data on specific NUMA nodes of the machine through a dedicated API we provide~\cite{Durand2013} (\verb!omp_locality_domain_allocate_XXX!) where \verb/XXX/ may be a bloc cyclic data distribution for one or two dimensional arrays over MAMI~\cite{BroFurGogWacNam10IJPP};
\item or the application programmer only marks some regions of code that initialize data to give the runtime system the opportunity to map the corresponding tasks to make the first-touch allocation policy indirectly apply the data distribution we target. Indeed, Olivier et al.~\cite{Olivier:2012:CMW:2388996.2389085}
have shown that specifying affinity for initialization tasks can lead to huge improvement over locality oblivious techniques.
To avoid remote memory accesses, the threads must access the data during the computation phase the exact same way it was accessed during the initialization phase, which is very difficult to guarantee with dynamic task-based parallelism. We extend the OpenMP runtime functions to provide a dedicated API: \verb!omp_set_affinity! to give directive to the runtime to map the next task to a specific NUMA node.
\end{itemize}
Both of these approaches give the runtime system a way of keeping track of where the memory pages associated with the application data are actually stored.
This information is then used to guide the way we perform task creation and load balancing.

\subsection{Distribution of initial ready tasks : WSpush\_init strategies}
Initial tasks appears in any task based computation in OpenMP parallel region. A ready task is typically a source of DAG such a an initialization task.
We have implemented three task's distribution strategies : \verb!cyclicnuma! which distributes the tasks in a round-robin fashion over the NUMA nodes ; \verb!randnuma! which randomly distributes the tasks over the NUMA nodes and \verb!defaultnuma! which distributes the tasks according to the affinity hints provided by the application programmer using \verb!omp_set_affinity!.
Note that unlike \verb!numactl!, the strategies we implemented consider the whole data appearing in the OpenMP task \verb!depend! clause instead of working at the page level. In other words, while the two memory pages holding an 8K-wide array would be distributed on different nodes by \verb!numactl --interleave=all!, they are always assigned to the same NUMA node when using one of our data distribution strategies.

\subsection{Distribution of ready tasks : WSpush strategies}

This section describes four different ways of pushing ready tasks to a NUMA system places.
Two of them are data-oblivious while the other two rely on the dependencies expressed using the \verb!depend! keyword on OpenMP tasks.

The \verb/pLoc/ strategy makes a processor push ready tasks to
    its own place, while the \verb/pLocNum/ strategy makes a processor push ready tasks to the place of its NUMA node (\emph{local NUMA node}).
The \verb/pNumW/ strategy pushes tasks on the node-level place corresponding to the NUMA node where most of their output data are allocated to (W stands for Write).
The last WSpush strategy, called \verb/pNumWLoc/, behaves almost the same than \verb!pNumW! except that if
    the data are allocated on the NUMA node of the processor pushing the task, we directly push the task to this processor's place instead of pushing it to the node-level place (Loc stands for Local).
    
    It's important to note that \verb!pLoc! and \verb!pLocNum! does not take initial data placement into account, while \verb!pNumW! and \verb!pNumWLoc! are both aware of where a task's data are physically allocated and which of them are written, thanks to the OpenMP \verb!depend! keyword.

\subsection{Dynamic load balancing using work-stealing : WSselect strategies}

Another important step when implementing work-stealing is the selection of the victim processor we want to steal from.
This section describes the selection strategies we implemented, that take the architecture memory hierarchy into account.
The first two strategies, \verb!sRand! and \verb!sRandNum! are similar to those studied in~\cite{DBLP:journals/ijhpca/OlivierPWSP12}
and distinguish two levels of hierarchy : the processor level and the NUMA node level.
\verb/sRand/ selects a random processor's place while \verb/sRandNum/ selects a random NUMA node's place.
We additionally implemented several strategies mixing both levels of hierarchy,
described below.

%\begin{figure}[t]
%  \centering
%  \includegraphics[scale=0.45]{figures/strategies.pdf}
%  \caption{Illustration of the "sProcNum" strategy}
%\label{fig:detail-strategy}
%\end{figure}
\begin{itemize}
  \item \verb/sProcNum/ : First, we browse the processor's place. Upon failure, we browse the topology in the following order : we first browse one of the neighbor processors ; when all the neighbors have been visited, we browse the local NUMA place ; we continue by browsing all the processors' places from a random remote node and we eventually consider the place of its NUMA node.
  \item \verb/sNumProc/ : This strategy is similar, except we always look at the
    NUMA place before looking at the processors' place.
  \item \verb/sProc/ : In this strategy the stealer will visit only the
    processors' places and its own NUMA place.
  \item \verb/sNum/ : In this strategy the stealer will visit only NUMA places
    and its neighbors. 
\end{itemize}

Like proposed in \cite{Olivier:2012:CMW:2388996.2389085}, all these strategies come in two versions : a \verb!strict! version in which we prevent processors from stealing from other NUMA nodes to improve data locality and a \verb!loose! version where these restrictions do not apply.

% TODO : truc court sur implémentation OpenMP (trois points d'entrée à implémenter)

\section{Evaluation}
\label{sec:performances-evaluation}

\subsection{Experimental setup}

We ran all our experiments on the Intel192 machine described in section \ref{sec:hardware}.
We evaluated our strategies using the KASTORS~\cite{virouleau:hal-01081974}\footnote{git available at https://scm.gforge.inria.fr/anonscm/git/kastors/kastors.git, tag "tag-europar16"} benchmark suite.
More specifically, we used the dependent tasks version of the blocked QR factorization
(\verb/dgeqrf_taskdep/), and of the blocked Cholesky factorization (\verb/dpotrf_taskdep/).
These applications rely on kernels from BLAS and LAPACK libraries provided by OpenBLAS 2.15.
We used the OpenMP GCC compliant runtime libKOMP~\cite{libkomp} based on \kaapi runtime system. We tagged the version we used on \kaapi's
git repository\footnote{https://scm.gforge.inria.fr/anonscm/git/kaapi/kaapi.git } in the branch \emph{public/europar2016}.
For all of the above applications, we used the GCC 5.2.0 release compiler.
We also made our execution log files public\footnote{https://github.com/viroulep/europar-2016-public}, as well as all the scripts we used, so that
anyone may reproduce our data analysis.

\subsection{Impact of the data distribution}

We first evaluated the impact the initial data distribution has on the application performance. We did an evaluation for multiple matrix sizes and block sizes,
as well as multiple combinations of WSpush and WSselect strategies. Figure \ref{fig:eval-distrib} reports the
results we obtained for the Cholesky application, on 32K-wide matrices divided into blocks of 512$\times$512 elements. We observed similar behavior running Cholesky on different matrix sizes (16K to 64K) and block sizes (256 to 1024).
The dashed horizontal line is the \kaapi performance baseline, and the plain
horizontal line is the \kaapi performance baseline using \verb/numactl/. Both use the basic sRand/pLoc without any placement control for the tasks.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.5]{figures/graph_distrib.pdf}
\caption{Evaluating several data distributions}
\label{fig:eval-distrib}
\end{figure}

The figure shows that using a \verb/cyclicnuma/ or \verb/randnuma/ to control
the tasks placement during initialization can improve significantly the performances
comparing to both \kaapi baselines.
On the contrary, using a bad distribution will have a negative impact on the performances.
\PV{décrire defaultnuma, dire qu'on a choisi un truc qui marche pas ?}
%TODO Un peu mal dit ?...

The \verb/cyclicnuma/ distribution is the one that works best regardless of
the strategies, and we will use it as the default strategy for the next experiments.



\subsection{Impact of the stealing restriction}

Given a data distribution, previous works~\cite{Olivier:2012:CMW:2388996.2389085}
have shown that restricting the task execution to the node where the data are
written leads to better data locality which may improve the application performance.
%Note explicative :
% Pour i dans 0-N le kernel dpot agit sur chaque bloc de la diagonale
% Au rang i,
% - trsm agit sur la colonne en dessous de la case i,i
% - syrk agit sur toutes les cases de la diagonale à partir de i+1,i+1 jusqu'à n,n
% - gemm agit sur le reste des cases de la triangulaire inférieure non touchées par trsm ou syrk
% Bref : il y a 64 tiles en round robin, ça fait un peu moins de 3 tiles/noeuds, sur 64 tiles
% seulement 36 sont écrites, parmis elles la colonne inférieure 1 est écrite 1 fois,
% la 2 deux fois, etc. Chaque case i,i est écrite i+1 fois, les dépendances
% en écriture sont donc très largement concentrées sur la partie inférieure droite de la matrice !
% Donc le strict c'est nul.
However, this is heavily dependent on the algorithm the application implements. For instance, in the case
of a Cholesky factorization, many tasks write to the diagonal tiles
of the matrix comparatively to other tiles of the matrix. Therefore applying
a steal restriction on these tasks will potentially lead to an important number
of inactive processors.
We evaluated both strict and loose versions of our work-stealing strategies and found
out that preventing processors from stealing from other NUMA nodes can lead to a loss of performance by
around 25\% to more than 75\% with respect to the same setup without the \emph{strict}
restriction.
For the sake of brevity we did not include a figure for this, but the results of these
experiments are included in the logs publicly published.

%\begin{figure}[t]
  %\centering
  %\includegraphics[scale=0.6]{figures/graph_strict.pdf}
%\caption{Evaluating impact of strictness}
%\label{fig:eval-strict}
%\end{figure}


\subsection{Overview of the strategies performances}

We took a given data distribution, \verb/cyclicnuma/, and compared the different strategies, without any steal restriction.
The performance obtained running the Cholesky application compiled with GCC 5.2.0 and executed by the corresponding libGOMP runtime system (without modification) is considered as a baseline for these experiments.
Once again, even if the performances we obtained are obviously not the same, the behavior of the different strategies comparatively to each others are similar for the different applications we ran.
Figure \ref{fig:eval-all-strat} shows the results of the experiments for the
Cholesky application on 32K-wide matrices divided into blocks of 512$\times$512 elements (best configuration for this matrix size).
The dashed horizontal line is the GCC performance baseline.

It is first interesting to note that even very basic \verb/WSselect/
and \verb/WSpush/ strategies, like \verb/sRand/+\verb/pLoc/, obtained decent performances
thanks to the data distribution.
Also, given a selection strategy (e.g. \verb/sRandNum/), placing
the task on the NUMA node where the written data are allocated (\verb/pNumW/) behaves better than simply pushing the data to its NUMA node (\verb/pLocNum/).
However, assuming the tasks are being pushed using the \verb/pNumWLoc/ strategy, focusing the
place selection on only one level of the hierarchy (\verb/sProc/ or \verb/sNum/)
fails to reach the same level of performance we obtained with naive strategies.
On the contrary, taking into account both levels of the hierarchy (\verb/sProcNum/,
\verb/sNumProc/) achieve similar performance that outperforms other strategies.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.5]{figures/graph_all_strat.pdf}
  \caption{Evaluating all strategies (WSselect + WSpush)}
\label{fig:eval-all-strat}
\end{figure}


\subsection{Strategies performance scaling}

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.5]{figures/graph_details_strat.pdf}
\caption{Evaluating specific strategies on multiple sizes}
\label{fig:eval-strat-sizes}
\end{figure}

We eventually selected the three strategy combinations that outperformed the GCC baseline to evaluate their scalability depending on the size of the input matrix.
These strategy combinations are :
\begin{itemize}
  \item \verb/sRand/ + \verb/pLoc/, which is a basic strategy that does not take the architecture topology into account ;
  \item \verb/sNumProc/ + \verb/pNumWLoc/, which was the best strategy in our previous
    evaluation and is also equivalent to using \verb/sProcNum/ ;
  \item \verb/sRandNum/ + \verb/pNumW/ that performs random selection of node-level places.
\end{itemize}

Figure \ref{fig:eval-strat-sizes} reports their performances using a \verb/cyclicnuma/ distribution without steal restriction.
The figure shows the performances using the best block size for each matrix size (which is, for our setup, 256 for a matrix
size of 16384, and 512 for the others).
As expected, combinations of strategies taking both the architecture hierarchy and data locality into account (\verb/sNumProc/
+ \verb/pNumWLoc/) achieve the best performances.
The only exceptions are for small matrix sizes (16384), where there may just be
not enough work to be able to take advantage of this strategy.
We must note that simply distributing the data over the nodes enables the basic
\verb/sRand/ + \verb/pLoc/ combination to achieve satisfying performances.

\section{Related work}
\label{sec:related-work}

Numerous works focus on data locality and/or topology-aware task scheduling strategies for
NUMA architectures.
Clet-Ortega et al.~\cite{DBLP:conf/europar/Clet-OrtegaCP14} studied different ways of decorating the architecture topology with task lists and how it impacts the performance of task-based applications on NUMA systems, promoting private per-threads lists of tasks browsed in a hierarchical way by work-stealing strategies.
We somehow extended this work considering also node-level task lists. We showed considering these lists for pushing ready tasks and selecting work-stealing victims can help improving performance on NUMA systems.
Olivier et al.~\cite{DBLP:journals/ijhpca/OlivierPWSP12} evaluated hierarchical task scheduling with respect to traditional centralized or distributed task schedulers. Creating a thread list, called \emph{shepherd}, per NUMA node allowed their hierarchical scheduler to outperforms other approaches on several task-based applications.
Tahan et al.~\cite{DBLP:journals/corr/Tahan14} also studied the behavior of task-based OpenMP applications on NUMA systems, extending the NANOS runtime system with two NUMA-aware task schedulers called DFWSPT and DFWSRPT, taking into account the notion of task priority when pushing tasks to core-level queues. They also try to minimize the number of \emph{memory hops} when performing load balancing.
While the same kind of studies have been conducted in other contexts~\cite{DBLP:conf/europar/TerbovenSCM12,DBLP:journals/corr/abs-1101-0093}, none of them takes advantage of the OpenMP \verb/depend/ clause, which precisely indicates which data are read and written by a given task. As advertised by the results obtained by our \verb!sNumProc+pNumWLoc! combined strategy, this information is worth taking into account when choosing a place to push ready tasks to.

%TODO parler des schedule static.

%\begin{itemize}
%\item Ce qu'on fait, c'est mieux que Bronis parce qu'on a la vision
%  (et le contrôle indirect) du placement des données. Encore faut-il
%  le démontrer! Une coucourbe hwsnumafirst.numa.* (TODO : à vérifier)
%  devrait suffire à montrer que ce qu'on fait va plus loin (en termes
%  de perfs) que ce qu'ils proposaient.
%\item La question qui tue (Thierry ?) : si on est capable de contrôler le
%  placement des tâches dans les kprocs, et de contrôler le placement
%  mémoire, pourquoi on ne fait pas de placement statique à la SCOTCH
%  ou autre?
%\end{itemize}

%CEA~\cite{DBLP:conf/europar/Clet-OrtegaCP14},
%Bronis~\cite{DBLP:journals/sp/OlivierSSP13} TODO: c'est peut-être pas
%la bonne ref pour Bronis, OpenMP task-scheduling et
%NUMA~\cite{DBLP:journals/corr/Tahan14}, solution au niveau appli
%(terboven)~\cite{DBLP:conf/europar/TerbovenSCM12}, Olivier et al.~\cite{DBLP:journals/ijhpca/OlivierPWSP12}.

\section{Conclusion and future works}
Task-based programming environments like OpenMP have become a standard way to program large scale NUMA systems.
Indeed, they give the programmer ways of expressing massive fine-grain parallelism that can be dynamically mapped to the architecture topology at runtime.
OpenMP recently evolved to deal with tasks dependencies describing the data a task reads as input and writes as output. 

This paper presented several runtime-level strategies to efficiently assign tasks to processors on any NUMA architecture. We presented strategies assigning ready tasks to lists of tasks, called \emph{places}, attached to processors and NUMA nodes. These strategies define the way a task-based runtime system pushes ready tasks to their initial place and the way idle processors browse the architecture topology to select a place to steal from. We considered several initial data distributions and evaluated different combinations of "push" and "select" strategies on a 192 core NUMA system, on linear algebra applications. We achieved the best performance with strategies taking into account both the architecture topology and the initial data placement obtained through OpenMP tasks dependencies.

A short-term future work will be to extend an OpenMP compiler to be able to identify the initialization tasks in a more OpenMP-friendly manner, like extending the \verb!task! construct with a \verb!init! clause. We also intend to experiment with more OpenMP 4 applications.
In a longer term, we intend to move our focus to compile-time techniques able to infer and to attach valuable information on tasks, like an estimation of a task operational intensity, that could guide some of the runtime system's decisions regarding task scheduling and load balancing. We strongly believe a tight cooperation between the compiler and the runtime system is a key step to enhance the performance and scalability of task-based programs on large-scale platforms.

\section*{Acknowledgments}

ELCI
  \small \bibliographystyle{Styles/iplain}
%\nocite{*}
  \bibliography{Bib/paper}

\end{document}
