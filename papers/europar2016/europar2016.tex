\documentclass{Styles/llncs}
%\documentclass[12pt,letterpaper]{article}
\usepackage{times}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{boxedminipage}
\usepackage{xspace}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{verbatim}
\usepackage{parcolumns}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%Prevents floating item to "jump" between sections
\usepackage[section]{placeins}
\usepackage{booktabs}
\newcommand{\arevoir}[1]{#1}

\newcommand{\kaapi}{\textsc{\mbox{kaapi}}\xspace}

\newcommand{\libXKOMP}{\textsc{libKOMP}\xspace}

\usepackage{xcolor}
\usepackage{todonotes}
\usepackage[color,leftbars]{changebar}

\newcommand{\cfsect}[1]{(\textit{cf.} section~\ref{#1})}
\newcommand{\cfsectpage}[1]{(\textit{cf.} section~\ref{#1}, page~\pageref{#1})}
\providecommand{\figureref}[1]{\figname~\ref{#1}}
\providecommand{\cftab}[1]{(\textit{cf.} tableau~\ref{#1})}
\newcommand{\cmd}[1]{{\upshape\texttt{\symbol{"5C}#1}}}

\newenvironment{remarque}
{\description \item[Remarque:] \ \slshape}
{\enddescription}

\makeatletter
\newbox\sf@box
\newenvironment{SubFloat}[2][]%
  {\def\sf@one{#1}%
   \def\sf@two{#2}%
   \setbox\sf@box\hbox
     \bgroup}%
  { \egroup
   \ifx\@empty\sf@two\@empty\relax
     \def\sf@two{\@empty}
   \fi
   \ifx\@empty\sf@one\@empty\relax
     \subfloat[\sf@two]{\box\sf@box}%
   \else
     \subfloat[\sf@one][\sf@two]{\box\sf@box}%
   \fi}
\makeatother
\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}
\renewcommand{\ttdefault}{pcr}
\lstset{
	tabsize=4,
%	frame=single,
	breaklines=true,
	basicstyle=\ttfamily,
	frame=tb,
	framerule=0.2pt,
%	frameround={tttt},
	showstringspaces=false,
	language=c,
%	linewidth=0.95\textwidth,
	keywordstyle=\color{black}\bfseries,
%	keywordstyle=\color{blue},
	commentstyle=\color{OliveGreen},
	stringstyle=\color{red}\itshape,
	inputencoding=utf8/latin1,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
% OMP define
emph={\#,pragma, taskwait, omp, task, depend}, emphstyle=\color{RoyalBlue}\bfseries,
emph={[2]in,inout,out,cw}, emphstyle={[2]\color{BrickRed}\bfseries},
emph={[3]tied,untied,shared}, emphstyle={[3]\color{Gray}\bfseries},
emph={[4]lu0,fwd,bdiv,bmod}, emphstyle={[4]\color{DarkGreen}\bfseries},
emph={[5]cw}, emphstyle={[5]\color{DarkViolet}\bfseries},
    %moredelim=**[is][\only<3>{\color{red}}]{@}{@},
}
\lstdefinestyle{smaller}{basicstyle=\scriptsize\ttfamily}
\lstMakeShortInline|

\newcommand{\benchs}{KASTORS }

\sloppy

\begin{document}

\title{Using data dependencies to improve task based scheduling strategies on NUMA architectures}
\author{
  Philippe Virouleau$^1$ \and François Broquedis$^2$ \and Thierry Gautier$^{1,3}$
 \vspace*{-1ex}}
\institute{
   $^1$INRIA, $^2$Grenoble Institure of Technology, $^3$LIP\\
   CORSE and AVALON Teams, Computer Science Laboratories of Grenoble
   and Lyon, France\\
   \email{philippe.virouleau@inria.fr}\\
   \email{francois.broquedis@imag.fr}\\
   \email{thierry.gautier@inrialpes.fr}\\
}
\date{}
\maketitle

\begin{abstract}
  \vspace*{-5ex} The OpenMP 4.0 specifications extended the task parallelism
  constructs by providing the user a more flexible way to synchronize tasks :
  data dependencies between tasks.
  Using such an approach allows both the compiler and the runtime system
  to know exactly which data are read or written by a given task, and how these
  data will be used through the program lifetime.  Data placement and scheduling
  strategies have a significant impact on performances when considering
  NUMA architectures.  Numerous papers focus on these topics, however none
  has made extensive use of the information available through dependencies.
  One can use these information to modify the behaviour of the application at
  several levels : during initialization to control data placement,
  and during the application runtime to dynamically control both the preferred
  task placement and stealing strategy, depending on the topology.
  This paper introduces several heuristics for these strategies and their
  implementations in our OpenMP runtime XKaapi.
  We also evaluate their performances by using several linear algebra
  applications, executed on a 192 cores NUMA machine. We finally
  compare them to strategies presented previously by related works.

\smallskip
  \noindent\textbf{Keywords:}
  \emph{
    OpenMP, task dependencies, benchmark, runtime systems, NUMA, Kaapi, scheduling, workstealing
  }
\end{abstract}


\section{Introduction}

TODO

task dependencies enable full knowledge about data

Different approaches for dealing with NUMA :

Initial data distribution/placement (set affinity)

Initial task placement (pushinit)

Dynamic ready-task placement (push)

Work-stealing strategies (select)

Contribution : new heuristic for these approaches, evaluation and comparison against previous one on a large NUMA machine.



%HPC architectures evolved so rapidly that it is now common to build
%shared-memory configurations with several dozens of cores.  The recent
%appearance of technologies such as the Intel Xeon Phi co-processor makes
%affordable configurations with thousands of cores a not-so-far
%reality.  Efficiently programming such large-scale platforms requires
%to express more and more fine-grain parallelism.

%Standard parallel programming environments such as OpenMP have evolved
%to address this requirement, introducing new ways of designing highly
%parallel programs. Extending OpenMP to support task parallelism stands
%as a first step to improve the scalability of OpenMP applications on
%large-scale platforms. Indeed, task parallelism usually comes with lower
%runtime-related overhead than thread-based approaches, allowing OpenMP
%programmers to create a large amount of tasks at low cost. Task
%parallelism also promotes the runtime system to a central role, as
%having more units of work to execute requires smarter scheduling
%decisions and load balancing capabilities.

%OpenMP was recently extended to support task dependencies. Instead of
%explicitly synchronizing all the tasks of a parallel region at once, the
%application programmer can now specify a list of variables a task will
%read as input or write as output instead. This information is
%transmitted to the task scheduling runtime system. The runtime then marks
%a task as ready for execution only once all its dependencies have been
%resolved. Dependencies therefore provide a way to define finer
%synchronizations between tasks, able to scale better than global
%synchronizations on large-scale platforms. Dependencies also give the
%runtime system more options to efficiently schedule tasks, as these
%become ready for execution as soon as the data they access has been
%updated.

%As promising as it looks however, any new feature needs proper
%evaluation to encourage application programmers to embrace it. While
%several compilers and runtime systems are now beginning to support
%OpenMP~4.0 task dependencies, no benchmark suite currently exists to
%evaluate their respective benefits and compare them to traditional task
%parallelism.

%This paper highlights two major contributions. We first introduce a
%new benchmark suite to experiment with OpenMP~4.0 task dependencies. We
%present performance results for both the GCC/libGOMP and the
%CLANG\footnote{Intel branch with support for OpenMP:
%\url{http://clang-omp.github.io/}}/libIOMP compilers and their runtime
%systems, comparing kernels
%involving either dependent or independent tasks. Secondly, we
%comment on the issues we met while implementing these benchmarks along
%the lines of current 4.0 revision of the OpenMP specification. Building
%on this experience, we contribute some extension proposals to the
%existing OpenMP specification, to improve the expressiveness of the
%task dependency support.

%The remainder of this paper is organized as follows. Section
%\ref{sec:omp-deps} describes the task dependency programming model in
%OpenMP~4.0. It then analyzes the strategies adopted by GCC/libGOMP and
%CLANG/libIOMP to implement this model. Section \ref{sec:benchs}
%introduces the \benchs benchmark suite we have designed to evaluate
%OpenMP~4.0's task model implementations. Section \ref{sec:perfs}
%presents the performance results of \benchs using two different hardware
%configurations. We identify and discuss practical issues with the
%current OpenMP specification, and we propose extensions in section
%\ref{sec:extensions} to address these issues. We finally present some
%related works in section \ref{sec:related-work} before concluding.

\section{Background}
\subsection{Hardware background}
\begin{itemize}
  \item Introduire les ccNUMA à grande échelle
  \item Donner des détails sur l'archi d'idchire (topologie, bande
    passante, latences, etc.)
\end{itemize}
\subsection{Software background}
\begin{itemize}
  \item shared memory + high level of parallelism => OpenMP tasks
\end{itemize}
\subsubsection{A glimpse at OpenMP task-based applications}
\begin{itemize}
  \item présentation du concept de tâches
  \item dépendances de données
  \item code d'exemple
\end{itemize}
\subsubsection{The way we execute task-based applications}
\paragraph{Inside kaapi}
\begin{itemize}
\item des trucs très simples sur le fonctionnement interne d'un
  runtime data-flow.
\end{itemize}
\paragraph{notre façon de voir la machine}
\begin{itemize}
\item places (kproc, NUMA) ;
\item impact du placement sur l'exécution / le vol (pop, steal, blabla)
\end{itemize}

\subsection{On pose le problème}
\begin{itemize}
  \item Modèle d'exécution très dynamique, équilibrage à la
    volée... Tout ça semble peu compatible avec un respect strict des
    affinités mémoire!
  \item D'un autre côté, pour exploiter des machines NUMA à grande
    échelle, on ne peut pas non plus renoncer à rééquilibrer la charge
    de temps en temps.
  \item Du coup, on cherche un (bon) compromis entre équilibrage de
    charge et respect des affinités.
  \item C'est le boulot du runtime (ça peut difficilement être celui
    du programmeur seulement), puisqu'on va devoir contrôler le
    placement des données, le placement initial des tâches et le
    parcours de la topologie lors d'un vol.
\end{itemize}

\section{Using OpenMP tasks dependencies to improve tasks and data
  placement on NUMA machines (TODO : horrible :-) )}
\subsection{Placement initial des données}
\begin{itemize}
  \item Pour contrôler le placement des données, les gens font au
    mieux numactl.
  \item Nous, on agit là-dessus à deux niveaux différents :
    \begin{itemize}
      \item au niveau applicatif, où on fournit au programmeur une API
        pour contrôler le placement mémoire de ses données
        (omp\_set\_affinity) ;
      \item un niveau du runtime, où on peut garantir la
        répartition des données de manière indirecte, en plaçant les
        tâches d'initialisation (famille WSpush\_init dans laquelle on
        retrouve : WSpush, cyclicnuma, randnuma, rand). => blabla first-touch
      \end{itemize}
    \item Les stratégies de l'état de l'art (numactl etc) s'appliquent
      pour la totalité d'une exécution. Notre approche est capable de
      différencier les tâches d'initialisations des tâches de calcul,
      de manière à adapter la politique de selection et de placement
      de tâches.
\end{itemize}

\subsection{Placement initial d'une tâche prête}
\begin{itemize}
  \item Quand le runtime détecte que les dépendances d'une tâche sont
    satisfaites, la tâche passe dans l'état prêt et le runtime doit
    choisir une place où l'insérer.
  \item Rappeler le fonctionnement du kproc qui mange des tâches chez
    lui d'abord : d'où l'importance du placement initial d'une tâche
  \item On définit plusieurs stratégies de placement de tâches prêtes
    : local, numa, Wnuma, Whws qui s'appuient sur une vision hiérarchique de
    la topologie de la machine pour placer les tâches au plus près de
    leurs données.
\end{itemize}

\subsection{Equilibrage de charge dynamique}

\subsubsection{Place selection strategies}
Some basic ones :
\begin{itemize}
  \item \verb/rand/ : selection from a random kproc place.
  \item \verb/numa/ : selection from a random numa place.
  \item \verb/strict/ : selection only from our numa place.
\end{itemize}

"Hierarchical" ones

\begin{itemize}
  \item \verb/hws_P/ : select from kprocs from the local node, then from the local numa place, then from a random remote kproc.
  \item \verb/hws_N/ : select from kprocs from the local node, then from the local numa place, then from a random remote numa place.
  \item \verb/hws_N_P/ : for every node starting by ours : select from kprocs on this node, then from the numa place of this node.
  \item \verb/hws_P_N/ : for every node starting by ours : select from the numa place of this node, then from the kprocs on this node.
\end{itemize}

Details for node iterations on the above strategy : linear iteration starting from a random index (e.g. given N nodes and an index i, loop from i to N, then from 0 to i).

\subsubsection{Push strategies for a ready task}
Some basic ones :
\begin{itemize}
  \item \verb/local/ : push to our kproc.
  \item \verb/numa/ : push to our numa node's place
\end{itemize}

More elaborated :
\begin{itemize}
  \item \verb/Wnuma/ : push to the numa place where the data are.
  \item \verb/Whws/ : push to the numa place where the data are, if it's ours, push to our kproc.
\end{itemize}

\subsubsection{Distribution strategies at initialization}
\begin{itemize}
  \item \verb/cyclicnumastrict/ : round robin on numa nodes
  \item \verb/randnuma/ : each task to a random node.
\end{itemize}

\subsubsection{loose vs strict}

Possibility to combine the above with KAAPI\_STRICT\_PUSH to prevent the task from being stolen by a non local kproc.

\begin{itemize}
  \item pas d'impact significatif : le sortir des courbes, en parler
    dans un paragraphe dédié où on présente les deux comportements et,
    de manière synthétique, les perfs associées (tableau, voire
    pourcentages de différence entre les deux dans le texte)
  \item {\color{red}Conclusion à revoir, maintenant que le wspush est strict pour tout !}
\end{itemize}

\section{Evaluation}

\section{Related work}
\begin{itemize}
\item Ce qu'on fait, c'est mieux que Bronis parce qu'on a la vision
  (et le contrôle indirect) du placement des données. Encore faut-il
  le démontrer! Une coucourbe hwsnumafirst.numa.* (TODO : à vérifier)
  devrait suffire à montrer que ce qu'on fait va plus loin (en termes
  de perfs) que ce qu'ils proposaient.
\item La question qui tue (Thierry ?) : si on est capable de contrôler le
  placement des tâches dans les kprocs, et de contrôler le placement
  mémoire, pourquoi on ne fait pas de placement statique à la SCOTCH
  ou autre?
\end{itemize}

CEA~\cite{DBLP:conf/europar/Clet-OrtegaCP14},
Bronis~\cite{DBLP:journals/sp/OlivierSSP13} TODO: c'est peut-être pas
la bonne ref pour Bronis, OpenMP task-scheduling et
NUMA~\cite{DBLP:journals/corr/Tahan14}, solution au niveau appli
(terboven)~\cite{DBLP:conf/europar/TerbovenSCM12}, Olivier et al.~\cite{DBLP:journals/ijhpca/OlivierPWSP12}.


\section*{Acknowledgments}

This work has been partially supported by the IRSES2011-295217
HPC-GA Project.

  \small \bibliographystyle{Styles/iplain}
%\nocite{*}
  \bibliography{Bib/paper}

\end{document}
