\documentclass{Styles/llncs}
%\documentclass[12pt,letterpaper]{article}
\usepackage{times}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{boxedminipage}
\usepackage{xspace}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{verbatim}
\usepackage{parcolumns}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%Prevents floating item to "jump" between sections
\usepackage[section]{placeins}
\usepackage{booktabs}
\usepackage{tkz-graph}
\newcommand{\arevoir}[1]{#1}

\newcommand{\kaapi}{\textsc{\mbox{kaapi}}\xspace}

\newcommand{\libXKOMP}{\textsc{libKOMP}\xspace}

\usepackage{xcolor}
\usepackage{todonotes}
\usepackage[color,leftbars]{changebar}

\newcommand{\cfsect}[1]{(\textit{cf.} section~\ref{#1})}
\newcommand{\cfsectpage}[1]{(\textit{cf.} section~\ref{#1}, page~\pageref{#1})}
\providecommand{\figureref}[1]{\figname~\ref{#1}}
\providecommand{\cftab}[1]{(\textit{cf.} tableau~\ref{#1})}
\newcommand{\cmd}[1]{{\upshape\texttt{\symbol{"5C}#1}}}

\newenvironment{remarque}
{\description \item[Remarque:] \ \slshape}
{\enddescription}

\makeatletter
\newbox\sf@box
\newenvironment{SubFloat}[2][]%
  {\def\sf@one{#1}%
   \def\sf@two{#2}%
   \setbox\sf@box\hbox
     \bgroup}%
  { \egroup
   \ifx\@empty\sf@two\@empty\relax
     \def\sf@two{\@empty}
   \fi
   \ifx\@empty\sf@one\@empty\relax
     \subfloat[\sf@two]{\box\sf@box}%
   \else
     \subfloat[\sf@one][\sf@two]{\box\sf@box}%
   \fi}
\makeatother
\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}
\renewcommand{\ttdefault}{pcr}
\lstset{
	tabsize=4,
%	frame=single,
	breaklines=true,
	basicstyle=\ttfamily,
	frame=tb,
	framerule=0.2pt,
%	frameround={tttt},
	showstringspaces=false,
	language=c,
%	linewidth=0.95\textwidth,
	keywordstyle=\color{black}\bfseries,
%	keywordstyle=\color{blue},
	commentstyle=\color{OliveGreen},
	stringstyle=\color{red}\itshape,
	inputencoding=utf8/latin1,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
% OMP define
emph={\#,pragma, taskwait, omp, task, depend}, emphstyle=\color{RoyalBlue}\bfseries,
emph={[2]in,inout,out,cw}, emphstyle={[2]\color{BrickRed}\bfseries},
emph={[3]tied,untied,shared}, emphstyle={[3]\color{Gray}\bfseries},
emph={[4]lu0,fwd,bdiv,bmod}, emphstyle={[4]\color{DarkGreen}\bfseries},
emph={[5]cw}, emphstyle={[5]\color{DarkViolet}\bfseries},
    %moredelim=**[is][\only<3>{\color{red}}]{@}{@},
}
\lstdefinestyle{smaller}{basicstyle=\scriptsize\ttfamily}
\lstMakeShortInline|

\newcommand{\benchs}{KASTORS }

\sloppy

\begin{document}

\title{Using data dependencies to improve task based scheduling strategies on NUMA architectures}
\author{
  Philippe Virouleau$^1$ \and François Broquedis$^2$ \and Thierry Gautier$^{1,3}$
 \vspace*{-1ex}}
\institute{
   $^1$INRIA, $^2$Grenoble Institure of Technology, $^3$LIP\\
   CORSE and AVALON Teams, Computer Science Laboratories of Grenoble
   and Lyon, France\\
   \email{philippe.virouleau@inria.fr}\\
   \email{francois.broquedis@imag.fr}\\
   \email{thierry.gautier@inrialpes.fr}\\
}
\date{}
\maketitle

\begin{abstract}
  \vspace*{-5ex} The OpenMP 4.0 specifications extended the task parallelism
  constructs by providing the user a more flexible way to synchronize tasks :
  data dependencies between tasks.
  Using such an approach allows both the compiler and the runtime system
  to know exactly which data are read or written by a given task, and how these
  data will be used through the program lifetime.  Data placement and scheduling
  strategies have a significant impact on performances when considering
  NUMA architectures.  Numerous papers focus on these topics, however none
  has made extensive use of the information available through dependencies.
  One can use these information to modify the behaviour of the application at
  several levels : during initialization to control data placement,
  and during the application runtime to dynamically control both the preferred
  task placement and stealing strategy, depending on the topology.
  This paper introduces several heuristics for these strategies and their
  implementations in our OpenMP runtime XKaapi.
  We also evaluate their performances by using several linear algebra
  applications, executed on a 192 cores NUMA machine. We finally
  compare them to strategies presented previously by related works.

\smallskip
  \noindent\textbf{Keywords:}
  \emph{
    OpenMP, task dependencies, benchmark, runtime systems, NUMA, Kaapi, scheduling, workstealing
  }
\end{abstract}


\section{Introduction}

TODO

task dependencies enable full knowledge about data

Different approaches for dealing with NUMA :

Initial data distribution/placement (set affinity)

Initial task placement (pushinit)

Dynamic ready-task placement (push)

Work-stealing strategies (select)

Contribution : new heuristic for these approaches, evaluation and comparison against previous one on a large NUMA machine.



%HPC architectures evolved so rapidly that it is now common to build
%shared-memory configurations with several dozens of cores.  The recent
%appearance of technologies such as the Intel Xeon Phi co-processor makes
%affordable configurations with thousands of cores a not-so-far
%reality.  Efficiently programming such large-scale platforms requires
%to express more and more fine-grain parallelism.

%Standard parallel programming environments such as OpenMP have evolved
%to address this requirement, introducing new ways of designing highly
%parallel programs. Extending OpenMP to support task parallelism stands
%as a first step to improve the scalability of OpenMP applications on
%large-scale platforms. Indeed, task parallelism usually comes with lower
%runtime-related overhead than thread-based approaches, allowing OpenMP
%programmers to create a large amount of tasks at low cost. Task
%parallelism also promotes the runtime system to a central role, as
%having more units of work to execute requires smarter scheduling
%decisions and load balancing capabilities.

%OpenMP was recently extended to support task dependencies. Instead of
%explicitly synchronizing all the tasks of a parallel region at once, the
%application programmer can now specify a list of variables a task will
%read as input or write as output instead. This information is
%transmitted to the task scheduling runtime system. The runtime then marks
%a task as ready for execution only once all its dependencies have been
%resolved. Dependencies therefore provide a way to define finer
%synchronizations between tasks, able to scale better than global
%synchronizations on large-scale platforms. Dependencies also give the
%runtime system more options to efficiently schedule tasks, as these
%become ready for execution as soon as the data they access has been
%updated.

%As promising as it looks however, any new feature needs proper
%evaluation to encourage application programmers to embrace it. While
%several compilers and runtime systems are now beginning to support
%OpenMP~4.0 task dependencies, no benchmark suite currently exists to
%evaluate their respective benefits and compare them to traditional task
%parallelism.

%This paper highlights two major contributions. We first introduce a
%new benchmark suite to experiment with OpenMP~4.0 task dependencies. We
%present performance results for both the GCC/libGOMP and the
%CLANG\footnote{Intel branch with support for OpenMP:
%\url{http://clang-omp.github.io/}}/libIOMP compilers and their runtime
%systems, comparing kernels
%involving either dependent or independent tasks. Secondly, we
%comment on the issues we met while implementing these benchmarks along
%the lines of current 4.0 revision of the OpenMP specification. Building
%on this experience, we contribute some extension proposals to the
%existing OpenMP specification, to improve the expressiveness of the
%task dependency support.

%The remainder of this paper is organized as follows. Section
%\ref{sec:omp-deps} describes the task dependency programming model in
%OpenMP~4.0. It then analyzes the strategies adopted by GCC/libGOMP and
%CLANG/libIOMP to implement this model. Section \ref{sec:benchs}
%introduces the \benchs benchmark suite we have designed to evaluate
%OpenMP~4.0's task model implementations. Section \ref{sec:perfs}
%presents the performance results of \benchs using two different hardware
%configurations. We identify and discuss practical issues with the
%current OpenMP specification, and we propose extensions in section
%\ref{sec:extensions} to address these issues. We finally present some
%related works in section \ref{sec:related-work} before concluding.

\section{Background}
\subsection{Hardware background}
Most of parallel shared memory architectures are now built according to a NUMA~\footnote{Non-Uniform Memory Access} design where the memory is physically split in several banks attached to processors.
Many vendors assemble these banks in a hierarchical way, thus building shared memory machines embedding several hundreds of cores.
However, exploiting such architectures at their full potential requires a fine control of the execution of a parallel application, as accessing local memory is most of the time faster than accessing memory stored in a memory bank attached to a remote processor.

The machine we experimented on is a SGI UV2000 platform made of 24 NUMA nodes.
Each NUMA node holds a 8-core Intel Xeon E5-4640 CPU for a total of 192 cores.
We refer to this machine as Xeon192 in the paper. 

\begin{figure}
\begin{center}
\begin{tikzpicture}
\tikzset{VertexStyle/.append style = {minimum size = 30pt, inner sep = 0pt}}

\Vertex[x=0, y=0, L=$n_{14,15}$]{n0}
\Vertex[x=0, y=2, L=$n_{22,23}$]{n4}
\Vertex[x=0, y=4, L=$n_{8,9}$]{n8}
\Vertex[x=0, y=6, L=$n_{12,13}$]{n12}
\Vertex[x=2, y=0, L=$n_{10,11}$]{n1}
\Vertex[x=2, y=2, L=$n_{6,7}$]{n5}
\Vertex[x=2, y=4, L=$n_{0,1}$]{n9}
\Vertex[x=2, y=6, L=$n_{16,17}$]{n13}

\Vertex[x=4, y=0, L=$n_{20,21}$]{n2}
\Vertex[x=4, y=2, L=$n_{4,5}$]{n6}
\Vertex[x=4, y=4, L=$n_{2,3}$]{n10}
\Vertex[x=4, y=6, L=$n_{10,11}$]{n14}
\Vertex[x=6, y=0, L=$n_{12,13}$]{n3}
\Vertex[x=6, y=2, L=$n_{8,9}$]{n7}
\Vertex[x=6, y=4, L=$n_{18,19}$]{n11}
\Vertex[x=6, y=6, L=$n_{14,15}$]{n15}

\Edge(n0)(n5)
\Edge(n4)(n5)
\Edge(n1)(n5)

\Edge(n13)(n9)
\Edge(n12)(n9)
\Edge(n8)(n9)

\Edge(n15)(n10)
\Edge(n14)(n10)
\Edge(n11)(n10)

\Edge(n3)(n6)
\Edge(n2)(n6)
\Edge(n7)(n6)

\Edge(n9)(n5)
\Edge(n9)(n10)
\Edge(n5)(n10)
\Edge(n9)(n6)
\Edge(n5)(n6)
\Edge(n10)(n6)

\end{tikzpicture}
\end{center}
\caption{Memory topology of the Xeon192 SGI UV2000 machine.}
\label{fig:idchire}
\end{figure}

Figure \ref{fig:idchire} shows the Xeon192 machine memory topology.
NUMA nodes are organized by pairs of nodes connected together through Intel QuickPath Interconnect.
These pairs can communicate together through a proprietary fabric called NUMALink6, represented by the edges on the graph.
In other words, taking as example node $n_0$ :
\begin{itemize}
\item node $n_0$ communicates with node $n_1$ through Intel QPI ;
\item node $n_0$ is one hop away from node $n_4$ \emph{(e.g. communications between node $n_0$ et node $n_4$ cross one NUMALink6 memory controller)};
\item node $n_0$ is two hops away from node $n_{20}$ \emph{(e.g. communications between node $n_0$ et node $n_{20}$ cross two NUMALink6 memory controllers)};
\end{itemize}

Table \ref{tab:idchire} shows the distances advertised by the hwloc library~\cite{DBLP:conf/pdp/BroquedisCMFGMTN10} that represent factors on the communication time, communicating from node $n_0$ to any other node of the Xeon192 machine, wether the node is a peer (node $n_1$ in that case), is located one hop away (nodes $n_2$ to $n_9$, $n_{12}$, $n_{13}$, $n_{16}$ and $n_{17}$) or is located two hops away (nodes $n_{10}$, $n_{11}$, $n_{14}$, $n_{15}$ and $n_{18}$ to $n_{23}$). These factors, that have been correlated with experimental values on both bandwidth and latency by Pilla et al.~\cite{pilla:tel-00981136}, show how much impact the architecture topology may have on a parallel application performance.

\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\begin{table}
\caption{NUMA distances from node 0 advertised by the hwloc library on Xeon192.}
\begin{center}
\begin{tabular}{C{2cm} C{2cm} C{2cm} C{2cm} C{2cm}}
    \toprule[0.15em]
		\emph{NUMA nodes location} & \emph{local} & \emph{peer} & \emph{one hop away} & \emph{two hops away} \\
    \midrule[0.1em]
	hwloc distances & 1.0 & 5.0 & 6.5 & 7.9 \\
    \bottomrule[0.15em]
\end{tabular}
\end{center}
\label{tab:idchire}
\end{table}

\subsection{Software background}

To exploit such large-scale shared memory architectures, the application programmer needs :
\begin{enumerate}
\item to express massive fine grain parallelism to get the most out of the numerous processing units of the platform ;
\item to control the execution of the application, especially the way computations and data are distributed over the platform, to prevent the NUMA design to have a negative impact on the overall application performance.
\end{enumerate}

Task-based parallel programming environments provide ways of expressing fine grain parallelism that can be dynamically assigned to processors at runtime.
OpenMP~\cite{openmp40}, the de-facto standard for shared-memory parallel programming, supports task parallelism with dependencies since revision 4.0.

\subsubsection{A glimpse at OpenMP tasking}

An OpenMP \emph{task} can be seen as an independant \emph{unit of work} an OpenMP thread can execute.
Tasks can be created by an OpenMP thread and executed by any thread of the same parallel region. 
As managing tasks at runtime is way cheaper than creating and synchronizing threads, the application programmer can take the parallelization of its application further, as he can now consider portions of code that were to fine grain to be parallelized using only threads.
The application programmer is thus responsible for creating and synchronizing OpenMP tasks explicitly. 
However, the runtime system is in charge of assigning tasks to threads during the application execution, considering the underlying system state.

OpenMP 4.0 pushes the concept of task further introducing the |depend| keyword to specify the access mode of each shared variable a task will access during its execution. 
Access modes can be set to either |in|, |out| or |inout| whether the corresponding variable is respectively read as input, written as output or both read and written by the considered task. 
This information is then processed by the underlying runtime system to decide whether a task is ready for execution or should first wait for the completion of other ones.

Listing 1.1 shows the implementation of a blocked matrix multiplication. It is
taken from the official OpenMP 4.0 examples~\footnote{available at http://www.openmp.org}.
TODO : blabla matrix multiplication

%Instead of waiting for the
%termination of \emph{all} previous tasks before executing |bmod| tasks,
%we specify dependencies to make sure |bmod| tasks can
%execute as soon as the data they access has been updated by the
%corresponding |fwd| and |bdiv| tasks. Running this |depend| version of
%the program leads to the creation of a dependency graph. When an OpenMP
%thread turns idle, the runtime system browses this graph to decide which
%task should be executed next, according to tasks' current state and
%resolved dependencies.

%Task dependency support comes with several benefits. First, task
%dependencies involve decentralized, selective synchronization operations
%that should scale better than the broad-range taskwait-based approaches.
%In some situations, this way of programming unlocks more valid execution
%scenarios than explicitly synchronized tasks, which provides the runtime
%system with many more valid task schedules to choose from. For example,
%many instances of the |fwd|, |bdiv| and |bmod| computations can legally
%run concurrently with the version of the LU factorization expressing
%task dependencies. On the contrary, this level of concurrence is not
%possible with the |taskwait| version because the lack of accurate
%dependency information leads to an over-conservative synchronization
%scheme. As an added benefit, information about task dependencies also
%enables the runtime system to optimize further, such as improving task
%and data placement on NUMA systems.

\begin{figure}[htbp]
\begin{lstlisting}[caption=Blocked matrix multiplication with OpenMP task dependencies,frame=tlrb,style=smaller,label=lst:MM-deps]{lst:MM-deps}
// Assume BS divides N perfectly
void matmul_depend(int N, int BS, float A[N][N], float B[N][N], float C[N][N]) {
     int i, j, k, ii, jj, kk;
     for (i = 0; i < N; i+=BS) {
          for (j = 0; j < N; j+=BS) {
               for (k = 0; k < N; k+=BS) {
                    #pragma omp task depend (in: A[i:BS][k:BS], B[k:BS][j:BS]) \
                         depend (inout: C[i:BS][j:BS])
                    for (ii = i; ii < i+BS; ii++) {
                         for (jj = j; jj < j+BS; jj++) {
                              for (kk = k; kk < k+BS; kk++) {
                                   C[ii][jj] = C[ii][jj] + A[ii][kk] * B[kk][jj];
                              }
                         }
                    }
               }
          }
     }
}\end{lstlisting}
\end{figure}

\subsubsection{The way we execute task-based applications}
\paragraph{Inside kaapi}
\begin{itemize}
\item des trucs très simples sur le fonctionnement interne d'un
  runtime data-flow.
\end{itemize}
\paragraph{notre façon de voir la machine}
\begin{itemize}
\item places (kproc, NUMA) ;
\item impact du placement sur l'exécution / le vol (pop, steal, blabla)
\end{itemize}

\subsection{On pose le problème}
\begin{itemize}
  \item Modèle d'exécution très dynamique, équilibrage à la
    volée... Tout ça semble peu compatible avec un respect strict des
    affinités mémoire!
  \item D'un autre côté, pour exploiter des machines NUMA à grande
    échelle, on ne peut pas non plus renoncer à rééquilibrer la charge
    de temps en temps.
  \item Du coup, on cherche un (bon) compromis entre équilibrage de
    charge et respect des affinités.
  \item C'est le boulot du runtime (ça peut difficilement être celui
    du programmeur seulement), puisqu'on va devoir contrôler le
    placement des données, le placement initial des tâches et le
    parcours de la topologie lors d'un vol.
\end{itemize}

\section{Using OpenMP tasks dependencies to improve tasks and data
  placement on NUMA machines (TODO : horrible :-) )}
\subsection{Placement initial des données}
\begin{itemize}
  \item Pour contrôler le placement des données, les gens font au
    mieux numactl.
  \item Nous, on agit là-dessus à deux niveaux différents :
    \begin{itemize}
      \item au niveau applicatif, où on fournit au programmeur une API
        pour contrôler le placement mémoire de ses données
        (omp\_set\_affinity) ;
      \item un niveau du runtime, où on peut garantir la
        répartition des données de manière indirecte, en plaçant les
        tâches d'initialisation (famille WSpush\_init dans laquelle on
        retrouve : WSpush, cyclicnuma, randnuma, rand). => blabla first-touch
      \end{itemize}
    \item Les stratégies de l'état de l'art (numactl etc) s'appliquent
      pour la totalité d'une exécution. Notre approche est capable de
      différencier les tâches d'initialisations des tâches de calcul,
      de manière à adapter la politique de selection et de placement
      de tâches.
\end{itemize}

\subsection{Placement initial d'une tâche prête}
\begin{itemize}
  \item Quand le runtime détecte que les dépendances d'une tâche sont
    satisfaites, la tâche passe dans l'état prêt et le runtime doit
    choisir une place où l'insérer.
  \item Rappeler le fonctionnement du kproc qui mange des tâches chez
    lui d'abord : d'où l'importance du placement initial d'une tâche
  \item On définit plusieurs stratégies de placement de tâches prêtes
    : local, numa, Wnuma, Whws qui s'appuient sur une vision hiérarchique de
    la topologie de la machine pour placer les tâches au plus près de
    leurs données.
\end{itemize}

\subsection{Equilibrage de charge dynamique}

\subsubsection{Place selection strategies}
Some basic ones :
\begin{itemize}
  \item \verb/rand/ : selection from a random kproc place.
  \item \verb/numa/ : selection from a random numa place.
  \item \verb/strict/ : selection only from our numa place.
\end{itemize}

"Hierarchical" ones

\begin{itemize}
  \item \verb/hws_P/ : select from kprocs from the local node, then from the local numa place, then from a random remote kproc.
  \item \verb/hws_N/ : select from kprocs from the local node, then from the local numa place, then from a random remote numa place.
  \item \verb/hws_N_P/ : for every node starting by ours : select from kprocs on this node, then from the numa place of this node.
  \item \verb/hws_P_N/ : for every node starting by ours : select from the numa place of this node, then from the kprocs on this node.
\end{itemize}

Details for node iterations on the above strategy : linear iteration starting from a random index (e.g. given N nodes and an index i, loop from i to N, then from 0 to i).

\subsubsection{Push strategies for a ready task}
Some basic ones :
\begin{itemize}
  \item \verb/local/ : push to our kproc.
  \item \verb/numa/ : push to our numa node's place
\end{itemize}

More elaborated :
\begin{itemize}
  \item \verb/Wnuma/ : push to the numa place where the data are.
  \item \verb/Whws/ : push to the numa place where the data are, if it's ours, push to our kproc.
\end{itemize}

\subsubsection{Distribution strategies at initialization}
\begin{itemize}
  \item \verb/cyclicnumastrict/ : round robin on numa nodes
  \item \verb/randnuma/ : each task to a random node.
\end{itemize}

\subsubsection{loose vs strict}

Possibility to combine the above with KAAPI\_STRICT\_PUSH to prevent the task from being stolen by a non local kproc.

\begin{itemize}
  \item pas d'impact significatif : le sortir des courbes, en parler
    dans un paragraphe dédié où on présente les deux comportements et,
    de manière synthétique, les perfs associées (tableau, voire
    pourcentages de différence entre les deux dans le texte)
  \item {\color{red}Conclusion à revoir, maintenant que le wspush est strict pour tout !}
\end{itemize}

\section{Evaluation}

\section{Related work}
\begin{itemize}
\item Ce qu'on fait, c'est mieux que Bronis parce qu'on a la vision
  (et le contrôle indirect) du placement des données. Encore faut-il
  le démontrer! Une coucourbe hwsnumafirst.numa.* (TODO : à vérifier)
  devrait suffire à montrer que ce qu'on fait va plus loin (en termes
  de perfs) que ce qu'ils proposaient.
\item La question qui tue (Thierry ?) : si on est capable de contrôler le
  placement des tâches dans les kprocs, et de contrôler le placement
  mémoire, pourquoi on ne fait pas de placement statique à la SCOTCH
  ou autre?
\end{itemize}

CEA~\cite{DBLP:conf/europar/Clet-OrtegaCP14},
Bronis~\cite{DBLP:journals/sp/OlivierSSP13} TODO: c'est peut-être pas
la bonne ref pour Bronis, OpenMP task-scheduling et
NUMA~\cite{DBLP:journals/corr/Tahan14}, solution au niveau appli
(terboven)~\cite{DBLP:conf/europar/TerbovenSCM12}, Olivier et al.~\cite{DBLP:journals/ijhpca/OlivierPWSP12}.


\section*{Acknowledgments}

This work has been partially supported by the IRSES2011-295217
HPC-GA Project.

  \small \bibliographystyle{Styles/iplain}
%\nocite{*}
  \bibliography{Bib/paper}

\end{document}
