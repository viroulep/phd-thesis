\section{Techniques d'amélioration de la localité des données}\label{sec:rw:numa}


La problématique de la localité des données a été étudiée de manière extensive par la communauté, en particulier dans le contexte des architectures NUMA.
Parmi les nombreuses publications sur le sujet, deux d'entre elles font l'effort d'en présenter une synthèse didactique, énumérant un certain nombre de bonnes pratiques à adopter s'agissant de l'exécution de tâches en contexte NUMA.

%NUMA-aware scheduling for both memory- and compute-bound tasks
\paragraph{Reinman~\cite{Reinman2015}} a regroupé différents travaux selon le type de tâches qu'ils ciblaient, et propose en conclusion des recommandations lorsque l'on cherche à ordonnancer des programmes à base de tâches sur des machines NUMA.

L'auteur distingue principalement deux types de tâches : compute-bound et memory-bound.
Si certaines des 8 recommandations sont très spécifiques aux types de tâches, d'autres décrivent bien les points qu'il faut avoir en tête lorsque l'on cible les architectures NUMA.
Il indique notamment l'importance de la distribution des données de l'application sur l'ensemble des nœuds NUMA, l'importance de la localité des données, ainsi que l'importance de garder les tâches et leur données ensemble lors du vol de travail.

Dans le cadre spécifique des tâches OpenMP, \textbf{Terboven et al.~\cite{Terboven2012}} ont fait une étude du comportement général des tâches sur les architectures NUMA, et donnent des indications assez générales sur les bases pour aborder le problème, comme par exemple assurer le placement fixe d'un thread sur cœur de calcul, ou encore les types d'ordonnancement prévus dans le standard qui sont les plus efficaces dans ce cas.



\subsection{Groupement des calculs ensemble}

Les techniques utilisées par les travaux suivants ont toutes en commun le groupement des calculs sur des éléments topologiquement proches dans la hiérarchie.
L'objectif derrière cette idée est de "cloisonner" la charge de travail (par exemple les itérations de boucles) afin de limiter les accès distants et encourager la réutilisation du cache.

%OpenMP task scheduling strategies for multicore NUMA systems
\paragraph{Olivier et al.~\cite{Olivier2012, Olivier2013}} font l'état des problèmes se posant lors de l'équilibrage de charge sur les architectures NUMA.
Ils notent en particulier que s'il permet de limiter l'inactivité de certains cœurs, il entraine aussi une augmentation des cache miss ainsi qu'une augmentation des accès distants, plus coûteux.

Leur principale contribution s'oriente autour d'une stratégie d'ordonnancement prenant en compte la hiérarchie de la mémoire au sein de l'architecture.
Ils définissent un domaine de voisinage, qu'ils appellent \emph{shepherd}, qui regroupe un ensemble de cœurs voisins dans la hiérarchie.
Cet ensemble peut être défini à plusieurs niveaux : il peut être défini pour un seul cœur, pour un ensemble de cœurs partageant le même cache L3, ou pour un socket comprenant plusieurs processeurs multicœurs.

Ils proposent ensuite une extension au vol de travail : ils définissent un \emph{shepherd} (et donc une file de tâches) par nœud NUMA, et ils proposent que chaque \emph{shepherd} aille voler un ensemble de tâches lorsque leur propre file de tâches est vide.
Au sein d'un \emph{shepherd}, le travail est réparti entre les threads associés aux cœurs d'une manière LIFO, qui permet un ordonnancement en profondeur dans le but de rester au plus proche de l'exécution et de favoriser la réutilisation des caches.


%Evaluation of OpenMP Task Scheduling Algorithms for Large NUMA Architectures
\paragraph{Clet-Ortega et al.~\cite{Clet2014}} présentent des techniques de vol de travail favorisant les vols locaux plutôt que distants.
Ils constatent qu'une file centralisée introduit un fort coût de gestion compte tenu du fait que le nombre de tâches créées peut être élevé, et proposent l'utilisation d'une file de tâches par thread du support exécutif pour pallier ce problème.
Cela introduit du même coup une vision décentralisée de la gestion des tâches, et en couplant ça avec une analyse de la topologie via hwloc, ils introduisent la notion de voisins pour un thread.

La deuxième partie de leur contribution consiste à étendre le vol de travail pour privilégier les vols dans les files voisines, restreignant ainsi les calculs sur des groupes de cœurs physiques proches.
%Towards Efficient OpenMP Strategies for Non-Uniform Architectures
Cette approche est reprise par Tahan et al.~\cite{Tahan2014}. Chaque thread dispose d'une liste privée des autres files de threads, triée par la distance au cœur sur lequel s'exécute le thread.



\paragraph{Pilla et al.~\cite{Pilla2014}} proposent des extensions au support exécutif ciblant spécifiquement les architectures NUMA et les applications itératives.
À chaque pas de temps, leur ordonnanceur va tenter de corriger un déséquilibre de charge en déplaçant des tâches d'un cœur à un autre.
Pour évaluer si une tâche devrait être déplacée sur un autre cœur, ils prennent en compte plusieurs facteurs : la charge du cœur, la quantité de communication que la tâche effectue avec d'autres tâches du cœur, le facteur NUMA entre les deux cœurs, et la quantité de communication locale que la tâche effectuerait sur son cœur actuel.
Ils ont effectué ces extensions dans Charm++~\cite{Kale1993} qui à l'avantage de leur fournir directement des informations sur les performances des tâches aux précédents pas de temps.
Pour résumer ils vont favoriser le rééquilibrage de charge sur des cœurs voisins, et considérer un équilibrage sur un cœur distant si les caractéristiques de la tâche le permettent.


\subsection{Distribution initiale des données et placement des calculs}

Nous avons ici regroupé les techniques consistant à contrôler le placement de ses données, et à essayer d'ajuster au mieux l'ordonnancement pour que le calcul s'effectue proche des données.
Cette approche assez naturelle vise d'une part à réduire la contention au sein d'un même contrôleur mémoire, mais également à minimiser les accès distants coûteux.

\paragraph{HPF~\cite{HPF} (High Performance Fortran)} illustre très bien ces idées.
Cette extension de Fortran 90 ne concerne pas du tout les architectures NUMA, mais les concepts qui y sont développés ont servi d'inspiration pour nos travaux décrits dans le chapitre~\ref{chap:contrib:openmp}.

Le principe de cette extension est de distribuer la \emph{possession} de chaque élément des tableaux manipulés sur les différents cœurs, et de répartir la charge de travail (les instructions ou groupes d'instructions) sur les cœurs qui possèdent les éléments que ces instructions manipulent.

Ils proposaient un ensemble complet de fonctionnalités pour contrôler le découpage des données, qui pour certaines n'ont jamais été implémentées par les compilateurs du fait de leur complexité.

%Memory Affinity for Hierarchical Shared Memory Multiprocessors
\paragraph{Pousa Ribeiro et al.~\cite{Pousa2009}} propose une bibliothèque externe pour contrôler l'allocation de données sur les différents nœuds NUMA.
Ils proposent plusieurs types de mécanismes :
\begin{itemize}
  \item bind : un contrôle précis du nœud sur lequel doit être allouée la donnée
  \item cyclic : pour distribuer de manière cyclique les pages (ou groupes de pages) de la mémoire allouée sur un ensemble de nœuds.
  \item random : pour distribuer aléatoirement les pages (ou groupes de pages) de la mémoire allouée sur un ensemble de nœuds.
\end{itemize}

Contrairement à \emph{numactl}, cette bibliothèque permet d'avoir une distribution basée sur les blocs de données effectivement manipulés par l'application.
Elle peut s'utiliser soit via une modification du code, soit via un compilateur source à source.



%An Efficient OpenMP Loop Scheduler for Irregular Applications on Large-Scale NUMA Machines
\paragraph{Durand et al.~\cite{Durand2013}} abordent le thème de l'ordonnancement de boucles irrégulières sur machine NUMA.
D'une part ils utilisent le concept de \emph{domaine de voisinage} défini dans la section précédente pour avoir une vue hiérarchique de la machine, et assurent la distribution des données de l'application sur l'ensemble des \emph{domaines de voisinage}.
D'autre part ils étendent le support exécutif pour rendre les boucles \emph{adaptatives}~: lorsqu'un thread n'a plus d'itérations à exécuter, il peut aller voler un autre thread qui lui donnera une partie des itérations non traitées qui lui ont été attribuées.

Cette approche est intéressante dans notre contexte, car au delà de montrer qu'ils compensent l'irrégularité des itérations, ils montrent surtout l'importance de la distribution des données manipulées.

%Optimizing ccNUMA locality for task-parallel execution under OpenMP and TBB on multicore-based systems
\paragraph{Wittman et Hager~\cite{Wittmann2011}} proposent une approche très intéressante compte tenu des constructions disponibles en 2011 dans le standard OpenMP (tâches indépendantes).
La supposition de départ est une application dans laquelle chaque tâche lit et écrit un unique bloc de données.
Ils proposent l'introduction d'une structure similaire au \emph{domaine de voisinage} par nœud NUMA.
Dans cette structure ils stockent une file de blocs de données.

Dans un premier temps le programmeur distribue équitablement l'ensemble des blocs de données dans l'ensemble des files des \emph{domaines de voisinage}.
Ensuite il va créer autant de tâches que de blocs de données existants.
Lors de l'exécution, la tâche va déterminer dans quel \emph{domaine de voisinage} elle se trouve, et manuellement retirer un bloc de données de la file correspondante pour effectuer le calcul.

Si cette approche a le mérite d'améliorer significativement la localité des données, certains aspects sont assez contraignants.
D'une part le programmeur se retrouve à effectuer lui même des actions qui appartiennent typiquement au support exécutif, et pourraient être faites de manière transparente.
Et d'autre part, aucun contrôle n'est fait pour s'assurer que le nombre de tâches sur chaque \emph{domaine de voisinage} est le bon (seul le nombre global de tâches est assuré).
Ils résolvent ce problème en permettant à une tâche d'aller voler un bloc de données dans un autre voisinage, si le sien est vide.


%Dense Matrix Computations on NUMA Architectures with Distance-Aware Work Stealing
%4.2. Data distribution and locality detection The runtime is able to track the location of the data and schedule tasks in the node with
%the highest number of bytes. To track the locality, it assumes a first touch policy and looks for initialization tasks. The criteria to detect such tasks is plain and simple: tasks with an output dependency (at least one) where it is the first time that data will be written. In our Cholesky implementation those tasks are the ones that initialize a block of the matrix. Initialization tasks are scheduled in round robin across the available NUMA nodes, enabling
%us to use a similar data distribution. When a task of that type is executed, the data it initializes will be marked as located in the NUMA node of the running thread. Otherwise, when a non initialization task is submitted, the number of bytes accessed by each
%node will be computed, based on the dependency information provided by the programmer, and the task will be scheduled in the node with the largest amount of data. Note that once work stealing is introduced, the locality information becomes a hint that the
%runtime will always follow unless there is starvation in the local node. Kurzak [23] described an implementation of the Cholesky factorization using static schedul- ing where threads work only on a one-dimensional cyclic distribution in order to keep locality
\paragraph{Al-Omairy et al.~\cite{Al-Omairy2015}} ont concentré leurs travaux sur l'inversion de matrices symétriques et la factorisation de Cholesky.
Ils ont étendu le support exécutif et le modèle de programmation d'OmpSs afin de permettre de prendre en compte la distance aux données lors du vol de travail.
Leur premier axe est de restreindre le vol de travail à l'intérieur du nœud : les threads ne peuvent voler que dans une file commune par nœud NUMA.
Le second est de tracer la localité des données : ils utilisent des tâches d'initialisation pour détecter où sont allouées les données. Lors de l'exécution du programme cette information est utilisée pour ordonnancer une tâche sur le nœud où il y a le plus de données manipulées par la tâche.


\paragraph{Terboven et al.~\cite{Terboven2016}} ont effectués une première analyse d'un support de l'affinité pour les tâches OpenMP.
Leur étude s'est portée sur deux points~: l'affinité d'une tâche vers un thread ou une donnée, et l'affinité d'un groupe de tâches entre elles.
Le premier point est très proches de notre proposition de clause affinité~\cite{Virouleau2016b}, et les deux articles ont tous les deux été présentés lors d'IWOMP 2016.
Leur proposition d'affinité se base comme nous sur les réflexions du comité de standardisation, néanmoins certains points clés de l'implémentation divergent.
Pour déterminer l'emplacement d'une donnée (et satisfaire la demande d'affinité), ils se basent sur le placement des threads de calcul (\emph{OMP\_PLACES}) et sur les précédentes références à la même donnée. Ils cherchent ensuite à regrouper les tâches utilisant les même réferences ensembles.
De notre côté nous récupérons dynamiquement l'emplacement de l'allocation physique des données à l'aide d'un appel système.
Une autre différence importante est que dans leur approche une affinité est \emph{stricte} par défaut~: seuls les threads situé sur la ressource possédant la donnée peuvent exécuter la tâche.
Dans notre proposition l'affinité est un indice de placement que le support exécutif peut ignorer selon les besoins de l'ordonnancement, mais que le programmeur peut rendre obligatoire, rejoignant dans ce cas l'approche précédente.


%Scheduling Data Flow Program in XKaapi: A New Affinity Based Algorithm for Heterogeneous Architectures
%Design and analysis of scheduling strategies for multi-CPU and multi-GPU architectures
\paragraph{En contexte hétérogène} plusieurs travaux s'attaquent à l'ordonnancement de tâches en ciblant plusieurs GPU.
Le problème de localité des données que l'on rencontre sur les architectures NUMA est encore plus critique lorsque l'on cible ce type d'accélérateur.
Bleuse et al.~\cite{Bleuse2014}, ainsi que Lima et al.~\cite{Lima2015} étendent le vol de travail avec des stratégies ciblant les architectures multi-GPU.
Pour décider du placement d'une tâche prête, ils considèrent les données déjà présentes sur les différents GPUs, et placent la tâche dans la file du GPU sur lequel le plus de données sont présentes.

Ils montrent une réduction significative de la quantité de données transférée au cours de l'exécution, ce qui se traduit sur certaines classes d'application par un gain important de performance comparé aux stratégies basées sur HEFT ou du vol de travail standard.

\begin{todo}
  Ici la conclusion est un peu générale et je n'ai pas vraiment fini la description : je n'ai pas vraiment insisté sur les différences entre les travaux.
  @Thierry : une discussion avec toi permettrait sûrement de mieux rédiger ça.
\end{todo}


\subsection{Migration dynamique des données et conservation de la localité}\label{sec:rw:numa:thread-data}

Dans cette dernière catégorie, les travaux effectués vont chercher à garantir la localité des données en regroupant les calculs, mais aussi en déplaçant les données manipulées par les calculs pendant l'exécution, pour réduire les accès distants.
Cela peut être fait à deux niveaux : soit au niveau du support exécutif, soit au plus bas niveau, directement dans le noyau du système.
Dans le premier cas on a potentiellement une meilleure connaissance des données pour faire de meilleurs choix, mais cela restreint le type d'application ou impose une action de la part du programmeur.
Si l'on se place au niveau du noyau les informations ne sont disponibles qu'à l'exécution, et nécessitent une analyse. Cela a l'avantage de cibler n'importe quelle application sans la modifier, mais le potentiel de gain est un peu plus faible.

%ForestGOMP: an efficient OpenMP environment for NUMA architectures
\paragraph{ForestGOMP~\cite{Broquedis2010a}} est un support exécutif ciblant spécifiquement les architectures NUMA.
Ils constatent que le parallélisme que permet d'exprimer OpenMP~2.5 est plat, et que cela pose problème lors de l'exploitation des machines NUMA.
Étant donné que le parallélisme imbriqué peut être vu comme une forme de hiérarchie, ils proposent de superposer ce type de parallélisme sur la topologie des machines NUMA.
Pour cela ils regroupent les threads imbriqués en \emph{bulles}, qui pourront ensuite être ordonnancés sur différents niveaux de la hiérarchie mémoire.


ForestGOMP repose sur deux composants :
\begin{itemize}
  \item MARCEL : ce composant est en charge de l'ordonnancement des \emph{bulles} sur la hiérarchie de la machine. Son objectif est de les placer au mieux, c'est-à-dire faire en sorte qu'elles soient exécutées par des cœurs d'un même nœud, et les déplacer si l'équilibrage de charge le nécessite.
  \item MaMI : ce composant gère le placement et la migration des données manipulées par les \emph{bulles}.
Il prend en charge une distribution initiale des données des différentes \emph{bulles}, qui sera prise en compte par MARCEL lors du placement initial des \emph{bulles}.
Lors de l'équilibrage de charge, MaMI permet la migration des données associées à la \emph{bulle} en marquant les pages manipulées, et en forçant le déplacement physique des pages proches du prochain thread qui touchera une page marquée (\emph{next-touch}).
\end{itemize}

Ils montrent des gains significatifs de performance et une réduction de la contention sur des benchmarks populaires tels que STREAM~\cite{mccalpin1995}.

%Topology-Aware and Dependence-Aware Scheduling and Memory Allocation for Task-Parallel Languages
\paragraph{Drebes et al.~\cite{Drebes2014}} ont implémenté des stratégies d'ordonnancement ciblant les architectures NUMA dans OpenStream.
Le support exécutif fonctionne par vol de travail (avec une file de tâches par cœur), et récupère des informations sur la hiérarchie mémoire via hwloc lors de son initialisation.
Les tâches expriment leurs dépendances de données sur des \emph{buffers}, dans lesquels plusieurs d'entre elles peuvent écrire ou lire des données.
Ces dépendances permettent non seulement de déterminer l'ordre d'exécution des différentes tâches, mais également de connaître précisément l'ensemble des données manipulées par plusieurs tâches, ainsi que leur taille.

Leurs stratégies visent à améliorer la localité et la distribution des données au cours de l'exécution en jouant sur plusieurs facteurs.
\begin{itemize}
  \item Lors de la sélection d'une file de tâches à voler, la stratégie va favoriser le vol sur les cœurs voisins.
  \item Lors du placement des tâches prêtes, la stratégie va déterminer le nœud sur lequel sont alloués ses \emph{buffers} de données en entrée, et placer la tâche sur un cœur de ce nœud.
  \item La gestion de l'allocation des \emph{buffers} des tâches est faite de manière à garder le maximum de données locales.
Lorsque l'allocation d'un \emph{buffer} est nécessaire (par exemple avant l'exécution d'une tâche qui écrit dedans), le support exécutif va regarder les différentes tâches qui contribuent à ce \emph{buffer} ainsi que la taille de leur contribution. À partir de là ils définissent la notion de \emph{dépendance forte} sur la tâche qui contribue le plus, et alloue le \emph{buffer} sur le nœud où va s'exécuter cette tâche.
Si la tâche à l'origine de l'allocation a été volée, l'allocation est faite localement et ignore la \emph{dépendance forte} : cela permet de corriger des problèmes constatés sur des schémas de dépendances en forme d'arbre.
\end{itemize}

Ils montrent que la combinaison de ces facteurs permet d'augmenter significativement le rapport du nombre d'accès locaux sur le nombre d'accès distants, et ce sur des classes d'applications variées.




%kMAF: Automatic Kernel-level Management of Thread and Data Affinity
\paragraph{Diener et al.~\cite{Diener2014}} proposent d'attaquer le problème de la localité des données au sein même du système d'exploitation.
Ils implémentent, au sein du noyau Linux, un mécanisme d'observation et d'analyse des accès effectués aux pages mémoire par les différents threads.
À partir de ces observations, ils peuvent effectuer deux opérations : d'une part décider de migrer certaines pages mémoire sur un nœud différent, en modifiant directement la table des pages.
D'autre part ils peuvent décider de migrer certains threads sur un nœud NUMA plus adapté.
Ils montrent que la combinaison de ces deux opérations permet d'améliorer significativement les performances de certains benchmarks courants, sans avoir à effectuer de modifications au niveau du code source.

%Design and Implementation of Bandwidth-aware Memory Placement and Migration Policies for Heterogeneous Memory Systems
Des travaux similaires ont été effectués par Yu et al.~\cite{Yu2017} sur des architectures disposant de mémoires hétérogènes.



\subsection*{Conclusion}

Tous ces travaux ont eu un impact sur les idées que nous avons développés au cours de la thèse, mais ceux prenant en compte à la fois la hiérarchie mémoire, le placement des calculs par rapport aux données, et le contrôle du placement des données nous semblent les plus intéressants dans notre contexte.

Les travaux que nous mettons en avant dans le chapitre~\ref{chap:contrib:openmp} se basent sur une combinaison des idées évoquées~: nous suivons une approche où l'on tire parti des informations présentes concernant la hiérarchie de la machine, et où nous proposons de contrôler à la fois le placement des données, et à la fois la localité des données tout au long de l'exécution de l'application.

Contrairement aux travaux décrits dans cette section, nous avons accès à plus d'informations via les dépendances des tâches. Nous proposons également une clause supplémentaire pour permettre à l'utilisateur de fournir des informations explicites concernant l'affinité éventuelle entre les tâches et leurs données.
Cela nous permet donc d'effectuer des choix pertinents vis à vis de l'application, en nous basant sur l'emplacement physique réel des données des tâches, que l'utilisateur peut également distribuer lors de l'initialisation de l'application via une autre clause spécifique.
