\section{Techniques d'amélioration de la localité des données}\label{sec:rw:numa}

\subsection{Introduction}

La problématique de la localité des données a été abordée dans plusieurs contextes.
Rien qu'en prenant les travaux se restreignant aux architectures NUMA, on obtient une quantité d'articles qui est suffisamment grande pour qu'il existe plusieurs papiers résumant ces travaux et synthétisant les avancées.

%NUMA-aware scheduling for both memory- and compute-bound tasks
\paragraph{Reinman~\cite{Reinman2015}} a regroupé différents travaux selon le type de tâches qu'ils ciblaient, et propose en conclusion des recommandations lorsque l'on cherche à ordonnancer des programmes à base de tâches sur des machines NUMA.

L'auteur distingue principalement deux types de tâches : compute-bound et memory-bound.
Si certaines des 8 recommandations sont très spécifiques aux types de tâches, d'autres décrivent bien les points qu'il faut avoir en tête lorsque l'on cible les architectures NUMA.
Il indique notamment l'importance de la distribution des données de l'application sur l'ensemble des nœuds NUMA, l'importance de la localité des données, ainsi que l'importance de garder les tâches et leur données ensemble lors du vol de travail.

Dans le cadre spécifique des tâches OpenMP, Terboven et al.~\cite{Terboven2012} ont fait une étude du comportement général des tâches sur les architectures NUMA, et donne des indications assez générales sur les bases pour aborder le problème, comme par exemple assurer le placement fixe d'un thread sur sur cœur de calcul, ou encore les types d'ordonnancement prévus dans le standard qui sont le plus efficace dans ce cas.

Décrire l'ensemble des travaux qui ont été effectués dans ce domaine n'est pas possible, voir dans certains cas n'est pas pertinent, puisque les architectures et langages évoluent.

Les sections suivantes décrivent quelques approches standards et des travaux représentatifs de ces techniques, qui ont eu un impact sur le développement de nos travaux décrits dans les chapitres~\ref{chap:contrib:characterization} et~\ref{chap:contrib:openmp}.



\subsection{Grouper les calculs ensemble}

Les techniques utilisées par les travaux suivant ont tous en commun le groupement des calculs sur des éléments topologiquement proches dans la hiérarchie.
L'objectif derrière cette idée est de "cloisonner" la charge de travail (par exemple les itérations de boucles) afin de limiter les accès distant et encourager la réutilisation du cache.

%OpenMP task scheduling strategies for multicore NUMA systems
\paragraph{Olivier et al.~\cite{Olivier2012, Olivier2013}} font l'état des problèmes se posant lors de l'équilibrage de charge sur les architectures NUMA.
Ils notent en particulier que s'il permet de limiter l'inactivité de certains cœurs, il entraine aussi une augmentation des cache miss ainsi qu'une augmentation des accès distants, plus coûteux.

Leur principale contribution s'oriente autour d'une stratégie d'ordonnancement prenant en compte la hiérarchie de la mémoire au sein de l'architecture.
Ils définissent un domaine de voisinage, qu'ils appellent \emph{shepherd}, qui regroupe un ensemble de cœur voisins dans la hiérarchie.
Cet ensemble peut être défini à plusieurs niveaux : il peut être défini pour un seul cœur, pour un ensemble de cœur partageant le même cache L3, ou pour un socket comprenant plusieurs processeurs multicœurs.

Ils proposent ensuite une extensions au vol de travail : ils définissent un \emph{shepherd} (et donc une queue de tâches) par nœud NUMA, et ils proposent que chaque \emph{shepherd} aille voler un ensemble de tâches lorsque leur propre queue de tâche est vide.
Au sein d'un \emph{shepherd}, le travail est réparti aux threads associés aux cœurs d'une manière LIFO, qui permet un ordonnancement en profondeur dans le but de rester au plus proche de l'exécution et de favoriser la réutilisation des caches.


%Evaluation of OpenMP Task Scheduling Algorithms for Large NUMA Architectures
\paragraph{Clet-Ortega et al.~\cite{Clet2014}} présentent des techniques de vols de travail favorisant les vols locaux plutôt que distant.
Ils constatent qu'une queue centralisée introduit un fort coût de gestion compte tenu du fait que le nombre de tâches créées peut être élevé, et proposent l'utilisation d'une queue de tâche par thread du support exécutif pour pallier ce problème.
Cela introduit du même coup une vision décentralisée de la gestion des tâches, et en couplant ça avec une analyse de la topologie via hwloc, ils introduisent la notion de voisins pour un thread.

La deuxième partie de leur contribution consiste à étendre le vol de travail pour privilégier les vols dans les queues voisines, restreignant ainsi les calculs sur des groupes de cœurs physiques proche.
%Towards Efficient OpenMP Strategies for Non-Uniform Architectures
Cette approche est reprise par Tahan et al.~\cite{Tahan2014}. Chaque thread dispose d'une liste privée des autres queues de threads, triée par la distance au cœur sur lequel s'exécute le thread.



\paragraph{Pilla et al.~\cite{Pilla2014}} proposent des améliorations aux algorithmes de liste ciblant spécifiquement les architectures NUMA.
À chaque point d'ordonnancement, leur ordonnanceur va tenter de corriger un déséquilibre de charge en déplaçant des tâches d'un cœur à un autre.
Pour évaluer si une tâche devrait être déplacer sur un autre cœur, ils prennent en compte plusieurs facteurs : la charge du cœur, la quantité de communication que la tâche effectue avec d'autres tâche du cœur, le facteur NUMA entre les deux cœurs, et la quantité de communication locale que la tâche effectuerait sur son cœur actuel.
Pour résumé ils vont favoriser le rééquilibrage de charge sur des cœurs voisins, et considérer un équilibrage sur un cœur distant si les caractéristiques de la tâche le permettent.

\begin{todo}
@François : vu que tu es dans les co auteurs je veux bien que tu me confirmes que j'ai bien saisi les travaux de Laércio et que je ne raconte pas des bêtises
\end{todo}


\subsection{Distribution des données et déplacement des calculs}

Nous avons ici regroupé les techniques consistant à contrôler le placement de ses données, et à essayer d'ajuster au mieux l'ordonnancement pour que le calcul s'effectue proche des données.
Cette approche assez naturelle vise d'une part à réduire la contention au sein d'un même contrôleur mémoire, mais également à minimiser les accès distants coûteux.

\paragraph{HPF~\cite{HPF} (High Performance Fortran)} illustre très bien ces idées.
Cette extension de Fortran 90 ne concerne pas du tout les architectures NUMA, mais les concepts qui y sont développés ont servi d'inspiration pour nos travaux décrit dans le chapitre~\ref{chap:contrib:openmp}.

Le principe de cette extension est de distribuer la \emph{possession} de chaque élément des tableaux manipulés sur les différents cœurs, et de répartir la charge de travail (les instructions ou groupe d'instructions) sur les cœurs qui possèdent les éléments que ces instructions manipulent.


%An Efficient OpenMP Loop Scheduler for Irregular Applications on Large-Scale NUMA Machines
\paragraph{Durand et al.~\cite{Durand2013}} abordent le thème de l'ordonnancement de boucles irrégulières sur machine NUMA.
D'une part ils utilisent le concept de \emph{domaine de voisinage} définie dans la section précédente pour avoir une vue hiérarchique de la machine, et assurent la distribution des données de l'application sur l'ensemble des \emph{domaines de voisinage}.
D'autre part ils étendent le support exécutif pour rendre les boucles \emph{adaptatives}~: lorsqu'un thread n'a plus d'itérations à exécuter, il peut aller voler un autre thread qui lui donnera une partie des itérations qui lui ont été attribué.

Cette approche est intéressante dans notre contexte, car au delà de montrer qu'ils compensent l'irrégularité des itérations, ils montrent surtout l'importance de la distribution des données manipulées.

%Optimizing ccNUMA locality for task-parallel execution under OpenMP and TBB on multicore-based systems
\paragraph{Wittman et Hager~\cite{Wittmann2011}} proposent une approche très intéressante compte tenu des constructions disponibles en 2011 (tâches indépendantes).
La supposition de départ est une application dans laquelle chaque tâche lit et écrit un unique bloc de données.
Ils proposent l'introduction d'une structure similaire au \emph{domaine de voisinage} par nœud NUMA.
Dans cette structure ils stockent une queue de blocs de données.

Dans un premier temps le programmeur distribue équitablement l'ensemble des blocs de données dans l'ensemble des queues des \emph{domaine de voisinage}.
Ensuite il va créer autant de tâches que de blocs de données existant.
Lors de l'exécution, la tâche va déterminer dans quelle \emph{domaine de voisinage} elle se trouve, et manuellement retirer un bloc de données de la queue correspondante pour effectuer le calcul.

Si cette approche a le mérite d'améliorer significativement la localité des données, certains aspects sont assez contraignant.
D'une part le programmeur se retrouve à effectuer lui même des actions qui appartiennent typiquement au support exécutif, et pourrait être faites de manière transparentes.
Et d'autre part, aucun contrôle n'est fait pour s'assurer que le nombre de tâches sur chaque \emph{domaine de voisinage} est le bon (seul le nombre global de tâches est assurés).
Ils résolvent ce problème en permettant à une tâche d'aller voler un bloc de données dans un autre voisinage, si le sien est vide.


%Dense Matrix Computations on NUMA Architectures with Distance-Aware Work Stealing
%4.2. Data distribution and locality detection The runtime is able to track the location of the data and schedule tasks in the node with
%the highest number of bytes. To track the locality, it assumes a first touch policy and looks for initialization tasks. The criteria to detect such tasks is plain and simple: tasks with an output dependency (at least one) where it is the first time that data will be written. In our Cholesky implementation those tasks are the ones that initialize a block of the matrix. Initialization tasks are scheduled in round robin across the available NUMA nodes, enabling
%us to use a similar data distribution. When a task of that type is executed, the data it initializes will be marked as located in the NUMA node of the running thread. Otherwise, when a non initialization task is submitted, the number of bytes accessed by each
%node will be computed, based on the dependency information provided by the programmer, and the task will be scheduled in the node with the largest amount of data. Note that once work stealing is introduced, the locality information becomes a hint that the
%runtime will always follow unless there is starvation in the local node. Kurzak [23] described an implementation of the Cholesky factorization using static schedul- ing where threads work only on a one-dimensional cyclic distribution in order to keep locality
\paragraph{Al-Omairy et al.~\cite{Al-Omairy2015}} ont concentré leurs travaux sur l'inversion de matrices symétriques et la factorisation de Cholesky.
Ils ont étendu le support exécutif et le modèle de programmation d'OmpSs afin de permettre de prendre en compte la distance aux données lors du vol de travail.
Leur premier axe est de restreindre le vol de travail à l'intérieur du noeud : les threads ne peuvent voler que dans une queue commune au noeud NUMA.
Le second est de tracer la localité des données : ils utilisent des tâches d'initialisations pour détecter où sont allouées les données. Lors de l'exécution du programme cette information est utilisée pour ordonnancer une tâche sur le noeud où il y a le plus de données manipulées par la tâche.




%Scheduling Data Flow Program in XKaapi: A New Affinity Based Algorithm for Heterogeneous Architectures
%Design and analysis of scheduling strategies for multi-CPU and multi-GPU architectures
\paragraph{Dans un contexte hétérogène} plusieurs travaux s'attaquent à l'ordonnancement de tâches en ciblant plusieurs GPU.
Le problème de localité des données que l'on rencontre sur les architectures NUMA est encore plus critique lorsque l'on cible ce type d'accélérateur.
Bleuse et al.~\cite{Bleuse2014}, ainsi que Lima et al.~\cite{Lima2015} étendent le vol de travail avec des stratégies ciblant les architectures multi-GPU.
Pour décider du placement d'une tâche prête, ils considèrent les données déjà présentes sur les différents GPUs, et place la tâche dans la queue du GPU sur lequel le plus de données sont présentes.

Ils montrent une réduction significative de la quantité de données transférée au cours de l'exécution, ce qui se traduit sur certaines classes d'application par un gain important de performance comparé aux stratégies basées sur HEFT ou du vol de travail standard.

\begin{todo}
  Ici la conclusion est un peu générale et je n'ai pas vraiment fini la description : je n'ai pas vraiment insisté sur les différences entre les travaux.
  @Thierry : une discussion avec toi permettrait sûrement de mieux rédiger ça.
\end{todo}


\subsection{Migrations de données et des threads}\label{sec:rw:numa:thread-data}


%ForestGOMP: an efficient OpenMP environment for NUMA architectures
\paragraph{ForestGOMP~\cite{Broquedis2010a}} est un support exécutif ciblant spécifiquement les architectures NUMA.
Ses fonctionnalités incluent un ordonnanceur de threads hiérarchiques, groupés selon les équipes de threads OpenMP afin de maximiser la localité des données.

\cite{Yu2017}, Design and Implementation of Bandwidth-aware Memory Placement and Migration Policies for Heterogeneous Memory Systems


\cite{Drebes2014}, Topology-Aware and Dependence-Aware Scheduling and Memory Allocation for Task-Parallel Languages

\cite{Pousa2009}, Memory Affinity for Hierarchical Shared Memory Multiprocessors
Librairie utilisateur d'allocation+ distribution de données selon une variété de stratégies pour s'adapter à l'application :
 - bind (restriction à certains noeuds), cyclic (avec possibilité de bloc, ou de skew (répartition non linéaire, i + i/M +1 mod M)), random (par bloc aussi)



(vieux?)\cite{Weng2002}, Implementing OpenMP Using Dataflow Execution Model for Data Locality and Efficient Parallel Execution

(vieux?)\cite{Nikolopoulos2001}, Exploiting Memory Affinity in OpenMP Through Schedule Reuse


\subsection*{Conclusion}

TODO

Beaucoup de tâche indépendantes
L'ajout des dépendances avec OpenMP~4.0 amène un nouvel ensemble d'informations concernant l'utilisation des données par les tâches, qui n'a pas été traités par les travaux mentionnés.


%\subsection{TODO : inclassés}

%\cite{Selva2015}, (thèse) Performance Monitoring of Throughput Constrained Dataflow Programs Executed On Shared-Memory Multi-core Architectures

%\cite{Zhuravlev2012}, Survey of Scheduling Techniques for Addressing Shared Resources in Multicore Processors
%Travaux point de vue OS, pour un processeur multicoeurs (UMA !).
%Contention-aware scheduler.
%Cool, mais seulement à l'intérieur d'un noeud.


%\cite{Marowka2004}, OpenMP-oriented applications for distributed shared memory architectures
  %%\item NUMA (with ref to the "main" chapter)
  %%\item task + Workstealing (with ref to the "main" chapter)


%\cite{Tchiboukdjian2010}, A Work Stealing Scheduler for Parallel Loops on Shared Cache Multicores

