\begin{savequote}[6cm]
<< That's all folks! >>
\end{savequote}

\chapter{Conclusion et perspectives}\label{chap:conclusion}
\chaptertoc

Le calcul haute performance répond aux besoins toujours plus grand de la simulation, et les supercalculateurs embarquent un nombre très important de machines à mémoire partagée.
La tendance dans ces machines est à la multiplication du nombre de cœurs de calcul plutôt qu'à l'augmentation de leur fréquence.
Les architectures ont donc évolué, et pour répondre aux besoins en terme d'accès à la mémoire elles sont maintenant découpées en plusieurs nœuds, regroupant des cœurs de calcul et de la mémoire.
Cela introduit du même coup un temps d'accès à la mémoire non uniforme pour les différents cœurs, d'où le qualificatif NUMA --- Non Uniform Memory Access --- pour ce type d'architectures.

Pour en faire une exploitation efficace, les applications scientifiques devraient prendre en compte la hiérarchie mémoire imposée par le matériel.
Mais cela n'est possible que si les modèles de programmation et les supports exécutifs utilisés par ces applications évoluent pour supporter ces architectures.
Les modèles de programmation à base de tâche sont particulièrement adaptés pour cibler les architectures NUMA~: l'aspect dynamique de l'ordonnancement d'un graphe de tâches permet d'assurer un équilibrage de charge efficace même lors d'irrégularités lors des calculs.
De plus cela permet d'exprimer un parallélisme à grain fin, nécessaire pour fournir du travail au nombre important de cœurs de calcul.

OpenMP est le standard \emph{de-facto} pour les architectures à mémoire partagée.
Les évolutions du langage ont montré qu'il pouvait s'adapter au nouveau matériel, et il propose maintenant un ensemble de fonctionnalités autour du paradigme de la programmation par tâches.
Néanmoins les concepts spécifiques aux architectures NUMA sont encore peu présents~: bien que des efforts récents aient été faits pour contrôler le placement des threads de calcul sur la topologie de l'architecture, il n'y a rien pour aider le support exécutif à conserver la localité des données au cours de l'ordonnancement.

\section*{Contributions de cette thèse}

Les travaux de cette thèse ont porté sur plusieurs facettes liés au développement et à l'exécution d'applications scientifiques sur les architectures NUMA.

Dans un premier temps nous avons introduit \outil, un outil permettant de faciliter la caractérisation de machines et d'applications scientifiques, à travers des scénarios définis par l'utilisateur.
Nous l'avons utilisé pour évaluer la hiérarchie mémoire de nos machines d'expérimentation, idchire et brunch.
Nous l'avons également utilisé dans le contexte d'une étude de cas sur la factorisation de Cholesky, où il nous a permis d'évaluer précisément le comportement des quatre noyaux sur lesquels la factorisation repose.
Nous avons confirmé et mesuré l'impact de la localité des données sur ces noyaux, et identifié un axe possible d'amélioration pour l'ordonnancement de ce type d'application.

Dans un second temps nous avons donc introduit des extensions au modèle de programmation utilisé pour cette application, OpenMP, permettant aux programmeurs~: de contrôler la distribution des données lors de l'initialisation de l'application, et d'exprimer une \emph{affinité} entre une tâche et une donnée ou un élément de la topologie.

Nous avons implémenté ces propositions dans le compilateur Clang.
Ces changements ont été accompagnés de l'extension de deux supports exécutifs~: XKaapi, un support exécutif expérimental, et libOMP, le support exécutif grand public de l'infrastructure LLVM.
L'objectif des extensions a été à la fois de pouvoir utiliser l'affinité exprimée dans le programme, mais aussi d'exploiter les plusieurs niveaux de hiérarchie présentés par la machine lors du vol de travail.
Les résultats des expériences que nous avons menées ont confirmé et ont permis d'apprécier le gain de performances lié à l'affinité sur un ensemble d'applications.
Ils ont aussi permis de mettre en avant que l'intérêt de l'affinité était principalement lié à la taille des données manipulées par les tâches.

Enfin nous avons pu commencer des développements autour d'un simulateur, avec deux objectifs principaux~: pouvoir tester de nouvelles heuristiques sans avoir de lourds coûts de développement à payer dans les supports exécutifs~; et pouvoir comparer les performances réelles des supports exécutifs par rapport à ce qu'il serait théoriquement possible d'atteindre.
Les résultats préliminaires obtenus sont encourageants et confirment l'intérêt d'une telle approche.


\section*{Perspectives}

Au delà de la simulation mentionnée dans la section précédente, cette thèse ouvre plusieurs perspectives.

La première et la plus importante semble être \textbf{l'amélioration de la collaboration entre le compilateur et le support exécutif}.
Le compilateur peut dans certains cas avoir accès à des informations critiques concernant les différentes tâches.
En supposant qu'il ait accès à toute l'application, il peut alors connaître la taille des données manipulées en fonction de l'instance de l'application considérée, ainsi que le type d'opérations effectuées sur ces données.
Transmettre ces informations au support exécutif pourrait lui permettre de prendre de meilleures décisions quand au placement des tâches.
Une piste que nous n'avons pas eu le temps d'explorer concerne l'\emph{intensité opérationnelle} des tâches (le nombre d'instructions effectuées par rapport aux données utilisées)~: a priori une tâche avec une grande intensité serait plus <<rentable>> à voler qu'une tâche de faible intensité, où la performance est complètement dirigée par le temps d'accès à ses données.

Une deuxième perspective serait \textbf{d'appliquer des idées similaires dans un contexte hétérogène}.
L'arrivée de constructions pour supporter les accélérateurs dans OpenMP offre une porte d'entrée standard pour attaquer des supports exécutifs hétérogènes.
Les idées développées au cours de cette thèse pourraient être étendues pour attacher des informations aux constructions \emph{target}, et permettre au support exécutif de faire des choix pertinents vis à vis des tâches à exécuter sur le ou les accélérateurs.

Enfin les architectures vont continuer à évoluer, et les fabricants eux même ne sont pas toujours sûr de la manière de se préparer à l'exascale.
Après avoir sorti en 2012 le \emph{Xeon Phi}, un processeur multicœurs sous forme d'accélérateur avec une soixantaine de cœurs physiques, Intel a rapidement annoncé une deuxième mouture du processeur montée sur socket l'année suivante, et a finalement annoncé l'abandon des nouvelles versions sous forme d'accélérateur en Août 2017.
Les deux gammes de produits proposées par Intel et ciblées HPC sont donc maintenant les \emph{Xeon Phi} sur socket et les \emph{Xeon Scalable}, et ils viennent s'intégrer dans une solution complète pour le HPC (Intel Scalable System Framework), où ils sont associés aux autres solutions qu'Intel propose pour chacun des composants critiques (disques durs, interconnexion).
Le simulateur que nous avons développé peut donc ici répondre à un besoin de tester les nouvelles architectures~: il peut permettre de donner un aperçu des performances des applications compte tenu des algorithmes d'ordonnancement connu, et peut également servir pour prototyper de nouveaux algorithmes d'ordonnancement.

Compte tenu des changements fréquent en terme de topologie des machines, les propositions qui ont été faites au cours de cette thèse constituent une bonne base pour permettre aux supports exécutifs de s'adapter au nouveau matériel.
