\section{Amélioration de l'expressivité du langage}\label{sec:openmp:langage}

\subsection{Description du besoin}

Dans un contexte où l'on souhaite améliorer le contrôle sur les tâches et leurs données, OpenMP~4.0 ne propose pas grand chose d'efficace.
Les fonctionnalités les plus proche que nous pourrions utiliser sont les |OMP_PLACES| et |OMP_PROCBIND|, mais cela n'a d'effet que sur le placement des threads sur la topologie, et non sur les tâches qui leur sont attribuées. Il n'y a rien au sein d'OpenMP qui permet d'exprimer une relation entre une tâche et une partie de la topologie de la machine.

En dehors d'OpenMP, les programmeurs utilisent généralement des bibliothèques ou outils externes dans le but de contrôler le placement des données~\cite{Pousa2009, Broquedis2010a}.
Ces approches peuvent être soit inefficaces (comme dans le cas de \emph{numactl}), ou relativement intrusive (dans le cas de l'utilisation d'une bibliothèque externe).

OpenMP pourrait donc bénéficier de deux type de constructions dont les utilisateurs auraient besoin : d'une part un moyen de contrôler la distribution initiale des données manipulées par les tâches, et d'autres part un moyen d'associer les tâches à ces données (ou mieux, à n'importe quel partie de la machine).

\subsection{Contrôle de la distribution des données}\label{sec:openmp:langage:init}

De la même manière que le programmeur exprime son applications à base de tâches, on va supposer ici aussi qu'il initialise les données de son application à l'aide de tâches, qu'il est raisonnable de supposer indépendantes, dans une région parallèle séparée.

Dans la plupart des applications que nous avons utilisé nous avons constaté que l'initialisation des données suivait l'un des deux schémas suivant :
\begin{itemize}
    \item Allocation d'un bloc mémoire couvrant la totalité des besoins de l'application, puis initialisation par blocs en fonction des accès lors des calculs.
  \item Allocation et initialisation de chacun des blocs au fur et à mesure des besoins ou de la construction des structures de données de l'application.
\end{itemize}

Dans les deux cas l'initialisation des données devant être groupées ensemble a lieu dans une même tâche, il suffit alors de pouvoir gérer la distribution de ces tâches avant l'exécution pour effectivement distribuer les données (en se basant sur le principe du \emph{first-touch}, décrit dans la section~\ref{sec:context:numa:os}).

Nous avons donc mis en place une clause s'appliquant sur une région parallèle, pouvant spécifier une distribution par défaut des tâches sur la topologie de l'architecture :

\begin{lstlisting}
init(random | cyclic | cyclicnuma)
\end{lstlisting}

Elle indique à l'ordonnanceur de tâche que pour la région parallèle courante les tâches prêtes devraient être distribuées sur la machine en suivant une stratégie :

\begin{description}
  \item [random :]
    distribution aléatoire sur les queues des cœurs de la machine.
  \item [cyclic :]
    distribution de manière cyclique sur les queues des cœurs de la machine.
  \item [cyclicnuma :]
    distribution cyclique sur les queues des nœuds de la machine, ou à défaut de manière cyclique sur les premiers cœurs de chaque nœud de la machine.
\end{description}


Bien qu'ayant certaines restriction, cet ajout permet au programmeur de spécifier une distribution de données avec une modification minimale du code.
Comme les restrictions initiales peuvent être trop fortes pour certaines applications, il est aussi possible pour les cas particuliers de spécifier une clause affinité stricte (définie dans la section suivante) sur les tâches d'initialisation.

\subsection{Ajout d'une clause affinité}\label{sec:openmp:langage:affinity}

Cette partie détaille la proposition d'introduction du mot clé |affinity| dans le langage OpenMP, et a été présenté lors du Workshop International sur OpenMP (IWOMP) en 2016~\cite{Virouleau2016b}.
Comme constaté dans le chapitre~\ref{chap:contrib:characterization} et souvent mentionné dans la littérature, un point clé pour obtenir de bonnes performances sur des architectures NUMA est de garantir la proximité entre une tâche et ses ressources.

L'objectif de cette clause est donc de permettre à l'utilisateur de pouvoir spécifier un lien privilégié - une \emph{affinité} - entre une tâche et un élément de l'architecture.
On distingue donc trois types d'affinité que le programmeur pourrait avoir besoin d'exprimer :

\begin{description}
    \item [affinité à un thread :]
      le support exécutif devrait essayer d'ordonnancer la tâche sur le thread donné.
    \item [affinité à un nœud NUMA :]
      le support exécutif devrait essayer d'ordonnancer la tâche sur n'importe
      quel thread du nœud NUMA donné.

    \item [affinité à une donnée :]
      quand une tâche devient prête pour l'exécution, le support exécutif devrait
      l'ordonnancer sur n'importe quel thread attaché au nœud NUMA sur lequel
      la donnée a été physiquement allouée.
\end{description}

De plus, le programmeur peut indiquer si cette affinité est \emph{stricte}, indiquant que la tâche \textbf{doit} s'exécuter sur la ressource indiquée.
Si le programmeur n'indique pas une affinité stricte, l'ordonnanceur peut décider d'exécuter la tâche sur une ressource différente, pour assurer l'équilibrage de charge par exemple.

Cette extension visant les constructions de type tâche, elle a été implémentée comme une nouvelle clause pour la directive |task|. La syntaxe proposée est la suivante~:

\begin{lstlisting}
affinity([node | thread | data]: expr[, strict])
\end{lstlisting}

Dans tous les cas l'expression |expr| est un entier naturel, mais en fonction du type d'affinité l'entier est interprété d'une manière spécifique :

\begin{description}
  \item [thread :]
    |expr| est interprétée comme un id de thread. On définie ici la notion d'id de thread comme l'indice du thread au sein des |OMP_PLACES| pour la \textit{team} OpenMP courante.
    Voici quelques exemples d'utilisation, en prenant comme valeur |OMP_PLACES="{2},{5},{8},{9}"| :
    \begin{itemize}
      \item Le thread d'id |0| désigne celui s'exécutant sur le cœur |2|.
      \item Pour accéder au thread s'exécutant sur le cœur |8|, il faut utiliser l'id de thread |2|.
      \item Dans cet exemple, les id de thread peuvent varier entre |0| et |3| inclus.
      \item Il n'est pas possible de spécifier une affinité avec le thread situé sur le cœur |0| par exemple, puisqu'il n'est pas dans la \emph{team}.
    \end{itemize}
  \item [node :]
    |expr| est interprétée comme un id de nœud NUMA. Comme pour le cas précédent, la notion d'id est définie relativement aux places de la \textit{team} OpenMP courante.
    En reprenant l'exemple précédent, supposons que les cœurs 2,5,8, et 9 sont physiquement situés sur 2 nœuds NUMA différents. Il y aura alors 2 nœuds NUMA déduits des places, et les ids utilisés pourront être 0 ou 1.
  \item [data :]
    |expr| est interprétée comme une adresse mémoire. Si le nœud NUMA associé à la donnée ne peut être déterminé, le nœud utilisé par défaut est le premier dans la \textit{team} OpenMP.
\end{description}

Si |expr| désigne une ressource hors limites, la valeur considérée par le support exécutif est prise modulo le nombre de ressources correspondantes.

\subsection{Extension de l'API}

Si les points précédents décrivent des extensions directement au niveau des constructions OpenMP, il est également important de pouvoir fournir dynamiquement certaines informations au programmeur au cours de l'exécution du programme.
Dans ce but nous avons également ajouté quelques fonctions à l'API d'OpenMP, dont le but est de fournir des informations à propos de l'architecture et de la \emph{team} OpenMP courante :

\begin{lstlisting}
// Retourne le nombre de nœuds NUMA dans la team
omp_get_num_nodes(void);

// Retourne le nœud NUMA sur lequel
// la tâches est actuellement exécutée
omp_get_node_num(void);

// Retourne le nœud NUMA sur lequel la donnée a été allouée
omp_get_node_from_data(void *ptr);
\end{lstlisting}

Ces fonctions retournant des informations spécifiques à une \emph{team} OpenMP, elles ne peuvent être appelées qu'au sein d'une région parallèle.
Sur les machines sans support NUMA, nous considérons que tous les threads sont sur un unique nœud NUMA.

Nous avons également rendu accessible l'ajout d'affinité sur une tâche à une fonction de l'API :
\begin{lstlisting}
omp_set_task_affinity( 
     omp_affinitykind_t k, uintptr_t ptr, int strict);
\end{lstlisting}
Cette fonction aura un impact sur la prochaine tâche créée dans la région.
Les paramètres de la fonction correspondent aux paramètres de la clause :

\begin{description}
  \item [omp\_affinitykind\_t k] peut être soit |omp_affinity_thread|, |omp_affinity_node|, ou |omp_affinity_data|.
  \item [uintptr\_t ptr] correspond à une expression désignant la ressource.
  \item [int strict] indique si l'affinité est stricte ou non.
\end{description}


\subsection*{Conclusion}

Le programmeur peut, à travers les extensions proposées, exprimer à la fois la distribution de données qu'il désire, ainsi que la manière dont la localité des données devrait être maintenu au cours de l'exécution de l'application.

\begin{todo}
  il faut que je résume ce que le programmeur donne, là où on l'a implémenté, et que j'ouvre sur le fait qu'on l'utilise au runtime pour enchainer sur la section suivante.
\end{todo}

%L'implémentation proposée pour |omp_get_node_from_data| repose sur les informations disponibles à travers l'appel système de Linux |get_mempolicy|.

%We implemented these extensions in the Clang compiler, based on the 3.8 version\footnote{https://github.com/viroulep/clang}; and we also added the corresponding entry points in Clang's OpenMP runtime\footnote{https://github.com/viroulep/openmp}.

%Please note only the entry points have been implemented in Clang's OpenMP runtime, the actual runtime support has only been implemented in our OpenMP runtime and is described in the following section.


%ouvrir sur support exécutif
