\begin{savequote}[6cm]
<< What do you work on?  >>
\qauthor{someone}
\end{savequote}
\chapter{Introduction}
\chaptertoc

L'évolution du calcul haute performance est aujourd'hui dirigée par les besoins des applications de simulation.

Ces applications sont omniprésentes dans l'industrie, et concerne parfois même directement le grand public.
Par exemple pour la météo, les prévisions sont faites à l'aide d'applications simulant les intéractions entre les différents éléments de l'atmosphère.
Il en va de même pour tout ce qui se rapporte à la prévision de réaction de différents bâtiments en cas de séismes.

Dans un cadre plus industriel, les secteurs comme l'aéronautique, les applications militaires, ou encore le nucléaire ont un grand besoin en terme de simulations.

Toutes ces simulations sont au final exécutées sur des supercalculateurs, et il n'y a pas de limite aux nombres de ressources qu'elle peuvent utiliser : que ce soit pour améliorer la précision de la simulation, ou augmenter l'ensemble simulé, elles pourront toujours bénéficier d'un plus grand nombres de ressources.

De plus un grand nombre de ressources peut également permettre d'exécuter un plus grand nombre de simulations simultanément.

Si les supercalculateurs peuvent proposer plusieurs milliers de coeurs, ils sont en fait composés d'un grand nombre de petite machine avec un nombre de coeurs beaucoup plus faible.
Ces machines peuvent proposer plusieurs types de ressources, tels que des processeurs (CPU) standard, ou des accélérateurs plus ou moins spécifiques comme des GPUs ou des FPGA.

La très grande majorité des machines "basiques" (TODO : reformuler) possèdent plusieurs processeurs.

Ces derniers ont subit des gros changements d'évolution entre le milieu des années 2000 et maintenant : jusqu'au milieu des années 2000, les fabricants se livraient à une course à la plus haute fréquence.
Passé un certain point, de sérieux problèmes de dissipation thermique et consommation énergétique ont forcés les fabricants à changer la manière de faire évoluer les processeurs. Plutôt que de continuer à augmenter la fréquence, ils l'ont stabilisée (voir parfois diminuée !), et ont multiplié le nombre de coeurs par processeurs.


Pour illustrer ce phénomène il suffit de regarder par exemple la gamme de produits proposés par Intel : la première série de Pentium 4 - Willamette - lancée par Intel en 2000 avait un unique coeur cadencé à 1.5GHz. 6 ans plus tard, la dernière génération de Pentium 4 - Cedar Mill - avait également un seul coeur, mais cette fois cadencé à 3.6 GHz.

10 ans plus tard en 2016, les processeurs de la génération Skylake d'Intel i7 ne dépassent pas les 3.4GHz de fréquence, mais tous ont 4 coeurs physiques au lieu d'un seul.

Avec ce changement de design, des problèmes d'accès à la mémoire sont apparus, et le moyen qui a été trouvé pour éviter trop de contention sur le bus mémoire fut de diviser la mémoire en plusieurs bloc physique différents, avec chacun leur contrôleur.
La conséquence directe de ce changement est que le temps d'accès à la mémoire est devenu non uniforme : il dépend directement de quel processeur essaye d'accéder à quelle partie de la mémoire.
On appelle ce genre d'architectures NUMA (pour \emph{Non Uniform Memory Access}) et elles sont aujourd'hui la brique de base pour créer des supercalculateurs.

L'amélioration des performances générales d'un supercalculateur passe donc directement par l'optimisation de l'exploitation des machines NUMA.

Plusieurs techniques de programmation permette de cibler ce genre d'architectures.
On peut notamment citer des techniques statiques à base de boucles parallèles.
En pratique beaucoup d'applications ne sont pas suffisamment régulières pour utiliser efficacement ce type de modèle de programmation.

Les techniques d'ordonnancement dynamiques peuvent garantir une utilisation plus efficace des ressources.
L'une des plus communes est l'ordonnancement par vol de travail : l'application est exprimée comme un graphe de flot de données, chaque sous partie - tâche - consommant et produisant des données.
Un programme dédié - le support exécutif - a la charge d'exécuter ce graphe : à chaque fois qu'un processeur devient inactif, il va récupérer une tâche disponible pour l'exécuter.
Pour que cela soit efficace, il faut pouvoir exprimer un maximum de parallélisme.

Les modèles de programmation standard on su s'adapter : par exemple OpenMP, qui est le standard \emph{de-facto} pour ce genre d'architectures, ne proposait au départ que du parallélisme de boucles. Les dernières versions proposent de la programmation par tâche avec dépendances de données.

En revanche ces modèles de programmation présentent toujours un manque lorsqu'il s'agit d'exploiter efficacement les machines NUMA.
Le programmeur doit donc faire de gros efforts pour effectuer des optimisations spécifiques, par exemple via des librairies externes pour contrôler précisément le placement des données.
Les meilleurs outils standard et non intrusifs permettent simplement de distribuer les pages de la mémoire sur les différentes parties physiques : cela a l'avantage d'améliorer l'uniformité des temps d'accès à la mémoire, mais pas d'améliorer le temps d'accès moyen.
Cela rend donc les processeurs uniformément mauvais vis à vis de l'accès à la mémoire.

Cette thèse est axée sur l'amélioration des standards et techniques pour l'exploitation des machines NUMA, et cela passe par plusieurs étapes : tout d'abord fournir au programmeur les moyens de comprendre et analyser le comportement des parties critiques de son application.
Ensuite lui permettre de fournir plus d'information au support exécutif, principalement en lui permettant d'exprimer une \emph{affinité} entre ses tâches et les ressources de la machine.
En enfin en proposant des techniques d'ordonnancement prenant en compte ces informations, dans le but d'améliorer efficacement les performances globales de l'application.



De quoi il faut parler dans l'intro (dans l'ordre) :


\begin{itemize}
  \item décrire exemples d'applications de simulation
  \begin{itemize}
    \item météo
    \item prévision de séismes
    \item collisions
    \item écoulement de fluide
    \item militaire
  \end{itemize}
  \item les simulations tournent sur des supercalculateurs, et ils en veulent toujours plus
  \begin{itemize}
    \item Plus grosses simulations
    \item Plus précise
    \item Ou tout simplement plus de simulations dans le même temps, donc "on" satisfait plus de monde
  \end{itemize}
  \item les supercalculateurs sont composés de plusieurs machines
  \begin{itemize}
    \item chaque machine est SMP
    \item assemblée ensemble pour obtenir une grosse puissance de calcul
    \item ces machines peuvent avoir des accélérateurs
    \item Exemple de supercalculateur (parler du nombre de coeur pour enchainer sur la suite)
  \end{itemize}
  \item description de l'évolution des processeurs (parallélisation)
  \begin{itemize}
    \item cf partie rédigée de l'intro sur processeur mono coeur -> processeurs multi coeur
  \end{itemize}
  \item au final maintenant toutes les machines SMP sont NUMA
  \begin{itemize}
    \item décrire succinctement le fonctionnement
    \item spécificité importante : temps d'exécution dépend du placement
  \end{itemize}
  \item Le but c'est d'améliorer l'exploitation de ces machines pour améliorer l'ensemble
  \begin{itemize}
    \item améliorer perf supercalculateur = améliorer perf NUMA
    \item améliorer perf NUMA = améliorer l'utilisation des noeuds
    \item => meilleur load balancing
  \end{itemize}
  \item et en fait le data flow c'est cool pour exploiter ce genre d'architectures, parce qu'on veut du dynamique
  \begin{itemize}
    \item statique (via des for, ou des schedules calculés)
    \item tâche avec synchronisations explicite
    \item en pratique les tâches à grain fin permettent de maximiser l'utilisation des ressources (moins de temps à synchroniser ou rien faire)
    \item parler du vol de travail : le concept de toujours essayer d'être actif est bien vendeur, on parlera des problèmes de synchro plus tard
    \item insister sur le fait que pour le le vol soit efficace il faut un maximum de travail (=le grain fin c'est bien, en plus une tâche c'est pas cher !). (mais quand même pas trop, sinon on passe trop de temps à les gérer)
  \end{itemize}
  \item les standards de programmation parallèle ont commencé à évoluer
  \begin{itemize}
    \item début avec des "simples" for
    \item ajout des tâches, puis des dépendances dans openmp
  \end{itemize}
  \item pour l'instant les perfs c'est nul
  \begin{itemize}
    \item problème : standard/modèles de programmation actuels pas assez "NUMA-aware"
    \item implique : gros efforts pour le programmeur pour "corriger" ça, via librairies spécifiques
    \item "meilleurs" outils : distribution des pages, rendant tous le monde "equally bad".
  \end{itemize}
  \item cette thèse est fantastique pour plusieurs raison : on peut mieux comprendre comment les applications elles marchent, du coup le programmeur il peut mieux programmer, et du coup le support exécutif il peut mieux exécuter.
  \begin{itemize}
    \item évident qu'il y a de la marge à gagner
    \item aider le programmeur (et le développeur de runtime en fait) à comprendre comment le NUMA réagit aux différents type d'application
    \item identifier ce que le programmeur peut exprimer en plus pour faciliter de travail du runtime (affinité, tailles des données manipulées, zone d'initialisation)
    \item indiquer les différentes améliorations du runtime possible (prendre en compte la localité si affinité)
  \end{itemize}
\end{itemize}





\input{tex/introduction/objectives}
\input{tex/introduction/outline}
