\begin{savequote}[6cm]
<< What do you work on?  >>
\qauthor{someone}
\end{savequote}
\chapter{Introduction}
\chaptertoc

L'évolution du calcul haute performance est aujourd'hui dirigée par les besoins des applications de simulation.

Ces applications sont omniprésentes dans l'industrie, et concerne parfois même directement le grand public.
Par exemple pour la météo, les prévisions sont faites à l'aide d'applications simulant les intéractions entre les différents éléments de l'atmosphère.
Il en va de même pour tout ce qui se rapporte à la prévision de réaction de différents bâtiments en cas de séismes.

Dans un cadre plus industriel, les secteurs comme l'aéronautique, les applications militaires, ou encore le nucléaire ont un grand besoin en terme de simulations.

Toutes ces simulations sont au final exécutées sur des supercalculateurs, et il n'y a pas de limite aux nombres de ressources qu'elle peuvent utiliser : que ce soit pour améliorer la précision de la simulation, ou augmenter l'ensemble simulé, elles pourront toujours bénéficier d'un plus grand nombres de ressources.

De plus un grand nombre de ressources peut également permettre d'exécuter un plus grand nombre de simulations simultanément.

Si les supercalculateurs peuvent proposer plusieurs milliers de coeurs, ils sont en fait composés d'un grand nombre de petite machine avec un nombre de coeurs beaucoup plus faible.
Ces machines peuvent proposer plusieurs types de ressources, tels que des processeurs (CPU) standard, ou des accélérateurs plus ou moins spécifiques comme des GPUs ou des FPGA.

La très grande majorité des machines "basiques" (TODO : reformuler) possèdent plusieurs processeurs.

Ces derniers ont subit des gros changements d'évolution entre le milieu des années 2000 et maintenant : jusqu'au milieu des années 2000, les fabricants se livraient à une course à la plus haute fréquence.
Passé un certain point, de sérieux problèmes de dissipation thermique et consommation énergétique ont forcés les fabricants à changer la manière de faire évoluer les processeurs. Plutôt que de continuer à augmenter la fréquence, ils l'ont stabilisée (voir parfois diminuée !), et ont multiplié le nombre de coeurs par processeurs.


Pour illustrer ce phénomène il suffit de regarder par exemple la gamme de produits proposés par Intel : la première série de Pentium 4 - Willamette - lancée par Intel en 2000 avait un unique coeur cadencé à 1.5GHz. 6 ans plus tard, la dernière génération de Pentium 4 - Cedar Mill - avait également un seul coeur, mais cette fois cadencé à 3.6 GHz.

10 ans plus tard en 2016, les processeurs de la génération Skylake d'Intel i7 ne dépassent pas les 3.4GHz de fréquence, mais tous ont 4 coeurs physiques au lieu d'un seul.

Avec ce changement de design, des problèmes d'accès à la mémoire sont apparus, et le moyen qui a été trouvé pour éviter trop de contention sur le bus mémoire fut de diviser la mémoire en plusieurs bloc physique différents, avec chacun leur contrôleur.
La conséquence directe de ce changement est que le temps d'accès à la mémoire est devenu non uniforme : il dépend directement de quel processeur essaye d'accéder à quelle partie de la mémoire.
On appelle ce genre d'architectures NUMA (pour \emph{Non Uniform Memory Access}) et elles sont aujourd'hui la brique de base pour créer des supercalculateurs.

L'amélioration des performances générales d'un supercalculateur passe donc directement par l'optimisation de l'exploitation des machines NUMA.

Plusieurs techniques de programmation permette de cibler ce genre d'architectures.
On peut notamment citer des techniques statiques à base de boucles parallèles.
En pratique beaucoup d'applications ne sont pas suffisamment régulières pour utiliser efficacement ce type de modèle de programmation.

Les techniques d'ordonnancement dynamiques peuvent garantir une utilisation plus efficace des ressources.
L'une des plus communes est l'ordonnancement par vol de travail : l'application est exprimée comme un graphe de flot de données, chaque sous partie - tâche - consommant et produisant des données.
Un programme dédié - le support exécutif - a la charge d'exécuter ce graphe : à chaque fois qu'un processeur devient inactif, il va récupérer une tâche disponible pour l'exécuter.
Pour que cela soit efficace, il faut pouvoir exprimer un maximum de parallélisme.

Les modèles de programmation standard on su s'adapter : par exemple OpenMP, qui est le standard \emph{de-facto} pour ce genre d'architectures, ne proposait au départ que du parallélisme de boucles. Les dernières versions proposent de la programmation par tâche avec dépendances de données.

En revanche ces modèles de programmation présentent toujours un manque lorsqu'il s'agit d'exploiter efficacement les machines NUMA.
Le programmeur doit donc faire de gros efforts pour effectuer des optimisations spécifiques, par exemple via des librairies externes pour contrôler précisément le placement des données.
Les meilleurs outils standard et non intrusifs permettent simplement de distribuer les pages de la mémoire sur les différentes parties physiques : cela a l'avantage d'améliorer l'uniformité des temps d'accès à la mémoire, mais pas d'améliorer le temps d'accès moyen.
Cela rend donc les processeurs uniformément mauvais vis à vis de l'accès à la mémoire.

Cette thèse est axée sur l'amélioration des standards et techniques pour l'exploitation des machines NUMA, et cela passe par plusieurs étapes : tout d'abord fournir au programmeur les moyens de comprendre et analyser le comportement des parties critiques de son application.
Ensuite lui permettre de fournir plus d'information au support exécutif, principalement en lui permettant d'exprimer une \emph{affinité} entre ses tâches et les ressources de la machine.
En enfin en proposant des techniques d'ordonnancement prenant en compte ces informations, dans le but d'améliorer efficacement les performances globales de l'application.



De quoi il faut parler dans l'intro (dans l'ordre) :


\begin{itemize}
  \item décrire exemples d'applications de simulation
  \begin{itemize}
    \item météo
    \item prévision de séismes
    \item collisions
    \item écoulement de fluide
    \item militaire
  \end{itemize}
  \item les simulations tournent sur des supercalculateurs, et ils en veulent toujours plus
  \begin{itemize}
    \item Plus grosses simulations
    \item Plus précise
    \item Ou tout simplement plus de simulations dans le même temps, donc "on" satisfait plus de monde
  \end{itemize}
  \item les supercalculateurs sont composés de plusieurs machines
  \begin{itemize}
    \item chaque machine est SMP
    \item assemblée ensemble pour obtenir une grosse puissance de calcul
    \item ces machines peuvent avoir des accélérateurs
    \item Exemple de supercalculateur (parler du nombre de coeur pour enchainer sur la suite)
  \end{itemize}
  \item description de l'évolution des processeurs (parallélisation)
  \begin{itemize}
    \item cf partie rédigée de l'intro sur processeur mono coeur -> processeurs multi coeur
  \end{itemize}
  \item au final maintenant toutes les machines SMP sont NUMA
  \begin{itemize}
    \item décrire succinctement le fonctionnement
    \item spécificité importante : temps d'exécution dépend du placement
  \end{itemize}
  \item Le but c'est d'améliorer l'exploitation de ces machines pour améliorer l'ensemble
  \begin{itemize}
    \item améliorer perf supercalculateur = améliorer perf NUMA
    \item améliorer perf NUMA = améliorer l'utilisation des noeuds
    \item => meilleur load balancing
  \end{itemize}
  \item et en fait le data flow c'est cool pour exploiter ce genre d'architectures, parce qu'on veut du dynamique
  \begin{itemize}
    \item statique (via des for, ou des schedules calculés)
    \item tâche avec synchronisations explicite
    \item en pratique les tâches à grain fin permettent de maximiser l'utilisation des ressources (moins de temps à synchroniser ou rien faire)
    \item parler du vol de travail : le concept de toujours essayer d'être actif est bien vendeur, on parlera des problèmes de synchro plus tard
    \item insister sur le fait que pour le le vol soit efficace il faut un maximum de travail (=le grain fin c'est bien, en plus une tâche c'est pas cher !). (mais quand même pas trop, sinon on passe trop de temps à les gérer)
  \end{itemize}
  \item les standards de programmation parallèle ont commencé à évoluer
  \begin{itemize}
    \item début avec des "simples" for
    \item ajout des tâches, puis des dépendances dans openmp
  \end{itemize}
  \item pour l'instant les perfs c'est nul
  \begin{itemize}
    \item problème : standard/modèles de programmation actuels pas assez "NUMA-aware"
    \item implique : gros efforts pour le programmeur pour "corriger" ça, via librairies spécifiques
    \item "meilleurs" outils : distribution des pages, rendant tous le monde "equally bad".
  \end{itemize}
  \item cette thèse est fantastique pour plusieurs raison : on peut mieux comprendre comment les applications elles marchent, du coup le programmeur il peut mieux programmer, et du coup le support exécutif il peut mieux exécuter.
  \begin{itemize}
    \item évident qu'il y a de la marge à gagner
    \item aider le programmeur (et le développeur de runtime en fait) à comprendre comment le NUMA réagit aux différents type d'application
    \item identifier ce que le programmeur peut exprimer en plus pour faciliter de travail du runtime (affinité, tailles des données manipulées, zone d'initialisation)
    \item indiquer les différentes améliorations du runtime possible (prendre en compte la localité si affinité)
  \end{itemize}
\end{itemize}


Le monde de l'informatique évolue rapidement, qui plus est quand il s'agit de la pièce maitresse de la machine : le processeur !

Cette évolution est principalement motivée par les besoins en calculs toujours plus important de la part des scientifiques et des industriels.

La plupart de ces besoins concernent en fait directement le grand public : lorsqu'il faut prévoir la météo, simuler des séismes, simuler des crashs, ou encore développer du matériel pour l'aéronautique, il y a besoin de supercalculateurs.

Il y a également d'autres types d'applications utilisées à but militaire (pour la simulation balistique par exemple), ainsi que des applications orientées purement recherche, comme celles utilisées en biologie ou dans les recherches ayant un rapport avec la thermo dynamique.

Dans certains cas simuler quelques secondes d'un phénomène peut prendre plusieurs minutes, l'amélioration des ressources des calculs permet donc d'accélérer les simulations, ce qui permet donc dans un même laps de temps de simuler plus de choses ou de simuler avec une plus grande précision.

Jusqu'à il y a environ 10 ans, les fabricants de processeurs se livraient à une course à la plus haute fréquence.
Jusqu'au moment où ce n'était plus rentable au point de vue thermique ou énergétique. À partir de ce moment là, les fabricants ont basculé leur intérêt vers la multiplication des coeurs au sein d'un processeur, et ont stabilisé la fréquence individuelle de chaque coeur.

Pour illustrer ce phénomène il suffit de regarder par exemple la gamme de produits proposés par Intel : la première série de Pentium 4 - Willamette - lancée par Intel en 2000 avait un unique coeur cadencé à 1.5GHz. 6 ans plus tard, la dernière génération de Pentium 4 - Cedar Mill - avait également un seul coeur, mais cette fois cadencé à 3.6 GHz.

10 ans plus tard en 2016, les processeurs de la génération Skylake d'Intel i7 ne dépassent pas les 3.4GHz de fréquence, mais tous ont 4 coeurs physiques au lieu d'un seul.

Pendant ce laps de temps, certains objectifs sont restés les même :
\begin{itemize}
  \item on souhaite toujours exploiter le matériel au plus fort de leur potentiel, que ce soit du point de vue énergétique, du point de vue de la performance, ou du compromis des deux.
  \item on souhaite toujours créer des machines de plus en plus puissantes, afin que les programmes s'exécutent plus vite.
\end{itemize}

En revanche un certains nombres de problématiques sont apparues : l'exploitation efficace de plusieurs processeurs est complètement différentes de l'exploitation d'un seul processeurs.

Lorsqu'on traite avec un seul coeur, on s'attend à ce que le compilateur optimise le code séquentiel en fonction de l'architecture.
Pour exploiter plusieurs coeurs il faut pouvoir exprimer du \emph{parallélisme} au sein de l'application, c'est à dire décrire les parties séquentielles du programme qui veut s'exécuter en même temps, tout en gardant un comportement correct du programme.
Le compilateur ne peut pas spontanément trouver ça, il faut donc une modification explicite de la part du programmeur.

On peut avoir plusieurs coeurs au sein d'un même processeur, plusieurs processeurs au sein d'une machine, mais on peut aussi avoir plusieurs machines inter connectées, qui forme une seule "grosse" machine que l'on appelle \emph{supercalculateur}.

La manière d'exprimer du parallélisme est différente en fonction de quel type de machine on cible :
\begin{itemize}
  \item Lorsque l'on cible une seule machine avec un ou plusieurs processeurs, généralement tout les coeurs des processeurs peuvent accéder de manière transparente à la mémoire : on dit que ce type d'architecture est à \emph{mémoire partagée}.
  \item Lorsque l'on cible un supercalculateur, chaque machine aura une \emph{mémoire partagée}, mais ne verra que sa propre mémoire, et pas celle des autres machines. Cela ne les empêche évidement pas de se transférer des données entre elles. On dit que ce type d'architecture est à \emph{mémoire distribuée}.
\end{itemize}

L'augmentation du nombre de coeurs par processeur et par machine a donné naissance à plusieurs problématiques et thèmes de recherche :

\paragraph{La mémoire}
L'augmentation du nombres de coeurs accédant à la même mémoire partagée augmente la contention sur le bus mémoire pour y accéder.
Pour pouvoir gérer un plus grand nombre de coeurs tout en limitant la contention, la majorité des architectures à mémoire partagée récente divisent physiquement la mémoire en sous parties appelées \emph{noeud}.
Chacun de ces noeuds possède son propre controleur mémoire, ainsi qu'un sous ensemble des coeurs du système.
Ces noeuds sont liés ensemble via un système d'interconnexion, afin que cette division soit transparent pour le système d'exploitation, qui ne voit toujours qu'une unique machine.
Pour un coeur donné, l'accès à la mémoire située sur son propre noeud est plus rapide que sur l'un des autres noeuds. Cela veut donc dire que le temps d'accès à la mémoire n'est pas uniforme en fonction de quelle partie est ciblée.
On qualifie ce type d'architectures de "NUMA", pour "Non-Uniform Memory Access".

\paragraph{L'énergie}

 %Note : ~5000 kWh par an par foyer (soit ~570 watts), les rangs 1 et 2 peuvent consommer ~15kW
Que l'on utilise une petite machine à mémoire partagée de 16 coeurs, ou un gros supercalculateur avec 1 millions de coeurs, faire attention à l'énergie consommée est critique.
L'augmentation continue du nombre de coeurs ne pourra pas se faire sans porter une attention particulière à l'énergie : aujourd'hui il faut l'équivalent de la puissance d'environ 25 familles française pour faire tourner certains des premiers supercalculateur du top500.

\paragraph{Les outils}

Les langages de programmation "classique" tels que C/C++/Fortran sont principalement axés sur l'exécution séquentielle du code.
Les programmeurs ont besoin d'exprimer du parallélisme, donc des extensions aux langages existant ou des nouveaux langages doivent être créés et testés. Si possible de manière portable.

Ces langages ou extensions reposent très souvent sur un programme spécifique pour exécuter une application : le support exécutif.
Le but d'un programme de ce type est d'optimiser l'utilisation des ressources disponibles, tout en conservant la sémantique et la "correctness" (TODO : trouver une traduction) du programme.
Augmenter le nombre de ressources signifient plus de synchronisations, et plus de choix d'ordonnancement à effectuer.

\paragraph{Le matériel dédié}

Une partie de l'optimisation des machines passe par l'amélioration du matériel existant.

Il est commun d'utiliser plusieurs types de matériel pour pouvoir répondre efficacement à des besoins spécifiques, c'est pour cela que les supercalculateurs actuels embarquent généralement un ensemble d'accélérateurs tels que des GPUs et des FPGAs, en plus des CPUs traditionnels.

Les systèmes d'interconnexion sont aussi de bon candidats pour des améliorations et optimisations.
Améliorer la latence et la bande passante entre plusieurs machines ou au sein d'une même machines permet toujours d'améliorer les performances, et les technologies impliquées sont très différentes.



Comme on peut le voir il y a un grand nombre d'opportunités pour faire de la recherche dans le calcul haute performance (HPC).
La section suivante sera dédiée à préciser les différents thèmes de recherche qui ont été abordés au cours de la thèse, préciser le contexte dans laquelle elle s'est déroulée, et préciser les objectifs auxquels elles devaient répondre.



\input{tex/introduction/objectives}
\input{tex/introduction/outline}
