\section{OpenMP, evolution of a programming model}\label{sec:context:openmp}

TODO: où et comment formuler le fait qu'il faut d'une part maximiser la performance séquentielle, mais aussi maximiser le parallélisme

OpenMP~\cite{openmp40} has been the de-facto standard for shared-memory parallel proramming, and has been adapted as the hardware evolved.

\subsection{Loop constructs}

From the very beginning OpenMP has been designed as a fork-join model (TODO: include a graph), and was mainly used to parallelize highly regular loops.

It works the following way: the program is executed sequentially until a \emph{worksharing} construct (such as a for loop) is encountered.

From this point, several threads are created to execute the construct in parallel.

At the end of the construct, all the threads synchronize and the sequential execution is resumed.

The programmer can control how many threads can be used, as well as how the loop iterations are distributed among them.

TODO: parler de static, dynamic, etc ?

During the execution of the program, a dedicated software is in charge of load balancing, and assigning work to threads: the \emph{runtime}.

If the OpenMP language is unique, several runtimes exist, and the performances of an application directly depend on which one is used.


To provide more control to the user, several functions are provided by the OpenMP API: for instance getting the total number of threads, the current thread, ...

All existed in the earliest version of OpenMP.

With the hardware and users' needs evolving, the OpenMP committee added the concept of \emph{task} to OpenMP, starting from version 3.0.


\subsection{Tasking}

Task-based parallel programming environments provide ways of expressing fine grain parallelism that can be dynamically assigned to processors at runtime.

An OpenMP \emph{task} can be seen as an independent \emph{unit of work} an OpenMP thread can execute.
Tasks can be created by an OpenMP thread and executed by any thread of the same parallel region.
As managing tasks at runtime is way cheaper than creating and synchronizing threads, the application programmer can take the parallelization of its application further, as he can now consider portions of code that were too fine grain to be parallelized using only threads.

The synchronization of OpenMP 3.0 tasks is performed thanks to the |taskwait| keyword that waits for the completion of all the tasks generated from the current OpenMP parallel region.

On one hand, the application programmer is responsible for creating and synchronizing OpenMP tasks explicitly.

On the other hand, the runtime system is in charge of correctly assigning tasks to threads during the application execution.

OpenMP 4.0 has pushed the concept of task further introducing the |depend| keyword to specify the access mode of each shared variable a task will access during its execution.

Access modes can be set to either |in|, |out| or |inout| whether the corresponding variable is respectively read as input, written as output or both read and written by the considered task.

This information is then processed by the underlying runtime system to decide whether a task is ready for execution or should first wait for the completion of other ones.


\subsection{Vectorization}

OpenMP 4.0 also brought in SIMD - Single Instruction Multiple Data - constructs.
The aim was to provide a standard interface for vectorization instructions.


\subsection{Accelerators}

Given more and more supercomputers feature a lot of accelerators (such as GPUs), OpenMP introduced support for accelerators through the |target| construct in OpenMP 4.0.

Other standards for programming GPUs exists, such as OpenCL, OpenACC, or cuda (TODO: refs).

\subsection{Affinity}

Talk about thread affinity, open on data affinity?

