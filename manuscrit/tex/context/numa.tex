\section{Architectures à mémoire partagée}\label{sec:context:numa}

Les architectures à mémoire partagée on vécue des changements majeures avec l'évolution des processeurs.
L'augmentation du nombre de coeurs par processeur a introduit des problèmes d'accès à la mémoire centrale : en effet comme plusieurs coeurs accèdent à la mémoire en même temps et que la bande passante n'est pas infinie, des problèmes de contention sur le bus mémoire apparaissent.

Pour palier à ce problème, les constructeurs ont divisé la mémoire centrale en plusieurs parties physiquement distinctes, appelées \emph{noeuds}.
Chaque noeud est constitué d'une partie de la mémoire centrale, d'un contrôleur local d'accès à ce bloc mémoire, ainsi que d'un certains nombre de coeurs de calcul.
L'ensemble des noeuds de la machine sont ensuite reliés entre eux par un système d'interconnexion.
La topologie de l'interconnexion ne permet généralement pas d'avoir des noeuds équidistant, ce qui introduit une hiérarchie mémoire.

En revanche cette complexité est invisible pour le système d'exploitation, qui ne voit qu'un seul système constitué par l'ensemble des coeurs et de la mémoire des différents noeuds.
Par exemple dans le cas simple d'une machine constituée de deux noeuds de 8 coeurs et 32 Go de RAM, le système verra de manière transparente une machine à 16 coeurs et 64 Go de RAM.

Nous décrivons dans un premier temps les caractéristiques techniques communes d'un noeud dans la section~\ref{sec:context:numa:node}, puis celles des systèmes d'interconnexion des noeuds dans la section~\ref{sec:context:numa:interconnect}.
Enfin la section~\ref{sec:context:numa:os} revient sur quelques particularités de traitement au sein du système d'exploitation.

\subsection{Descriptions d'un noeud}\label{sec:context:numa:node}

Il n'y a pas de nombre standard de coeurs pour un noeud, ni même de quantité standard de RAM associée.

En revanche certaines propriétés se retrouvent sur toutes les machines NUMA.

\subsubsection{Les caches}
Le cache est une mémoire pour laquelle le temps d'accès est bien meilleur que pour la mémoire centrale, mais dont la capacité est beaucoup plus restreinte.

\paragraph{Fonctionnement}

TODO : expliquer comment ça se passe ? requète sur un octet -> série de cache miss, puis chargement en cache.
écriture => dirty => eviction.
Parler des politiques d'éviction ?
Parler de l'associativité ?

Au niveau des accès le fonctionnement est légèrement différent : plutôt que de lire uniquement un octet, c'est généralement une partie fixe de la mémoire contenant cet octet qui est chargée.
On appelle cette quantité une \emph{ligne de cache}, et un exemple de taille standard pour une ligne de cache est 64 octets (8 double).

\paragraph{Niveaux de caches}

Il y a en général 3 niveaux de cache dans ces architectures :
\begin{itemize}
  \item L1 : C'est le cache le plus "proche" du CPU, mais aussi le plus petit - généralement quelques Ko. Il est généralement découpé en deux parties distinctes : une partie pour les instructions, une autre pour les données.
  \item L2 : Ce cache est un peu plus long à accéder que le L1, mais est un peu plus grand. Pour donner une taille représentative des architectures modernes on peut donner 256 Ko.
  \item L3 : Ce cache est un peu plus long à accèdes que le L2, mais est beaucoup plus grand. En général il fait plusieurs Mo.
\end{itemize}

En fonction des fabricants, ces caches peuvent être soit \emph{inclusifs} (c'est à dire que toutes les données présentes dans le L1 sont également présentes dans le L2), ou \emph{exclusifs} (c'est à dire qu'une donnée est garantie de n'être présente que dans l'un des caches).

Au sein d'un noeud, le L3 est généralement partagé par tous les coeurs, alors que les caches L1 et L2 sont spécifiques à un coeur.

En général les développeurs d'applications pour le HPC accordent beaucoup d'attention à l'optimisation de leur application pour que les parties de code séquentiel critique utilisent des données qui tiennent dans le L1/L2.
De même, beaucoup d'optimisations au sein des compilateurs visent également cet objectif.

Lorsqu'il s'agit de cibler des architectures NUMA, on va tout particulièrement s'intéresser au L3, qui représente la mémoire la plus rapide accessible par tous les coeurs d'un même noeud.

GRAPHE : 2.1.1 schéma de l'architecture des chaches

\subsubsection{Hyperthreading}

Chaque coeur possède un certain nombre d'UAL (Unité Arithmétique et Logique) qui lui sont privées, lorsqu'il est en attente d'une donnée de la mémoire centrale, ces UALs ne sont pas utilisées, et des cycles CPU sont donc "perdus" à ne rien faire.

Afin de maximiser l'utilisation de ces ressources, certains processeurs Intel sont équipés de la technologie \emph{hyperthreading}.
Le concept est assez simple : avoir deux coeurs logiques (\emph{hyperthreads}) associés à un seul coeur physique.
De cette manière lorsqu'un thread est en attente sur une donnée, le second peut profiter des UALs disponibles.

Pour des tâches peu gourmandes en ressources ou utilisant beaucoup de données, cela peut effectivement se traduire par un gain de performances, mais pour le cas des applications HPC il faut regarder le type d'application utilisées pour savoir si on peut espérer un gain ou non.

En particulier les caches L1 et L2 sont partagés par les deux hyperthreads, donc si le code séquentiel généré est optimisé pour les tailles de caches correspondant, exécuter le même type de code séquentiel sur deux hyperthreads va entrainer du \emph{cache trashing} et des baisses de performance.

De même si l'application est compute-intensive, l'hyperthreading n'apportera pas grand chose, voir rien.

L'hyperthreading est généralement une option que l'on peut désactiver dans le BIOS de la machine.


\subsection{Interconnexion des noeuds}\label{sec:context:numa:interconnect}

La partie la plus importante d'une machine NUMA est le système d'interconnexion entre les différents noeuds.

C'est cette partie qui va déterminer à quel point cela va être couteux de faire un accès mémoire sur un noeud distant, et donc à quel point le côté NUMA de la machine (le \emph{facteur NUMA} (là si je cite pas la thèse de François je me fais tapé dessus ?)) va impacter les performances d'une application.

Dans la majorité des cas, ce système d'interconnexion est \emph{cache-coherent}, c'est à dire que la cohérence de cache est assurée entre les différents noeuds par le matériel, est n'est pas la responsabilité du programmeur.

TODO : parler de la hiérarchie possible de l'interconnexion.

TODO : donner quelques chiffres à titre d'exemple ?

GRAPHE : 2.1.2 montrer une version schématique d'une machine NUMA


\subsection{Particularité au niveau du système d'exploitation}\label{sec:context:numa:os}


Parler de la taille de page (4K vs 2M), et de l'impact que ça a.

Parler de~\cite{Dobson2003}

Parler d'hwloc ?

Parler de comment le système gère l'hyperthreading ? (mal, de toute façon on pin les threads)


