\section{Architectures à mémoire partagée}\label{sec:context:numa}

Les architectures à mémoire partagée on vécue des changements majeures avec l'évolution des processeurs.
L'augmentation du nombre de coeurs par processeur a introduit des problèmes d'accès à la mémoire centrale : en effet comme plusieurs coeurs accèdent à la mémoire en même temps et que la bande passante n'est pas infinie, des problèmes de contention sur le bus mémoire apparaissent.

Pour palier à ce problème, les constructeurs ont divisé la mémoire centrale en plusieurs parties physiquement distinctes, appelées \emph{noeuds}.
Chaque noeud est constitué d'une partie de la mémoire centrale, d'un contrôleur local d'accès à ce bloc mémoire, ainsi que d'un certains nombre de coeurs de calcul.
L'ensemble des noeuds de la machine sont ensuite reliés entre eux par un système d'interconnexion.
La topologie de l'interconnexion ne permet généralement pas d'avoir des noeuds équidistant, ce qui introduit une hiérarchie mémoire.

En revanche cette complexité est invisible pour le système d'exploitation, qui ne voit qu'un seul système constitué par l'ensemble des coeurs et de la mémoire des différents noeuds.
Par exemple dans le cas simple d'une machine constituée de deux noeuds de 8 coeurs et 32 Go de RAM, le système verra de manière transparente une machine à 16 coeurs et 64 Go de RAM.

Nous décrivons dans un premier temps les caractéristiques techniques communes d'un noeud dans la section~\ref{sec:context:numa:node}, puis celles des systèmes d'interconnexion des noeuds dans la section~\ref{sec:context:numa:interconnect}.
Enfin la section~\ref{sec:context:numa:os} revient sur quelques particularités de traitement au sein du système d'exploitation.

\subsection{Descriptions d'un noeud}\label{sec:context:numa:node}

Il n'y a pas de nombre standard de coeurs pour un noeud, ni même de quantité standard de RAM associée.

En revanche certaines propriétés se retrouvent sur toutes les machines NUMA.

\subsubsection{Les caches}
Le cache est une mémoire pour laquelle le temps d'accès est bien meilleur que pour la mémoire centrale, mais dont la capacité est beaucoup plus restreinte.

\paragraph{Fonctionnement}

Au niveau des accès le fonctionnement d'un cache est légèrement différent de celui de la mémoire centrale : plutôt que de lire uniquement un octet, c'est généralement une partie fixe de la mémoire contenant cet octet qui est chargée.
On appelle cette quantité une \emph{ligne de cache}, et un exemple de taille standard pour une ligne de cache est 64 octets (8 double).

Lorsqu'une instruction demande le chargement d'une valeur située en mémoire, le contrôleur du cache reçoit la requête du processeur, détermine la ligne de cache correspondante, et effectue l'une des deux opérations suivantes :
\begin{itemize}
    \item Si la ligne correspondante est déjà présente dans le cache - \emph{cache hit} - la donnée est directement retournée.
    \item Si la ligne n'est pas présente - \emph{cache miss} - il va charger la valeur depuis la mémoire centrale dans le cache, et retourner la valeur au processeur.
\end{itemize}

Si le cache a plusieurs niveaux, la requête est transférée au niveau supérieur, et seul le dernier niveau effectue la requête à la mémoire centrale. TODO : ça doit être vrai que pour les caches inclusifs.

Une fois qu'une ligne est chargée dans le cache, elle n'y reste pas de manière permanente, plusieurs raisons décrit ci après peuvent entraîner son \emph{eviction} du cache :

\begin{itemize}
  \item Le maintient de la cohérence. Lorsqu'un processeur modifie une ligne de cache dans son propre cache, cette même ligne est \emph{invalidée} dans les caches des autres processeurs, si elle y est présente.
Si un processeur essaie de faire un accès sur une ligne invalidée, elle sera rechargée depuis la mémoire centrale, générant un \emph{cache miss}.
  \item Le dépassement de la capacité du cache. Il est assez commun que l'ensemble des données manipulées par le programme ne tienne pas dans le cache.
Lorsqu'une requête est effectuée sur une ligne, qu'elle n'est pas présente dans le cache, et que le cache est rempli, le contrôleur choisira une ligne à évicter du cache pour faire de la place à la nouvelle ligne.
Le choix de la ligne à évicter est un sujet très étudié, et le prochain paragraphe revient sur les politiques d'éviction communément utilisées.
  \item Le conflit d'adresse. En fonction de l'\emph{associativité} du cache, une ligne de cache peut avoir un emplacement précis dans le cache, qu'elle peut partager avec une ou plusieurs autres ligne de cache.
Ces lignes ne pouvant pas être présentes en même temps dans le cache, elles s'évicteront donc mutuellement si elles sont requises en même temps.
\end{itemize}



\paragraph{Associativité}

Le cache ne pouvant être suffisamment grand pour contenir toute la mémoire, il faut pouvoir déterminer l'association entre l'adresse d'une ligne dans la mémoire centrale et son emplacement dans le cache.

Il y a trois grandes catégories d'associativité utilisées par les caches :
\begin{itemize}
  \item Les caches à association directe (\emph{direct-mapped cache})
    Dans ce système, chaque ligne de la mémoire est associée à exactement un emplacement dans le cache.
    Ce système est simple, propose le meilleur temps de réponse, mais est peu prévisible et n'est pas optimal en terme d'efficacité par rapport à la taille.
  \item Les caches complètement associatifs (\emph{fully associative cache})
    Dans ce système, chaque ligne de la mémoire peut être associée à n'importe quel emplacement dans le cache.
    Cela permet de maximiser l'utilisation de l'espace disponible, mais cela induit un coût supplémentaire pour déterminer l'association entre une ligne et son emplacement.
  \item Les caches N-associatifs (\emph{N-way associative cache})
    Ce système est un compromis entre les deux précédents.
    Chaque ligne de la mémoire peut être associée à exactement N emplacements dans le cache.
    Cela implique un certain nombre d'opérations sur les adresses des lignes, et l'espace pris par le composant ainsi que le temps d'accès moyen dépend directement de la valeur de N.
\end{itemize}

\paragraph{Politiques d'éviction}

Le choix de la ligne de cache à remplacer lorsque le cache est plein a un impact direct sur les performances, et les politique de remplacement ont été très étudiées par les constructeurs et les chercheurs.

La politique optimale serait de remplacer la ligne qui sera réutilisée le plus tard, mais ce genre de clairvoyance est impossible à implémenter en pratique.
Un certain nombre d'alternatives ont donc été imaginées, et sont décrites ci-après :
\begin{itemize}
  \item Random : cette politique choisi une ligne à remplacer au hasard. Sa simplicité la rend simple à implémenter en pratique, et a notamment été utilisée dans les processeurs ARM Cortex-R~\cite{ARM-Cortex-R}.
  \item First-In First-Out : comme son nom l'indique, la ligne remplacée est la première qui est rentrée dans le cache parmi celles présentes.
  \item Least Recently Used (LRU) : dans cette politique, chaque ligne dispose de plusieurs bits représentant la dernière utilisation de cette ligne. La ligne remplacée est celle ayant été utilisée il y a le plus longtemps.
  \item Pseudo-LRU : cette politique offre un compromis à LRU.
Lorsque l'associativité du cache dépasse un certain seuil, le coût d'implémentation de LRU devient trop important~\cite{Kedzierski2010}, l'alternative proposée par les politiques pseudo-LRU est donc de remplacer une ligne parmi celles utilisées il y a le plus longtemps.
Cela permet de limiter le nombre de bits nécessaire pour tracer les lignes à remplacer potentielles.
\end{itemize}

La politique pseudo-LRU semble être la plus utilisée par les fabricants de processeurs, Al-Zoubi et al.~\cite{Al-Zoubi2004} proposent une évaluation complète et détaillée de l'impact des politiques d'éviction en fonction de l'associativité du cache, justifiant le choix des constructeurs.

\paragraph{Niveaux de caches}

Il y a en général 3 niveaux de cache dans ces architectures :
\begin{itemize}
  \item L1 : C'est le cache le plus "proche" du CPU, mais aussi le plus petit - généralement quelques Ko. Il est généralement découpé en deux parties distinctes : une partie pour les instructions, une autre pour les données.
  \item L2 : Ce cache est un peu plus long à accéder que le L1, mais est un peu plus grand. Pour donner une taille représentative des architectures modernes on peut donner 256 Ko.
  \item L3 : Ce cache est un peu plus long à accèdes que le L2, mais est beaucoup plus grand. En général il fait plusieurs Mo.
\end{itemize}

En fonction des fabricants, ces caches peuvent être soit \emph{inclusifs} (c'est à dire que toutes les données présentes dans le L1 sont également présentes dans le L2), ou \emph{exclusifs} (c'est à dire qu'une donnée est garantie de n'être présente que dans l'un des caches).

Au sein d'un noeud, le L3 est généralement partagé par tous les coeurs, alors que les caches L1 et L2 sont spécifiques à un coeur.

En général les développeurs d'applications pour le HPC accordent beaucoup d'attention à l'optimisation de leur application pour que les parties de code séquentiel critique utilisent des données qui tiennent dans le L1/L2.
De même, beaucoup d'optimisations au sein des compilateurs visent également cet objectif.

Lorsqu'il s'agit de cibler des architectures NUMA, on va tout particulièrement s'intéresser au L3, qui représente la mémoire la plus rapide accessible par tous les coeurs d'un même noeud.

GRAPHE : 2.1.1 schéma de l'architecture des caches

TODO : tableau latences accès caches vs processeurs (sandy, broadwell, knl, bulldozer, ryzen).

TODO : ref + paragraph sur les caches configurables du ryzen

\subsubsection{Instructions vectorielles}\label{sec:context:numa:simd}

Le concept de vectorisation est l'action de grouper ensemble, dans une même instruction, plusieurs éléments nécessitant la même opération.
Ce type d'instructions est appelée SIMD, pour \emph{Single Instruction Multiple Data}.

Afin d'illustrer ce point, il suffit d'imaginer que l'on souhaite additionner deux vecteurs d'entiers :
\begin{lstlisting}
int vecteur_a[8] = { 1 };
int vecteur_b[8] = { 1 };
int addition[8];
for (int i = 0; i < 8; i++) {
  addition[i] = vecteur_a[i] + vecteur_b[i];
}
\end{lstlisting}

Exécuter la boucle en entier effectuerait 8 additions sur des entiers.
En supposant que la largeur des registres (et des instructions) du processeur est de 64 bits, et que la taille d'un |int| est 32 bits, on gaspillerait en fait 32 bits par additions.

Étant donné que l'on souhaite effectuer la même opération (une addition), sur des éléments indépendant du tableau, on peut alors aisément grouper deux additions successives dans la même instructions : il suffit de mettre un entier de 32 bits sur la partie haute du registre, et un deuxième entier sur la partie basse, d'effectuer l'instruction, et de faire l'opération inverse pour stocker le résultat dans le tableau.
On divise alors le nombre d'itérations par 2.

Des extensions au jeu d'instruction x86 ont été créée afin de pouvoir opérer sur des éléments plus larges que 64 bits, et la plupart des architectures des processeurs récents utilisent des registres plus large que le type le plus large en C (|long long|, 64 bits).
La première d'entre elle, SSE (\emph{Streaming SIMD Extensions}), a été introduit par Intel dès 1999, et a évolué régulièrement jusqu'à l'extention AVX-512, permettant d'effectuer des instructions sur des registres de 512 bits (soit 64 double).

La plupart des processeurs actuels (depuis les Sandy Bridge d'Intel, et les Bulldozer d'AMD, en 2011) supportent au moins l'extension AVX avec des registres de 128 bits.

\subsubsection{Hyperthreading}

Chaque coeur possède un certain nombre d'UAL (Unité Arithmétique et Logique) qui lui sont privées, lorsqu'il est en attente d'une donnée de la mémoire centrale, ces UALs ne sont pas utilisées, et des cycles CPU sont donc "perdus" à ne rien faire.

Afin de maximiser l'utilisation de ces ressources, certains processeurs Intel sont équipés de la technologie \emph{hyperthreading}.
Le concept est assez simple : avoir deux coeurs logiques (\emph{hyperthreads}) associés à un seul coeur physique.
De cette manière lorsqu'un thread est en attente sur une donnée, le second peut profiter des UALs disponibles.

Pour des tâches peu gourmandes en ressources ou utilisant beaucoup de données, cela peut effectivement se traduire par un gain de performances, mais pour le cas des applications HPC il faut regarder le type d'application utilisées pour savoir si on peut espérer un gain ou non.
En particulier les caches L1 et L2 sont partagés par les deux hyperthreads, donc si le code séquentiel généré est optimisé pour les tailles de caches correspondant, exécuter le même type de code séquentiel sur deux hyperthreads va entrainer du \emph{cache trashing} et des baisses de performance.
De même si l'application est compute-intensive, l'hyperthreading n'apportera pas grand chose, voir rien.

L'hyperthreading est généralement une option que l'on peut désactiver dans le BIOS de la machine.


\subsection{Interconnexion des noeuds}\label{sec:context:numa:interconnect}

La partie la plus importante d'une machine NUMA est le système d'interconnexion entre les différents noeuds.
C'est cette partie qui va déterminer à quel point cela va être couteux de faire un accès mémoire sur un noeud distant, et donc à quel point l'aspect NUMA de la machine (le \emph{facteur NUMA}) va impacter les performances d'une application.

Dans la majorité des cas, ce système d'interconnexion est \emph{cache-coherent}, c'est à dire que la cohérence de cache est assurée entre les différents noeuds par le matériel, est n'est pas la responsabilité du programmeur ou du support exécutif.
La topologie du système d'interconnexion peut être très différentes d'une machine à une autre, et de multiples exemples existent dans les machines commercialisées.
Il existe des topologies plates, où chaque noeud est directement connecté aux autres (TODO: ref graph 2.1.2), des topologies où les noeuds du même socket passent par le même contrôleur local, mais passent par un ou deux autres liens du système d'interconnexion (TODO: ref graph idchire).

Le nombre de rebonds - \emph{hops} - à effectuer avant d'accéder à la mémoire demandée impacte directement la latence et la bande passante, comme le montre le tableau comparatif de la figure~(TODO).

GRAPHE : 2.1.2 les chiffres de base !
GRAPHE : 2.1.2 montrer une version schématique d'une machine NUMA (2 socket, 1 socket/2 noeuds, idchire, knl)


\subsection{Particularité au niveau du système d'exploitation}\label{sec:context:numa:os}

Le support des machines NUMA dans Linux est arrivé dès 2003~\cite{Dobson2003}.
Il est donc possible, d'une part d'obtenir des informations à propos de la machine, telles que le numéro de noeud d'un coeur donné, ou le numéro de noeud d'une adresse mémoire donnée.
Et d'autre part de contrôler le placement d'un thread sur un ensemble de coeur physique à l'aide d'un masque d'affinité, ce qui permet par exemple de garantir la proximité physique de deux threads vis à vis d'un noeud.

D'autres outils existent pour avoir encore plus d'informations et de contrôle, et particulièrement hwloc~\cite{Broquedis2010}.
Il permet d'avoir des informations précises sur la topologie de l'architecture, incluant les distances théoriques (fournies par le matériel) entre noeud, mais également de garantir des allocations de mémoire sur les noeuds fournis par l'utilisateur.

La compréhension de la gestion de l'allocation de la mémoire par le système d'exploitation est d'ailleurs un point critique lorsque l'on traite des machines NUMA.
Le système d'exploitation gère la mémoire par petit fragments, appelés \emph{pages}.
Lors de l'allocation d'un tableau par exemple, si sa taille dépasse celle d'une \emph{page}, l'espace mémoire alloué consistera donc de plusieurs pages.

Il faut savoir que, par défaut, le système ne donne pas une \emph{page} physique directement lors d'une simple allocation mémoire (comme par exemple lors d'un appel à |malloc|).
La mémoire n'est physiquement allouée que lors du premier accès à la donnée (\emph{first-touch}).
Certains outils, tels que \emph{numactl}~\cite{numactl} permettent de modifier la politique d'allocation physique des pages (par exemple en faisant en sorte de répartir les pages successives sur l'ensemble des noeud NUMA automatiquement).

L'allocateur par défaut de la libc fait de la réutilisation de pages. Cela veut dire qu'après plusieurs allocations/libération successives de mémoire, vous n'obtenez pas forcément des nouvelles pages.
L'objectif derrière cela est de réduire le coût général d'une allocation, puisque l'allocation et la libération physique de nouvelles pages est couteux, et que réutiliser des pages supprime ce coût.
En revanche le corollaire de ce constat est que le contrôle de l'allocation physique par \emph{first-touch} n'est possible que lors de la première allocation des pages.
Pour palier à ce problème, hwloc proposent des fonctions d'allocations qui garantissent l'attribution de nouvelles pages, permettant dans ce cas de faire confiance au \emph{first-touch}.

Enfin, un dernier détail mérite de l'attention, il s'agit de la taille des pages manipulées par le système.
En effet celle ci est configurable, et deux options sont généralement possibles : soit des pages de 4 Kilo octets, soit des - \emph{huge} - pages de 2 Mega octets.
L'impact de cette taille peut être importante dans le cas d'un bloc de données s'étendant sur plusieurs pages : plusieurs requêtes de page de petites tailles seront plus sensible à la \emph{latence} de la mémoire, alors qu'une requête d'une page de grande taille équivalente sera plus sensible à la \emph{bande passante}.
De plus lors de l'utilisation d'un outil tel que \emph{numactl} pour répartir les pages sur la machine, l'effet de la répartition sera plus visible avec des pages de petites tailles.
