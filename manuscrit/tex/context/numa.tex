\section{Architectures à mémoire partagée}\label{sec:context:numa}

Les architectures à mémoire partagée on vécue des changements majeures avec l'évolution des processeurs.
L'augmentation du nombre de coeurs par processeur a introduit des problèmes d'accès à la mémoire centrale : en effet comme plusieurs coeurs accèdent à la mémoire en même temps et que la bande passante n'est pas infinie, des problèmes de contention sur le bus mémoire apparaissent.

Pour palier à ce problème, les constructeurs ont divisé la mémoire centrale en plusieurs parties physiquement distinctes, appelées \emph{noeuds}.
Chaque noeud est constitué d'une partie de la mémoire centrale, d'un contrôleur local d'accès à ce bloc mémoire, ainsi que d'un certains nombre de coeurs de calcul.
L'ensemble des noeuds de la machine sont ensuite reliés entre eux par un système d'interconnexion.
La topologie de l'interconnexion ne permet généralement pas d'avoir des noeuds équidistant, ce qui introduit une hiérarchie mémoire.

En revanche cette complexité est invisible pour le système d'exploitation, qui ne voit qu'un seul système constitué par l'ensemble des coeurs et de la mémoire des différents noeuds.
Par exemple dans le cas simple d'une machine constituée de deux noeuds de 8 coeurs et 32 Go de RAM, le système verra de manière transparente une machine à 16 coeurs et 64 Go de RAM.

Nous décrivons dans un premier temps les caractéristiques techniques communes d'un noeud dans la section~\ref{sec:context:numa:node}, puis celles des systèmes d'interconnexion des noeuds dans la section~\ref{sec:context:numa:interconnect}.
Enfin la section~\ref{sec:context:numa:os} revient sur quelques particularités de traitement au sein du système d'exploitation.

\subsection{Descriptions d'un noeud}\label{sec:context:numa:node}

Il n'y a pas de nombre standard de coeurs pour un noeud, ni même de quantité standard de RAM associée.

En revanche certaines propriétés se retrouvent sur toutes les machines NUMA.

\subsubsection{Les caches}
Le cache est une mémoire pour laquelle le temps d'accès est bien meilleur que pour la mémoire centrale, mais dont la capacité est beaucoup plus restreinte.

\paragraph{Fonctionnement}

Au niveau des accès le fonctionnement d'un cache est légèrement différent de celui de la mémoire centrale : plutôt que de lire uniquement un octet, c'est généralement une partie fixe de la mémoire contenant cet octet qui est chargée.
On appelle cette quantité une \emph{ligne de cache}, et un exemple de taille standard pour une ligne de cache est 64 octets (8 double).

Lorsqu'une instruction demande le chargement d'une valeur située en mémoire, le contrôleur du cache reçoit la requête du processeur, détermine la ligne de cache correspondante, et effectue l'une des deux opérations suivantes :
\begin{itemize}
    \item Si la ligne correspondante est déjà présente dans le cache - \emph{cache hit} - la donnée est directement retournée.
    \item Si la ligne n'est pas présente - \emph{cache miss} - il va charger la valeur depuis la mémoire centrale dans le cache, et retourner la valeur au processeur.
\end{itemize}

Si le cache a plusieurs niveaux, la requête est transférée au niveau supérieur, et seul le dernier niveau effectue la requête à la mémoire centrale. TODO : ça doit être vrai que pour les caches inclusifs.

Une fois qu'une ligne est chargée dans le cache, elle n'y reste pas de manière permanente, plusieurs raisons décrit ci après peuvent entraîner son \emph{eviction} du cache :

\begin{itemize}
  \item Le maintient de la cohérence. Lorsqu'un processeur modifie une ligne de cache dans son propre cache, cette même ligne est \emph{invalidée} dans les caches des autres processeurs, si elle y est présente.
Si un processeur essaie de faire un accès sur une ligne invalidée, elle sera rechargée depuis la mémoire centrale, générant un \emph{cache miss}.
  \item Le dépassement de la capacité du cache. Il est assez commun que l'ensemble des données manipulées par le programme ne tienne pas dans le cache.
Lorsqu'une requête est effectuée sur une ligne, qu'elle n'est pas présente dans le cache, et que le cache est rempli, le contrôleur choisira une ligne à évicter du cache pour faire de la place à la nouvelle ligne.
Le choix de la ligne à évicter est un sujet très étudié, et le prochain paragraphe revient sur les politiques d'éviction communément utilisées.
  \item Le conflit d'adresse. En fonction de l'\emph{associativité} du cache, une ligne de cache peut avoir un emplacement précis dans le cache, qu'elle peut partager avec une ou plusieurs autres ligne de cache.
Ces lignes ne pouvant pas être présentes en même temps dans le cache, elles s'évicteront donc mutuellement si elles sont requises en même temps.
\end{itemize}



\paragraph{Associativité}

Le cache ne pouvant être suffisamment grand pour contenir toute la mémoire, il faut pouvoir déterminer l'association entre l'adresse d'une ligne dans la mémoire centrale et son emplacement dans le cache.

Il y a trois grandes catégories d'associativité utilisées par les caches :
\begin{itemize}
  \item Les caches à association directe (\emph{direct-mapped cache})
    Dans ce système, chaque ligne de la mémoire est associée à exactement un emplacement dans le cache.
    Ce système est simple, propose le meilleur temps de réponse, mais est peu prévisible et n'est pas optimal en terme d'efficacité par rapport à la taille.
  \item Les caches complètement associatifs (\emph{fully associative cache})
    Dans ce système, chaque ligne de la mémoire peut être associée à n'importe quel emplacement dans le cache.
    Cela permet de maximiser l'utilisation de l'espace disponible, mais cela induit un coût supplémentaire pour déterminer l'association entre une ligne et son emplacement.
  \item Les caches N-associatifs (\emph{N-way associative cache})
    Ce système est un compromis entre les deux précédents.
    Chaque ligne de la mémoire peut être associée à exactement N emplacements dans le cache.
    Cela implique un certain nombre d'opérations sur les adresses des lignes, et l'espace pris par le composant ainsi que le temps d'accès moyen dépend directement de la valeur de N.
\end{itemize}

\paragraph{Politiques d'éviction}

Le choix de la ligne de cache à remplacer lorsque le cache est plein a un impact direct sur les performances, et les politique de remplacement ont été très étudiées par les constructeurs et les chercheurs.

La politique optimale serait de remplacer la ligne qui sera réutilisée le plus tard, mais ce genre de clairvoyance est impossible à implémenter en pratique.
Un certain nombre d'alternatives ont donc été imaginées, et sont décrites ci-après :
\begin{itemize}
  \item Random : cette politique choisi une ligne à remplacer au hasard. Sa simplicité la rend simple à implémenter en pratique, et a notamment été utilisée dans les processeurs ARM Cortex-R~\cite{ARM-Cortex-R}.
  \item First-In First-Out : comme son nom l'indique, la ligne remplacée est la première qui est rentrée dans le cache parmi celles présentes.
  \item Least Recently Used (LRU) : dans cette politique, chaque ligne dispose de plusieurs bits représentant la dernière utilisation de cette ligne. La ligne remplacée est celle ayant été utilisée il y a le plus longtemps.
  \item Pseudo-LRU : cette politique offre un compromis à LRU.
Lorsque l'associativité du cache dépasse un certain seuil, le coût d'implémentation de LRU devient trop important~\cite{Kedzierski2010}, l'alternative proposée par les politiques pseudo-LRU est donc de remplacer une ligne parmi celles utilisées il y a le plus longtemps.
Cela permet de limiter le nombre de bits nécessaire pour tracer les lignes à remplacer potentielles.
\end{itemize}

La politique pseudo-LRU semble être la plus utilisée par les fabricants de processeurs, Al-Zoubi et al.~\cite{Al-Zoubi2004} proposent une évaluation complète et détaillée de l'impact des politiques d'éviction en fonction de l'associativité du cache, justifiant le choix des constructeurs.

\paragraph{Niveaux de caches}

Il y a en général 3 niveaux de cache dans ces architectures :
\begin{itemize}
  \item L1 : C'est le cache le plus "proche" du CPU, mais aussi le plus petit - généralement quelques Ko. Il est généralement découpé en deux parties distinctes : une partie pour les instructions, une autre pour les données.
  \item L2 : Ce cache est un peu plus long à accéder que le L1, mais est un peu plus grand. Pour donner une taille représentative des architectures modernes on peut donner 256 Ko.
  \item L3 : Ce cache est un peu plus long à accèdes que le L2, mais est beaucoup plus grand. En général il fait plusieurs Mo.
\end{itemize}

En fonction des fabricants, ces caches peuvent être soit \emph{inclusifs} (c'est à dire que toutes les données présentes dans le L1 sont également présentes dans le L2), ou \emph{exclusifs} (c'est à dire qu'une donnée est garantie de n'être présente que dans l'un des caches).

Au sein d'un noeud, le L3 est généralement partagé par tous les coeurs, alors que les caches L1 et L2 sont spécifiques à un coeur.

En général les développeurs d'applications pour le HPC accordent beaucoup d'attention à l'optimisation de leur application pour que les parties de code séquentiel critique utilisent des données qui tiennent dans le L1/L2.
De même, beaucoup d'optimisations au sein des compilateurs visent également cet objectif.

Lorsqu'il s'agit de cibler des architectures NUMA, on va tout particulièrement s'intéresser au L3, qui représente la mémoire la plus rapide accessible par tous les coeurs d'un même noeud.

GRAPHE : 2.1.1 schéma de l'architecture des caches

TODO : tableau latences accès caches vs processeurs (sandy, broadwell, knl, bulldozer, ryzen).

TODO : ref + paragraph sur les caches configurables du ryzen

\subsubsection{SIMD}

\subsubsection{Hyperthreading}

Chaque coeur possède un certain nombre d'UAL (Unité Arithmétique et Logique) qui lui sont privées, lorsqu'il est en attente d'une donnée de la mémoire centrale, ces UALs ne sont pas utilisées, et des cycles CPU sont donc "perdus" à ne rien faire.

Afin de maximiser l'utilisation de ces ressources, certains processeurs Intel sont équipés de la technologie \emph{hyperthreading}.
Le concept est assez simple : avoir deux coeurs logiques (\emph{hyperthreads}) associés à un seul coeur physique.
De cette manière lorsqu'un thread est en attente sur une donnée, le second peut profiter des UALs disponibles.

Pour des tâches peu gourmandes en ressources ou utilisant beaucoup de données, cela peut effectivement se traduire par un gain de performances, mais pour le cas des applications HPC il faut regarder le type d'application utilisées pour savoir si on peut espérer un gain ou non.

En particulier les caches L1 et L2 sont partagés par les deux hyperthreads, donc si le code séquentiel généré est optimisé pour les tailles de caches correspondant, exécuter le même type de code séquentiel sur deux hyperthreads va entrainer du \emph{cache trashing} et des baisses de performance.

De même si l'application est compute-intensive, l'hyperthreading n'apportera pas grand chose, voir rien.

L'hyperthreading est généralement une option que l'on peut désactiver dans le BIOS de la machine.


\subsection{Interconnexion des noeuds}\label{sec:context:numa:interconnect}

La partie la plus importante d'une machine NUMA est le système d'interconnexion entre les différents noeuds.

C'est cette partie qui va déterminer à quel point cela va être couteux de faire un accès mémoire sur un noeud distant, et donc à quel point le côté NUMA de la machine (le \emph{facteur NUMA} (là si je cite pas la thèse de François je me fais tapé dessus ?)) va impacter les performances d'une application.

Dans la majorité des cas, ce système d'interconnexion est \emph{cache-coherent}, c'est à dire que la cohérence de cache est assurée entre les différents noeuds par le matériel, est n'est pas la responsabilité du programmeur.

TODO : parler de la hiérarchie possible de l'interconnexion.

TODO : donner quelques chiffres à titre d'exemple ?

GRAPHE : 2.1.2 montrer une version schématique d'une machine NUMA


\subsection{Particularité au niveau du système d'exploitation}\label{sec:context:numa:os}


Parler de la taille de page (4K vs 2M), et de l'impact que ça a.

Parler de~\cite{Dobson2003}

Parler d'hwloc ?

Parler de comment le système gère l'hyperthreading ? (mal, de toute façon on pin les threads)


